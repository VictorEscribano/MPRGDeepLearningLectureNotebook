{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/16_vit/01_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWHD62ufU17k"
      },
      "source": [
        "# Vision Transformer\n",
        "\n",
        "---\n",
        "Vision Transformer (ViT) [1] はTransformerをコンピュータビジョンに応用した画像分類手法です．ViTは入力画像を固定領域のパッチに分割して埋め込み層を介して，Transformer Encoderに入力します．Transformer Encoder内のSelf Attentionでパッチの関係を学習することで，畳み込みニューラルネットワーク (CNN: Convolutional Neural Network) とは異なり，浅い層から画像全体の特徴を捉えられます．これにより，ImageNetなどのクラス分類タスクでCNNの性能を上回りました．また，ViTはセマンティックセグメンテーションや動画像認識などのタスクに応用され，CNNベースの性能を上回りました．\n",
        "\n",
        "<img src=\"https://github.com/ShokiSuzuki/MPRGDeepLearningLectureNotebook/blob/dev/16_vit/model_scheme.png?raw=true\" width=60%>\n",
        "\n",
        "\n",
        "## Patch Embedding\n",
        "\n",
        "Patch Embeddingは，入力画像を固定領域のパッチに分割して埋め込む処理を行います．例えば，$224 \\times 224$ピクセルの画像を入力として各パッチのサイズを$16 \\times 16$ピクセルとした場合，重なり合わないように$14 \\times 14$の領域に分割します．分割されたパッチは，それぞれflatにして全結合に入力することで埋め込みます．また，学習可能なパラメータであるクラストークンを結合します．\n",
        "\n",
        "## Position Embedding\n",
        "\n",
        "Position Embeddingは，パッチの位置情報を学習するパラメータです．このパラメータは，Patch Embeddingのあとにそれぞれのパッチに足されます．ネットワークが学習する過程で位置情報を獲得するため，学習条件で値が変化します．\n",
        "\n",
        "Patch EmbeddingとPosition Embeddingを定式化すると以下のようになります．\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_0 &= [ \\mathbf{x}_\\text{class}; \\, \\mathbf{x}^1_p \\mathbf{E}; \\, \\mathbf{x}^2_p \\mathbf{E}; \\cdots; \\, \\mathbf{x}^{N}_p \\mathbf{E} ] + \\mathbf{E}_{pos},\n",
        "&& \\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D},\\, \\mathbf{E}_{pos}  \\in \\mathbb{R}^{(N + 1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "ここで，$\\mathbf{x}_\\text{class}$はクラストークン，$\\mathbf{x}_p$はパッチ，$N$はパッチ数，$P$はパッチサイズ，$C$はチャンネル数，$D$は埋め込み次元数，$\\mathbf{E}$は全結合，$\\mathbf{E}_{pos}$はPosition Embeddingです．\n",
        "\n",
        "## ファインチューニング\n",
        "\n",
        "ViTは，大規模データセットで事前学習して小規模データセットでファインチューニングすることが効果的です．事前学習の画像枚数を変更すると，CNNは枚数を多くしても精度に限界がある一方で，ViTは枚数が多いほど精度向上が見込めます．ViTは，JFT-300Mという3億枚の画像が含まれているデータセットで事前学習し，様々なデータセットでファインチューニングをすることでSoTAを達成していますが，非公開のデータセットのため再現不可能です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmzYVTVzU17m"
      },
      "source": [
        "# Vision Transformerの学習\n",
        "\n",
        "CIFAR-10を用いて，フルスクラッチで学習したViTとCNNの比較を行います．また，ImageNetで事前学習をしたモデルを用いてファインチューニングを行います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3oG9bveU17n"
      },
      "source": [
        "### モジュールの読み込み\n",
        "\n",
        "まず，Colaboratoryにないパッケージをインストールします．timmにはViTやSwinなどのネットワークだけでなく，様々な最適化手法や学習率のスケジューラーが用意されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4iq2dBxU17n",
        "outputId": "01eb1b9c-a347-441e-9da9-ed7da7cd9773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.5.4) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (1.24.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install timm==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYr71AN1U17n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models import create_model\n",
        "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
        "from functools import partial\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0KxX4zU17o"
      },
      "source": [
        "### ネットワークの定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyPcVS9U17o"
      },
      "source": [
        "#### Patch Embedding\n",
        "\n",
        "Patch Embeddingでは，画像をパッチに分割して埋め込みます．埋め込まれたパッチをパッチトークンと呼びます．ViTはパッチをflatにして全結合に入力しますが，実装上は，カーネルサイズ（パッチサイズ）= ストライドとした2次元畳み込みでも可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsPouvAJU17o"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size    = img_size\n",
        "        self.patch_size  = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # 埋め込み処理のための重み\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x).flatten(2).transpose(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huFa_aa6U17p"
      },
      "source": [
        "#### Multi-Head Attention\n",
        "\n",
        "Self-Attentionはパッチトークンを空間方向に混ぜるような変換を行います．Multi-Head Attentionはパッチトークンをベクトルのdepth方向に$h$個に分割し，それぞれでSelf-Attentionを求めます．例えばSmallモデルの場合，埋め込み次元数が384でhead数が6であるため，64次元のベクトルが6つある状態になります．\n",
        "これにより，head毎に注目したパッチトークンが異なる特徴が得られるため，アンサンブル効果による精度向上が見込めます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwg7aH0GU17p"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # パッチトークンをQ, K, Vに変換し，それぞれベクトルのdepth方向に分割\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        # Attention Weightの算出\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Attention WeightとVを乗算\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnImeUIIU17p"
      },
      "source": [
        "#### Multi-Layer Perceptron\n",
        "\n",
        "Multi-Head Attentionでは空間方向に混ぜるような変換を行うのに対し，Multi-Layer Perceptronではベクトルのdepth方向に混ぜるような変換を行います．活性化関数にはGELUを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhYgdnmKU17q"
      },
      "outputs": [],
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg3afq0BU17q"
      },
      "source": [
        "#### Transformer Encoder\n",
        "\n",
        "Transformer Encoderは，Multi-Head AttentionとMulti-Layer Perceptronを交互に使用します．また，それぞれResidual Connectionを用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlMTvDm3U17q"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., \n",
        "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BOm0dA0U17q"
      },
      "source": [
        "#### ネットワーク全体の構築\n",
        "\n",
        "これまで定義したクラスをもとにViTを構築します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uej3haHDU17q"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, \n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, \n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, \n",
        "                 norm_layer=None, act_layer=None, block_fn=Block):\n",
        "        super().__init__()\n",
        "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embeddingの定義\n",
        "        self.patch_embed = embed_layer(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # クラストークンの定義\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        \n",
        "        # Position Embeddingの定義 (クラストークンのためにパッチ数+1)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop  = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth\n",
        "        \n",
        "        # Transformer Encoderの定義\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            block_fn(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i],\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier Headの定義\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "        \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        \n",
        "        # クラストークンの結合\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        \n",
        "        # Position Embeddingの加算\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "        \n",
        "        # Transformer Encoderへ入力\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        \n",
        "        # 0番目にあるクラストークンを取り出して全結合へ入力\n",
        "        x = self.head(x[:, 0])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXCXRDZsU17r"
      },
      "outputs": [],
      "source": [
        "def vit_tiny_patch16_224(pretrained=False, patch_size=16, num_heads=3, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=192, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit_small_patch16_224(pretrained=False, patch_size=16, num_heads=6, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=384, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit_base_patch16_224(pretrained=False, patch_size=16, num_heads=12, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=768, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-nIbCrU17r"
      },
      "source": [
        "### データの準備\n",
        "今回は，CIFAR-10を用いてフルスクラッチで学習します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "8dc8b0315d044f0c8e131425a8022511",
            "4413173c04e44cbfa1b25bcccb1aee3c",
            "730dd7575ecd49e987ac34fb8744faef",
            "b05928cc3b1449c483bb6eb15ae98a93",
            "1f00f9f4d20f4efe917f42d932e8b9bf",
            "3755f03ccce64ab0b5d06908d8f12863",
            "b52cf220b70740e3a2307d54e11bbc49",
            "0a3a3c6463664a46b2f7d66d9ea90eb9",
            "4129c4bb0a0f44c2b83b7419d1b32b59",
            "9c1a5759887a4e739a9b0c3a90d28a70",
            "2761669405e34ab692cc8f3a73f796fd"
          ]
        },
        "id": "N1he5n-VU17r",
        "outputId": "8f27af79-1fd3-436b-9b28-1568cbf85975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dc8b0315d044f0c8e131425a8022511"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n"
          ]
        }
      ],
      "source": [
        "img_size = 32\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9udJN4U17r"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ViTの性能をCNNと比較するために，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．今回はViTとCNNの学習条件を揃えて学習します．\n",
        "ViTを学習させるときのパッチサイズは4とします．「データの準備」で画像サイズを32と設定したため，パッチ数は$8\\times 8=64$となります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu3I1unkU17r"
      },
      "outputs": [],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義\n",
        "vit = vit_small_patch16_224(pretrained=False, num_classes=num_classes, img_size=img_size, patch_size=4, num_heads=6)\n",
        "# CNNの定義\n",
        "cnn = create_model(\"resnet50\", pretrained=False, num_classes=num_classes)\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0005\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 10\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 3\n",
        "\n",
        "# 最適化手法の設定 (ViT)\n",
        "optimizer_vit     = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (ViT)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)\n",
        "# 最適化手法の設定 (CNN)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (CNN)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnTcfijU17s"
      },
      "source": [
        "ViTとCNNのパラメータ数を確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-aK4ttgU17s",
        "outputId": "66b1cc1f-0111-4e84-ebf5-cf6b10cc905b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT parameters:  21342346\n",
            "CNN parameters:  23528522\n"
          ]
        }
      ],
      "source": [
        "def num_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"ViT parameters: \", num_parameters(vit))\n",
        "print(\"CNN parameters: \", num_parameters(cnn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHIom2qtU17s"
      },
      "source": [
        "### CNNの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svauzN4eU17s",
        "outputId": "45df3a02-a327-48da-9224-56900988fed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.342,            mean accuracy: 0.11,            elapsed_time : 41.49\n",
            "test accuracy: 0.104\n",
            "epoch: 2,            mean loss: 2.342,            mean accuracy: 0.11,            elapsed_time : 80.6\n",
            "test accuracy: 0.1045\n",
            "epoch: 3,            mean loss: 1.719,            mean accuracy: 0.36,            elapsed_time : 120.67\n",
            "test accuracy: 0.4896\n",
            "epoch: 4,            mean loss: 1.371,            mean accuracy: 0.5,            elapsed_time : 160.17\n",
            "test accuracy: 0.5815\n",
            "epoch: 5,            mean loss: 1.182,            mean accuracy: 0.57,            elapsed_time : 199.59\n",
            "test accuracy: 0.6303\n",
            "epoch: 6,            mean loss: 1.027,            mean accuracy: 0.63,            elapsed_time : 238.94\n",
            "test accuracy: 0.6572\n",
            "epoch: 7,            mean loss: 0.921,            mean accuracy: 0.67,            elapsed_time : 280.16\n",
            "test accuracy: 0.6934\n",
            "epoch: 8,            mean loss: 0.835,            mean accuracy: 0.7,            elapsed_time : 319.5\n",
            "test accuracy: 0.7134\n",
            "epoch: 9,            mean loss: 0.765,            mean accuracy: 0.73,            elapsed_time : 358.6\n",
            "test accuracy: 0.7334\n",
            "epoch: 10,            mean loss: 0.718,            mean accuracy: 0.74,            elapsed_time : 398.89\n",
            "test accuracy: 0.7472\n"
          ]
        }
      ],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# GPU周りの設定\n",
        "device = torch.device(\"cuda\")\n",
        "cnn.to(device)\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    cnn.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # CNNに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit = cnn(img)\n",
        "            loss  = criterion(logit, cls)\n",
        "            \n",
        "        # CNNの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    cnn.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit = cnn(img)\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAS4HmSQU17s"
      },
      "source": [
        "### ViTの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuPlFv3U17s",
        "outputId": "cda0166a-d187-471c-840a-4b1bf4b8ef8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.32,            mean accuracy: 0.15,            elapsed_time : 66.93\n",
            "test accuracy: 0.1545\n",
            "epoch: 2,            mean loss: 2.32,            mean accuracy: 0.15,            elapsed_time : 145.52\n",
            "test accuracy: 0.1545\n",
            "epoch: 3,            mean loss: 1.807,            mean accuracy: 0.32,            elapsed_time : 223.97\n",
            "test accuracy: 0.4158\n",
            "epoch: 4,            mean loss: 1.511,            mean accuracy: 0.44,            elapsed_time : 303.43\n",
            "test accuracy: 0.4892\n",
            "epoch: 5,            mean loss: 1.346,            mean accuracy: 0.51,            elapsed_time : 382.24\n",
            "test accuracy: 0.5326\n",
            "epoch: 6,            mean loss: 1.223,            mean accuracy: 0.56,            elapsed_time : 460.47\n",
            "test accuracy: 0.5819\n",
            "epoch: 7,            mean loss: 1.12,            mean accuracy: 0.6,            elapsed_time : 538.93\n",
            "test accuracy: 0.6114\n",
            "epoch: 8,            mean loss: 1.026,            mean accuracy: 0.63,            elapsed_time : 617.14\n",
            "test accuracy: 0.6426\n",
            "epoch: 9,            mean loss: 0.939,            mean accuracy: 0.66,            elapsed_time : 695.63\n",
            "test accuracy: 0.6784\n",
            "epoch: 10,            mean loss: 0.862,            mean accuracy: 0.69,            elapsed_time : 773.87\n",
            "test accuracy: 0.6945\n"
          ]
        }
      ],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# GPU周りの設定\n",
        "device = torch.device(\"cuda\")\n",
        "vit.to(device)\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    vit.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # ViTに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit = vit(img)\n",
        "            loss  = criterion(logit, cls)\n",
        "            \n",
        "        # ViTの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_vit.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    vit.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit = vit(img)\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "iP3j-qwNU17s"
      },
      "source": [
        "# ImageNetで事前学習したモデルを用いたファインチューニング\n",
        "\n",
        "次に，ImageNetで事前学習したモデルを用いてCIFAR-10でファインチューニングします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-h09wINU17s"
      },
      "source": [
        "### データの準備\n",
        "\n",
        "フルスクラッチで学習したときの画像サイズは32でしたが，ファインチューニングでは学習済みモデルに合わせるためにに224にします．画像サイズを変更する場合，パッチ数が変わる影響でPosition Embeddingのサイズを整える必要があるため，今回は省略します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey0GcQ2sU17s",
        "outputId": "7af6f11a-3d58-4a98-d5b7-5f643bb04d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "img_size = 224\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=64, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvx2qwkU17s"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ファインチューニングでも，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．ViTの学習済みモデルにはData-efficient image Transformers (DeiT) を使用します．ネットワーク構造は通常のViTと同様です．DeiTは学習時にRand AugmentやCutMix，MixupなどのData Augmentationを使用することで，ImageNetのみでCNNに匹敵する性能を達成した手法です．ResNet-50の学習済みモデルは，DeiTの学習条件に寄せて学習した重みです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0VWq0l2U17s",
        "outputId": "b902672d-ca3e-4120-cfe2-2d7aec931f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\" to /root/.cache/torch/hub/checkpoints/deit_small_patch16_224-cd65a155.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\n"
          ]
        }
      ],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義 (DeiTの学習済みモデルを使用)\n",
        "vit_finetune = create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "# CNNの定義\n",
        "cnn_finetune = create_model(\"resnet50\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0001\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 5\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 0\n",
        "\n",
        "# 最適化手法の設定 (ViT)\n",
        "optimizer_vit     = torch.optim.AdamW(vit_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (ViT)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)\n",
        "# 最適化手法の設定 (CNN)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (CNN)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtYsGiTqU17s"
      },
      "source": [
        "### CNNの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQvVUsRFU17t",
        "outputId": "ee51686f-3340-44ce-e330-2a0c9afdd491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 0.675,            mean accuracy: 0.81,            elapsed_time : 207.87\n",
            "test accuracy: 0.947\n",
            "epoch: 2,            mean loss: 0.167,            mean accuracy: 0.95,            elapsed_time : 443.39\n",
            "test accuracy: 0.9594\n",
            "epoch: 3,            mean loss: 0.106,            mean accuracy: 0.97,            elapsed_time : 679.07\n",
            "test accuracy: 0.9653\n",
            "epoch: 4,            mean loss: 0.072,            mean accuracy: 0.98,            elapsed_time : 914.53\n",
            "test accuracy: 0.9665\n",
            "epoch: 5,            mean loss: 0.05,            mean accuracy: 0.98,            elapsed_time : 1150.06\n",
            "test accuracy: 0.9661\n"
          ]
        }
      ],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# GPU周りの設定\n",
        "device = torch.device(\"cuda\")\n",
        "cnn_finetune.to(device)\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    cnn_finetune.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # CNNに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit = cnn_finetune(img)\n",
        "            loss  = criterion(logit, cls)\n",
        "            \n",
        "        # CNNの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    cnn_finetune.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit = cnn_finetune(img)\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bV1FrStU17t"
      },
      "source": [
        "### ViTの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7FZ2vW4U17t",
        "outputId": "d47b01d4-e3c5-4d4f-a7b5-49fdf6749d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 0.217,            mean accuracy: 0.93,            elapsed_time : 265.65\n",
            "test accuracy: 0.9562\n",
            "epoch: 2,            mean loss: 0.1,            mean accuracy: 0.97,            elapsed_time : 565.66\n",
            "test accuracy: 0.9588\n",
            "epoch: 3,            mean loss: 0.062,            mean accuracy: 0.98,            elapsed_time : 865.74\n",
            "test accuracy: 0.9637\n",
            "epoch: 4,            mean loss: 0.037,            mean accuracy: 0.99,            elapsed_time : 1165.83\n",
            "test accuracy: 0.9675\n",
            "epoch: 5,            mean loss: 0.016,            mean accuracy: 0.99,            elapsed_time : 1465.84\n",
            "test accuracy: 0.9733\n"
          ]
        }
      ],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# GPU周りの設定\n",
        "device = torch.device(\"cuda\")\n",
        "vit_finetune.to(device)\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    vit_finetune.train()\n",
        "    \n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "    for img, cls in dataloader_train:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "        # ViTに画像を入力 & 損失を計算\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logit = vit_finetune(img)\n",
        "            loss  = criterion(logit, cls)\n",
        "            \n",
        "        # ViTの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "        \n",
        "    lr_scheduler_vit.step(epoch)\n",
        "    \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1},\\\n",
        "            mean loss: {round(sum_loss/len(dataloader_train), 3)},\\\n",
        "            mean accuracy: {round(count/len(dataloader_train.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "    \n",
        "    # 評価\n",
        "    vit_finetune.eval()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "        \n",
        "            logit = vit_finetune(img)\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "            \n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0Q91mUU17t"
      },
      "source": [
        "# 課題\n",
        "1. Multi-Head Attentionのhead数を変えてみましょう．num_headsで変更できます．ただし，埋め込み次元数（Smallモデルの場合は384）が割り切れる値にしてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTXWbeohU17t"
      },
      "source": [
        "# 参考文献\n",
        "\n",
        "[1]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In *International Conference on Learning Representations*, 2021."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "vscode": {
      "interpreter": {
        "hash": "292b6db79841738ca824380b98b3729806180501b450715886339eec8f0eb9a3"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8dc8b0315d044f0c8e131425a8022511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4413173c04e44cbfa1b25bcccb1aee3c",
              "IPY_MODEL_730dd7575ecd49e987ac34fb8744faef",
              "IPY_MODEL_b05928cc3b1449c483bb6eb15ae98a93"
            ],
            "layout": "IPY_MODEL_1f00f9f4d20f4efe917f42d932e8b9bf"
          }
        },
        "4413173c04e44cbfa1b25bcccb1aee3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3755f03ccce64ab0b5d06908d8f12863",
            "placeholder": "​",
            "style": "IPY_MODEL_b52cf220b70740e3a2307d54e11bbc49",
            "value": "100%"
          }
        },
        "730dd7575ecd49e987ac34fb8744faef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a3a3c6463664a46b2f7d66d9ea90eb9",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4129c4bb0a0f44c2b83b7419d1b32b59",
            "value": 170498071
          }
        },
        "b05928cc3b1449c483bb6eb15ae98a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c1a5759887a4e739a9b0c3a90d28a70",
            "placeholder": "​",
            "style": "IPY_MODEL_2761669405e34ab692cc8f3a73f796fd",
            "value": " 170498071/170498071 [00:02&lt;00:00, 86214279.79it/s]"
          }
        },
        "1f00f9f4d20f4efe917f42d932e8b9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3755f03ccce64ab0b5d06908d8f12863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b52cf220b70740e3a2307d54e11bbc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a3a3c6463664a46b2f7d66d9ea90eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4129c4bb0a0f44c2b83b7419d1b32b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c1a5759887a4e739a9b0c3a90d28a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2761669405e34ab692cc8f3a73f796fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}