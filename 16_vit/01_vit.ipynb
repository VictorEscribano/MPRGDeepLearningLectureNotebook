{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/16_vit/01_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7SZPh9haZHn"
      },
      "source": [
        "# Vision Transformer\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "\n",
        "CIFAR-10を用いて，フルスクラッチで学習したViTとCNNの比較を行います．また，ImageNetで事前学習をしたモデルを用いてファインチューニングを行います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3oG9bveU17n"
      },
      "source": [
        "## 必要なモジュールの読み込み\n",
        "\n",
        "まず，Colaboratoryにないパッケージをインストールします．timmにはViTやSwinなどのネットワークだけでなく，様々な最適化手法や学習率のスケジューラーが用意されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4iq2dBxU17n"
      },
      "outputs": [],
      "source": [
        "!pip -q install timm==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYr71AN1U17n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models import create_model\n",
        "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
        "from functools import partial\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0KxX4zU17o"
      },
      "source": [
        "## ネットワークの定義\n",
        "\n",
        "Vision Transformer (ViT) [1] はTransformerをコンピュータビジョンに応用した画像分類手法です．ViTは入力画像を固定領域のパッチに分割して埋め込み層を介して，Transformer Encoderに入力します．Transformer Encoder内のSelf Attentionでパッチの関係を学習することで，畳み込みニューラルネットワーク (CNN) とは異なり，浅い層から画像全体の特徴を捉えられます．これにより，ImageNetなどのクラス分類タスクでCNNの性能を上回りました．また，ViTはセマンティックセグメンテーションや動画像認識などのタスクに応用され，CNNベースの性能を上回りました．\n",
        "\n",
        "<img src=\"https://github.com/ShokiSuzuki/MPRGDeepLearningLectureNotebook/blob/dev/16_vit/model_scheme.png?raw=true\" width=60%>\n",
        "\n",
        "\n",
        "### ファインチューニング\n",
        "\n",
        "ViTは大規模データセットで事前学習し，小規模データセットでファインチューニングすることが効果的です．事前学習の画像枚数を変更すると，CNNは枚数を多くしても精度に限界がある一方で，ViTは枚数が多いほど精度向上が見込めます．ViTはJFT-300Mという3億枚の画像が含まれているデータセットで事前学習し，様々なデータセットでファインチューニングをすることでSoTAを達成していますが，非公開のデータセットのため再現不可能です．\n",
        "\n",
        "\n",
        "### 各モジュールの実装\n",
        "\n",
        "以下では，ViTを構成する主要なモジュールについて一つづつ実装を行なっていきます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyPcVS9U17o"
      },
      "source": [
        "### Patch Embedding\n",
        "\n",
        "Patch Embeddingは，入力画像を固定領域のパッチに分割して埋め込む処理を行います．例えば，$224 \\times 224$ピクセルの画像を入力として各パッチのサイズを$16 \\times 16$ピクセルとした場合，重なり合わないように$14 \\times 14$の領域に分割します．分割されたパッチは，それぞれflatにして全結合に入力することで埋め込みます．また，学習可能なパラメータであるクラストークンを結合します．\n",
        "\n",
        "基本的にはパッチをflatにして全結合に入力しますが，実装上は，カーネルサイズ（パッチサイズ）= ストライドとした2次元畳み込みでも可能です．\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsPouvAJU17o"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size    = img_size\n",
        "        self.patch_size  = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # 埋め込み処理のための重み\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x).flatten(2).transpose(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2SyJfpou-4p"
      },
      "source": [
        "\n",
        "### Position Embedding\n",
        "\n",
        "Position Embeddingは，パッチの位置情報を学習するパラメータです．このパラメータは，Patch Embeddingのあとにそれぞれのパッチに足されます．ネットワークが学習する過程で位置情報を獲得するため，学習条件で値が変化します．\n",
        "\n",
        "Patch EmbeddingとPosition Embeddingを定式化すると以下のようになります．\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_0 &= [ \\mathbf{x}_\\text{class}; \\, \\mathbf{x}^1_p \\mathbf{E}; \\, \\mathbf{x}^2_p \\mathbf{E}; \\cdots; \\, \\mathbf{x}^{N}_p \\mathbf{E} ] + \\mathbf{E}_{pos},\n",
        "&& \\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D},\\, \\mathbf{E}_{pos}  \\in \\mathbb{R}^{(N + 1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "ここで，$\\mathbf{x}_\\text{class}$はクラストークン，$\\mathbf{x}_p$はパッチ，$N$はパッチ数，$P$はパッチサイズ，$C$はチャンネル数，$D$は埋め込み次元数，$\\mathbf{E}$は全結合，$\\mathbf{E}_{pos}$はPosition Embeddingです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huFa_aa6U17p"
      },
      "source": [
        "\n",
        "\n",
        "### Multi-Head Attention\n",
        "\n",
        "Self-Attentionはパッチトークンを空間方向に混ぜるような変換を行います．Multi-Head Attentionはパッチトークンをベクトルのdepth方向に$h$個に分割し，それぞれでSelf-Attentionを求めます．例えばSmallモデルの場合，埋め込み次元数が384でhead数が6であるため，64次元のベクトルが6つある状態になります．\n",
        "これにより，head毎に注目したパッチトークンが異なる特徴が得られるため，アンサンブル効果による精度向上が見込めます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwg7aH0GU17p"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # パッチトークンをQ, K, Vに変換し，それぞれベクトルのdepth方向に分割\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        # Attention Weightの算出\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Attention WeightとVを乗算\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnImeUIIU17p"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "\n",
        "Multi-Head Attentionでは空間方向に混ぜるような変換を行うのに対し，Multi-Layer Perceptronではベクトルのdepth方向に混ぜるような変換を行います．活性化関数にはGELUを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhYgdnmKU17q"
      },
      "outputs": [],
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg3afq0BU17q"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Transformer Encoderは，Multi-Head AttentionとMulti-Layer Perceptronを交互に使用します．また，それぞれResidual Connectionを用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlMTvDm3U17q"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0.,\n",
        "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BOm0dA0U17q"
      },
      "source": [
        "### ネットワーク全体の構築\n",
        "\n",
        "これまで定義したクラスをもとにViTを構築します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uej3haHDU17q"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed,\n",
        "                 norm_layer=None, act_layer=None, block_fn=Block):\n",
        "        super().__init__()\n",
        "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch Embeddingの定義\n",
        "        self.patch_embed = embed_layer(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # クラストークンの定義\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Position Embeddingの定義 (クラストークンのためにパッチ数+1)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop  = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth\n",
        "\n",
        "        # Transformer Encoderの定義\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            block_fn(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i],\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier Headの定義\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        # クラストークンの結合\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Position Embeddingの加算\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer Encoderへ入力\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # 0番目にあるクラストークンを取り出して全結合へ入力\n",
        "        x = self.head(x[:, 0])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlpaHLagvWFn"
      },
      "source": [
        "ViTでは，いくつかのネットワークサイズが提案されています．\n",
        "\n",
        "以下では，上で定義したVisionTransformerクラスを活用して，それらを呼び出せるような関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXCXRDZsU17r"
      },
      "outputs": [],
      "source": [
        "def vit_tiny_patch16_224(pretrained=False, patch_size=16, num_heads=3, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=192, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit_small_patch16_224(pretrained=False, patch_size=16, num_heads=6, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=384, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def vit_base_patch16_224(pretrained=False, patch_size=16, num_heads=12, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, num_heads=num_heads, embed_dim=768, depth=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-nIbCrU17r"
      },
      "source": [
        "## フルスクラッチでの学習\n",
        "\n",
        "まず，フルスクラッチ（乱数で初期化されたパラメータ）から学習します．\n",
        "\n",
        "\n",
        "### データの準備\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1he5n-VU17r"
      },
      "outputs": [],
      "source": [
        "img_size = 32\n",
        "\n",
        "# 学習用データのデータ増幅と評価用データのデータ増幅の設定\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "# 学習用データと評価用データの用意\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "# 学習用データのDataloaderと評価用データのDataloaderの用意\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9udJN4U17r"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ViTの性能をCNNと比較するために，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．今回はViTとCNNの学習条件を揃えて学習します．\n",
        "ViTを学習させるときのパッチサイズは4とします．「データの準備」で画像サイズを32と設定したため，パッチ数は$8\\times 8=64$となります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu3I1unkU17r"
      },
      "outputs": [],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義\n",
        "vit = vit_small_patch16_224(pretrained=False, num_classes=num_classes, img_size=img_size, patch_size=4, num_heads=6)\n",
        "# CNNの定義（パラメータ数比較用）\n",
        "cnn = create_model(\"resnet50\", pretrained=False, num_classes=num_classes)\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0005\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 10\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 3\n",
        "\n",
        "# 最適化手法の設定 (ViT)\n",
        "optimizer_vit     = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (ViT)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnTcfijU17s"
      },
      "source": [
        "ViTとCNNのパラメータ数を確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-aK4ttgU17s"
      },
      "outputs": [],
      "source": [
        "def num_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"ViT parameters: \", num_parameters(vit))\n",
        "print(\"CNN parameters: \", num_parameters(cnn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAS4HmSQU17s"
      },
      "source": [
        "### ViTの学習と評価\n",
        "\n",
        "\n",
        "#### 参考精度\n",
        "\n",
        "デフォルトの設定で学習を行なった場合に達成できる分類精度（目安）を記載します．\n",
        "\n",
        "※ 学習が演習時間内に終了しない場合を考慮して記載しています．\n",
        "\n",
        "**CNNの参考分類精度（スクラッチ）**\n",
        "\n",
        "```\n",
        "epoch: 10, mean loss: 0.725, mean accuracy: 0.74, elapsed_time : 213.11\n",
        "test accuracy: 0.7433\n",
        "```\n",
        "\n",
        "**ViTの参考分類精度（スクラッチ）**\n",
        "\n",
        "```\n",
        "epoch: 10, mean loss: 0.87, mean accuracy: 0.69, elapsed_time : 240.45\n",
        "test accuracy: 0.6954\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UuPlFv3U17s"
      },
      "outputs": [],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "device = torch.device(\"cuda\")\n",
        "vit.to(device)\n",
        "\n",
        "# 学習時間短縮のためにAutomatic Mixed Precision(amp)を利用\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    # ネットワークを学習モードへ変更\n",
        "    vit.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    for img, cls in dataloader_train:\n",
        "        # 学習用データをGPUへ\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # ネットワークに学習用データを入力\n",
        "            logit = vit(img)\n",
        "            # 損失を計算\n",
        "            loss  = criterion(logit, cls)\n",
        "\n",
        "        # ネットワークの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "    # 学習率の調整\n",
        "    lr_scheduler_vit.step(epoch)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1}, \", end='')\n",
        "    print(f\"mean loss: {round(sum_loss/len(dataloader_train), 3)}, \", end='')\n",
        "    print(f\"mean accuracy: {round(count/len(dataloader_train.dataset), 2)}, \", end='')\n",
        "    print(f\"elapsed_time : {round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークを評価モードへ変更\n",
        "    vit.eval()\n",
        "\n",
        "    # ログ用の設定\n",
        "    count = 0\n",
        "\n",
        "    # ネットワークの評価\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            # 評価用データをGPUへ\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "            # ネットワークに評価用データを入力\n",
        "            logit = vit(img)\n",
        "\n",
        "            # 正解したデータ数をカウント\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "        # 評価用データに対する正解率を計算して表示\n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP3j-qwNU17s",
        "tags": []
      },
      "source": [
        "## ImageNetで事前学習したモデルを用いたファインチューニング\n",
        "\n",
        "次に，ImageNetで事前学習したモデルを用いてCIFAR-10でファインチューニングします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-h09wINU17s"
      },
      "source": [
        "### データの準備\n",
        "\n",
        "フルスクラッチで学習したときの画像サイズは32でしたが，ファインチューニングでは学習済みモデルに合わせるためにに224にします．画像サイズを変更する場合，パッチ数が変わる影響でPosition Embeddingのサイズを整える必要があるため，今回は省略します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey0GcQ2sU17s"
      },
      "outputs": [],
      "source": [
        "img_size = 224\n",
        "\n",
        "# 学習用データのデータ増幅と評価用データのデータ増幅の設定\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "test_transform  = transforms.Compose([transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                     ])\n",
        "\n",
        "# 学習用データと評価用データの用意\n",
        "dataset_train = torchvision.datasets.CIFAR10(\"./\", train=True, transform=train_transform, download=True)\n",
        "dataset_test  = torchvision.datasets.CIFAR10(\"./\", train=False, transform=test_transform, download=False)\n",
        "\n",
        "# 学習用データのDataloaderと評価用データのDataloaderの用意\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=64, num_workers=2, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=64, num_workers=2, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvx2qwkU17s"
      },
      "source": [
        "### 学習条件の設定\n",
        "\n",
        "ファインチューニングでも，ViTのSmallモデルとパラメータ数が同等のResNet-50を用います．ViTの学習済みモデルにはData-efficient image Transformers (DeiT) を使用します．ネットワーク構造は通常のViTと同様です．DeiTは学習時にRand AugmentやCutMix，MixupなどのData Augmentationを使用することで，ImageNetのみでCNNに匹敵する性能を達成した手法です．ResNet-50の学習済みモデルは，DeiTの学習条件に寄せて学習した重みです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0VWq0l2U17s"
      },
      "outputs": [],
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# ViTの定義\n",
        "vit_finetune = create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=num_classes) # ImageNet (DeiT)\n",
        "#vit_finetune = create_model(\"vit_small_patch16_224_in21k\", pretrained=True, num_classes=num_classes) # ImageNet-21k\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0001\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 5\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 0\n",
        "\n",
        "# 最適化手法の設定 (ViT)\n",
        "optimizer_vit     = torch.optim.AdamW(vit_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (ViT)\n",
        "lr_scheduler_vit  = CosineLRScheduler(optimizer=optimizer_vit, t_initial=epochs, warmup_t=warmup_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bV1FrStU17t"
      },
      "source": [
        "### ViTの学習と評価\n",
        "\n",
        "#### 参考精度\n",
        "\n",
        "デフォルトの設定で学習を行なった場合に達成できる分類精度（目安）を記載します．\n",
        "\n",
        "※ 学習が演習時間内に終了しない場合を考慮して記載しています．\n",
        "\n",
        "**CNNの参考分類精度（ファインチューニング）**\n",
        "\n",
        "```\n",
        "epoch: 5, mean loss: 0.049, mean accuracy: 0.98, elapsed_time : 381.52\n",
        "test accuracy: 0.9679\n",
        "```\n",
        "\n",
        "**ViTの参考分類精度（ファインチューニング）**\n",
        "\n",
        "```\n",
        "epoch: 5, mean loss: 0.016, mean accuracy: 0.99, elapsed_time : 1170.5\n",
        "test accuracy: 0.9758\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7FZ2vW4U17t"
      },
      "outputs": [],
      "source": [
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "device = torch.device(\"cuda\")\n",
        "vit_finetune.to(device)\n",
        "\n",
        "# 学習時間短縮のためにAutomatic Mixed Precision(amp)を利用\n",
        "use_amp = True\n",
        "scaler_vit = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    # ネットワークを学習モードへ変更\n",
        "    vit_finetune.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    for img, cls in dataloader_train:\n",
        "        # 学習用データをGPUへ\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # ネットワークに学習用データを入力\n",
        "            logit = vit_finetune(img)\n",
        "            # 損失を計算\n",
        "            loss  = criterion(logit, cls)\n",
        "\n",
        "        # ネットワークの更新\n",
        "        optimizer_vit.zero_grad()\n",
        "        scaler_vit.scale(loss).backward()\n",
        "        scaler_vit.step(optimizer_vit)\n",
        "        scaler_vit.update()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "    # 学習率の調整\n",
        "    lr_scheduler_vit.step(epoch)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1}, \", end='')\n",
        "    print(f\"mean loss: {round(sum_loss/len(dataloader_train), 3)}, \", end='')\n",
        "    print(f\"mean accuracy: {round(count/len(dataloader_train.dataset), 2)}, \", end='')\n",
        "    print(f\"elapsed_time : {round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークを評価モードへ変更\n",
        "    vit_finetune.eval()\n",
        "\n",
        "    # ログ用の設定\n",
        "    count = 0\n",
        "\n",
        "    # ネットワークの評価\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            # 評価用データをGPUへ\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "            # ネットワークに評価用データを入力\n",
        "            logit = vit_finetune(img)\n",
        "\n",
        "            # 正解したデータ数をカウント\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "        # 評価用データに対する正解率を計算して表示\n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0Q91mUU17t"
      },
      "source": [
        "# 課題\n",
        "1. Multi-Head Attentionのhead数を変えてみましょう．num_headsで変更できます．ただし，埋め込み次元数（Smallモデルの場合は384）が割り切れる値にしてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTXWbeohU17t"
      },
      "source": [
        "# 参考文献\n",
        "\n",
        "[1]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In *International Conference on Learning Representations*, 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 備考（CNNの学習プログラム）\n",
        "\n",
        "演習時間中には行いませんが，比較としてCNN(ResNet50)を用いた際の学習プログラムに記します．\n"
      ],
      "metadata": {
        "id": "ZuAKe0fGAjUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNの学習（スクラッチ）"
      ],
      "metadata": {
        "id": "NmkuNyvlBD4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# CNNの定義\n",
        "cnn = create_model(\"resnet50\", pretrained=False, num_classes=num_classes)\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0005\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 10\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 3\n",
        "\n",
        "# 最適化手法の設定 (CNN)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (CNN)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)\n",
        "\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "device = torch.device(\"cuda\")\n",
        "cnn.to(device)\n",
        "\n",
        "# 学習時間短縮のためにAutomatic Mixed Precision(amp)を利用\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    # ネットワークを学習モードへ変更\n",
        "    cnn.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    for img, cls in dataloader_train:\n",
        "        # 学習用データをGPUへ\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # ネットワークに学習用データを入力\n",
        "            logit = cnn(img)\n",
        "            # 損失を計算\n",
        "            loss  = criterion(logit, cls)\n",
        "\n",
        "        # ネットワークの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "    # 学習率の調整\n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1}, \", end='')\n",
        "    print(f\"mean loss: {round(sum_loss/len(dataloader_train), 3)}, \", end='')\n",
        "    print(f\"mean accuracy: {round(count/len(dataloader_train.dataset), 2)}, \", end='')\n",
        "    print(f\"elapsed_time : {round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークを評価モードへ変更\n",
        "    cnn.eval()\n",
        "\n",
        "    # ログ用の設定\n",
        "    count = 0\n",
        "\n",
        "    # ネットワークの評価\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            # 評価用データをGPUへ\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "            # ネットワークに評価用データを入力\n",
        "            logit = cnn(img)\n",
        "\n",
        "            # 正解したデータ数をカウント\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "        # 評価用データに対する正解率を計算して表示\n",
        "        print(f\"    test accuracy: {count/len(dataloader_test.dataset)}\")"
      ],
      "metadata": {
        "id": "E18SO7bZAjMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNの学習（ファインチューニング）"
      ],
      "metadata": {
        "id": "zuH23Q5dBfer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# クラス数の設定\n",
        "num_classes = 10\n",
        "\n",
        "# CNNの定義\n",
        "cnn_finetune = create_model(\"resnet50\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "# 学習率の設定\n",
        "lr  = 0.0001\n",
        "# Weight Decayの設定\n",
        "weight_decay = 0.05\n",
        "# エポック数の設定\n",
        "epochs = 5\n",
        "# Warmup Epochの設定\n",
        "warmup_t = 0\n",
        "\n",
        "# 最適化手法の設定 (CNN)\n",
        "optimizer_cnn     = torch.optim.AdamW(cnn_finetune.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# 学習率のスケジューラーの設定 (CNN)\n",
        "lr_scheduler_cnn  = CosineLRScheduler(optimizer=optimizer_cnn, t_initial=epochs, warmup_t=warmup_t)\n",
        "\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "device = torch.device(\"cuda\")\n",
        "cnn_finetune.to(device)\n",
        "\n",
        "# 学習時間短縮のためにAutomatic Mixed Precision(amp)を利用\n",
        "use_amp = True\n",
        "scaler_cnn = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(epochs):\n",
        "    # ネットワークを学習モードへ変更\n",
        "    cnn_finetune.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count    = 0\n",
        "\n",
        "    for img, cls in dataloader_train:\n",
        "        # 学習用データをGPUへ\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # ネットワークに学習用データを入力\n",
        "            logit = cnn_finetune(img)\n",
        "            # 損失を計算\n",
        "            loss  = criterion(logit, cls)\n",
        "\n",
        "        # ネットワークの更新\n",
        "        optimizer_cnn.zero_grad()\n",
        "        scaler_cnn.scale(loss).backward()\n",
        "        scaler_cnn.step(optimizer_cnn)\n",
        "        scaler_cnn.update()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        count    += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "    # 学習率の調整\n",
        "    lr_scheduler_cnn.step(epoch)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch+1}, \", end='')\n",
        "    print(f\"mean loss: {round(sum_loss/len(dataloader_train), 3)}, \", end='')\n",
        "    print(f\"mean accuracy: {round(count/len(dataloader_train.dataset), 2)}, \", end='')\n",
        "    print(f\"elapsed_time : {round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークを評価モードへ変更\n",
        "    cnn_finetune.eval()\n",
        "\n",
        "    # ログ用の設定\n",
        "    count = 0\n",
        "\n",
        "    # ネットワークの評価\n",
        "    with torch.no_grad():\n",
        "        for img, cls in dataloader_test:\n",
        "            # 評価用データをGPUへ\n",
        "            img = img.to(device, non_blocking=True)\n",
        "            cls = cls.to(device, non_blocking=True)\n",
        "\n",
        "            # ネットワークに評価用データを入力\n",
        "            logit = cnn_finetune(img)\n",
        "\n",
        "            # 正解したデータ数をカウント\n",
        "            count += torch.sum(logit.argmax(dim=1) == cls).item()\n",
        "\n",
        "        # 評価用データに対する正解率を計算して表示\n",
        "        print(f\"test accuracy: {count/len(dataloader_test.dataset)}\")"
      ],
      "metadata": {
        "id": "lMsu51q4Bfng"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZuAKe0fGAjUi",
        "NmkuNyvlBD4f",
        "zuH23Q5dBfer"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "vscode": {
      "interpreter": {
        "hash": "292b6db79841738ca824380b98b3729806180501b450715886339eec8f0eb9a3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}