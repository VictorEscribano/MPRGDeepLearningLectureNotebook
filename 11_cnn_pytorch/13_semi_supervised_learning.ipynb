{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/13_semi_supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4x9rmDiUTI1"
      },
      "source": [
        "# 半教師あり学習\n",
        "\n",
        "---\n",
        "\n",
        "深層学習ネットワーク (Deep Neural Network, DNN) は，大量のラベルありデータを用いて学習を行うことで高い認識性能を発揮します．\\\n",
        "しかし，教師ラベルは人手によって付与を行うため，データ数に応じて人的・時間的コストが増加します．\\\n",
        "また，問題設定によって必要となるラベル情報が異なります．\\\n",
        "例としてセマンティックセグメンテーションは，画像に対して１ピクセルごとのラベル付けを必要とします．\\\n",
        "これらのことから，多くの場合では理想的なデータセット（大量のラベルありデータ）を用意するのは困難です．\\\n",
        "このような問題を解決する学習方法の１つとして「半教師あり学習」があります．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/cw3nd3iq0ggb01g/sup.png\" width = 57%>\n",
        "\n",
        "半教師あり学習（Semi-supervised Learning）は，ラベル付きデータとラベルなしデータの両方を含むデータセットを利用した学習方法です．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/wljsvzdlb9f9ivn/semi_sup.png\" width = 57%>\n",
        "\n",
        "半教師あり学習として様々な学習方法が提案されていますが，ここではConsistency Regularizationという枠組みについて紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPyzEOSCUTI1"
      },
      "source": [
        "# Consistency Regularization\n",
        "\n",
        "Consistency Regularizationは，ラベルなしデータに対する出力が一貫性を持つようにネットワークを学習をします．\\\n",
        "一貫性とは，同一の画像に対してノイズの付与や幾何学変換の適用などによって出力が変化しないことを表します．\\\n",
        "ネットワークは，ノイズや幾何学変換などに頑健な特徴抽出器を獲得するため，ラベル情報を使用しませんが正解率の向上に寄与します．\\\n",
        "半教師あり学習では，ノイズや幾何学変換などのラベルなしデータに付与するものを「摂動」と呼びます．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/40zfvwq2eksud9k/CR.png\" width = 57%>\n",
        "\n",
        "## 学習方法\n",
        "Consistency Regularizationは，ネットワークに入力されたデータが教師ラベルを持つか否かによって学習の流れが異なります．\n",
        "\n",
        "### ラベルありデータ\n",
        "1. ネットワークにデータを入力\n",
        "2. ネットワークが出力した確率分布と教師ラベルからCross Entropy lossを計算\n",
        "\n",
        "### ラベルなしデータ\n",
        "1. データに対して摂動を付与\n",
        "2. データ（または摂動を付与したデータ）と摂動を付与したデータをネットワークに入力\n",
        "3. ネットワークが出力した２つの確率分布の相違度を計算\n",
        "\n",
        "ネットワークは，「ラベルありデータに対するCross Entropy loss」と「ラベルなしデータに対する確率分布の相違度」を損失関数として学習を行います．\\\n",
        "学習の流れは多くの手法で共通ですが，摂動の内容や摂動の付与方法，確率分布間の相違度の計算方法などによって以下のような様々な手法が提案されています．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/t862ozembfzb1vi/semi_sup_list.png\" width = 65%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaz2xHBVUTI1"
      },
      "source": [
        "# 畳み込みニューラルネットワークの半教師あり学習\n",
        "クラス分類問題において，半教師あり学習によって畳み込みニューラルネットワーク (CNN) を学習します．\n",
        "\n",
        "プログラムの構成は以下の通りです．\n",
        "1. データセットの定義　　　：ラベルありデータのデータセットとラベルなしデータのデータセットを定義します．\n",
        "2. ネットワークの定義　　　：CNNを定義します．\n",
        "3. 教師あり学習による評価　：ラベルありデータセットのみを用いてCNNを教師あり学習します．\n",
        "4. 半教師あり学習による評価：ラベルありデータセットとラベルなしデータセットを用いてCNNを半教師あり学習します．\n",
        "\n",
        "３と４の結果を比較することで，ラベルなしデータの有無による精度の違いを比較します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK1A84wlAedD"
      },
      "source": [
        "## モジュールの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jjnSOmeTtzw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Sampler\n",
        "from torch.utils.data import SubsetRandomSampler, Subset, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25j5APz8F_Tf"
      },
      "source": [
        "## データセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jqF0puazTMK"
      },
      "source": [
        "### データ増幅の定義\n",
        "ラベルありデータに対するデータ増幅，ラベルなしデータに対するデータ増幅（摂動），評価用データに対するデータ増幅の3つを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND3_3dOWUTI1"
      },
      "outputs": [],
      "source": [
        "# ラベルありデータに対するデータ増幅\n",
        "transform_A = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# ラベルなしデータに対するデータ増幅（摂動）\n",
        "transform_B = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.Pad(4, padding_mode=\"reflect\"), \n",
        "    transforms.RandomCrop(32), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# 評価用データに対するデータ増幅\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWfN6vvFUTI1"
      },
      "source": [
        "### ベースとなるデータセットの定義\n",
        "多くの半教師あり学習の論文では，教師あり学習用のベンチマークデータセット（ラベルありデータ）の一部をラベルなしデータと見立てて学習・評価が行われます．\\\n",
        "今回はデータセットとして，一般物体認識用データセットであるCIFAR-10を用います．\\\n",
        "CIFAR-10は，10クラス（飛行機，自動車，鳥，猫，鹿，犬，カエル，馬，船，トラック）の画像から構成されるデータセットです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "b02a38d315a1425e8d19c6c7f587c2fe",
            "e590f9980f4c473b92883a38a1b1a344",
            "b591e51f68474d10bb6bfb3d1b210a32",
            "ac83ba10f1784275b062bba314994fcb",
            "4e9603078af2483aad04a0f621b8721e",
            "08fc43ff7f31442d9cd9f5a2aa67f698",
            "5bd6b9a6379645c4a31036b1eca8aa5c",
            "5a412f7d468e4164938fb07c62c67cff",
            "85c9c84bfbee42fc94a9da6220273f18",
            "26d6181299f34052adb9ca74b8de54f2",
            "4c431554f4c74956be214bf3f0e28651"
          ]
        },
        "id": "Au8la3afUTI1",
        "outputId": "df432e63-98a2-46f6-96af-cb42150377b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/CIFAR-10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b02a38d315a1425e8d19c6c7f587c2fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/CIFAR-10/cifar-10-python.tar.gz to ./dataset/CIFAR-10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# 学習用データ\n",
        "total_trainset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=True,  download=True, transform=transform_A)\n",
        "unsup_trainset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=True,  download=True, transform=None)\n",
        "\n",
        "# 評価用データ\n",
        "testset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=False, download=True, transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x993wPPUTI2"
      },
      "source": [
        "### データセットの分割\n",
        "先程定義したデータセットは，全てのデータにラベルが付与されています．\n",
        "そこで，データセットをラベルありデータとラベルなしデータに分割します．\\\n",
        "ここでは，ラベルありデータとラベルなしデータに割り振るデータのidを決定します．\\\n",
        "ラベルなしデータの数はStratifiedShuffleSplitの引数「test_size」で決定し，ラベルありデータの数は全体のデータ数からtest_sizeで指定したデータ数を引いた値となります．\\\n",
        "今回は，ラベルありデータを1,000サンプル，ラベルなしデータを49,000サンプルとします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5LEgWDCUTI2"
      },
      "outputs": [],
      "source": [
        "# StratifiedShuffleSplit：データをシャッフルして分割\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=49000, random_state=0)\n",
        "\n",
        "# list(range(len(total_trainset)))：データidのリスト，total_trainset.targets：データidに対応するラベル\n",
        "sss = sss.split(list(range(len(total_trainset))), total_trainset.targets)\n",
        "\n",
        "# ラベルありデータとラベルなしデータのデータidを取得\n",
        "label_idx, unlabel_idx = next(sss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR6aeccsUTI2"
      },
      "source": [
        "### Datasetの定義\n",
        "Consistency Regularizationは，ラベルなしデータ１サンプルにつき，摂動を付与した２サンプルのデータを必要とします．\\\n",
        "今回は，この挙動を自作のDatasetクラスによって実現します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40mubfsSwG6D"
      },
      "outputs": [],
      "source": [
        "# 自作 Dataset\n",
        "class UnsupervisedDataset(Dataset):\n",
        "    def __init__(self, dataset, transform_1, transform_2):\n",
        "        self.dataset = dataset          # データセット\n",
        "        self.transform_1 = transform_1  # 摂動１\n",
        "        self.transform_2 = transform_2  # 摂動２\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, _ = self.dataset[index]  # データセットからデータを取得\n",
        "\n",
        "        # 同一画像から2つの画像を作成\n",
        "        img1 = self.transform_1(img)  # データに摂動１を適用\n",
        "        img2 = self.transform_2(img)  # データに摂動２を適用\n",
        "\n",
        "        return img1, img2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)  # このクラスに対してlen()を行った際にデータセットのデータ数を返すように設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyqubj8WUTI2"
      },
      "source": [
        "### データセットの定義\n",
        "「データセットの分割」で取得したデータidを用いてラベルありデータセットとラベルなしデータセットを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QD0dF51wG4M"
      },
      "outputs": [],
      "source": [
        "# ラベルありデータセットを作成（データ増幅はtotal_trainsetで設定したものを実行）\n",
        "trainset = Subset(total_trainset, label_idx)\n",
        "train_labels = [total_trainset.targets[idx] for idx in label_idx]\n",
        "trainset.train_labels = train_labels\n",
        "\n",
        "# ラベルなしデータセットを作成\n",
        "otherset = Subset(unsup_trainset, unlabel_idx)\n",
        "otherset = UnsupervisedDataset(otherset, transform_B, transform_B)  # 第１引数：教師なしデータ，第２引数：摂動１，第３引数：摂動２"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06QrpYgE6X3J",
        "outputId": "fce83f8f-a687-4520-9f7b-7fe0e8d91f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データ数\n",
            "教師付きデータ： 1000\n",
            "教師なしデータ： 49000\n"
          ]
        }
      ],
      "source": [
        "print(\"データ数\")\n",
        "print(\"教師付きデータ：\", len(trainset))\n",
        "print(\"教師なしデータ：\", len(otherset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgDSZ48PUTI3"
      },
      "source": [
        "### Dataloaderの定義\n",
        "ラベルありデータセットのバッチサイズを16とし，ラベルなしデータのバッチサイズはラベルありデータ数とラベルなしデータ数の比から決定します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uin-5_AhwmnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67482889-ca01-4546-99b2-82b35eea15a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ラベルありデータのバッチサイズ： 16\n",
            "ラベルなしデータのバッチサイズ： 784\n",
            "合計バッチサイズ　　　　　　　： 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "sup_batchsize   = 16\n",
        "unsup_batchsize = int(sup_batchsize*(len(otherset)/len(trainset)))\n",
        "print(\"ラベルありデータのバッチサイズ：\", sup_batchsize)\n",
        "print(\"ラベルなしデータのバッチサイズ：\", unsup_batchsize)\n",
        "print(\"合計バッチサイズ　　　　　　　：\", sup_batchsize+unsup_batchsize)\n",
        "\n",
        "# ラベルありデータセットのDataloader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, \n",
        "                                           batch_size=sup_batchsize, \n",
        "                                           shuffle=True,\n",
        "                                           num_workers=16, \n",
        "                                           pin_memory=True,\n",
        "                                           drop_last=True)\n",
        "\n",
        "# ラベルなしデータセットのDataloader\n",
        "unsup_loader = torch.utils.data.DataLoader(otherset, \n",
        "                                           batch_size=unsup_batchsize,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=16, \n",
        "                                           pin_memory=True, \n",
        "                                           drop_last=True)\n",
        "\n",
        "# 評価用データのDataloader\n",
        "test_loader  = torch.utils.data.DataLoader(testset, \n",
        "                                           batch_size=16,\n",
        "                                           shuffle=False, \n",
        "                                           num_workers=16, \n",
        "                                           pin_memory=True, \n",
        "                                           drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ysGfkjygN8G"
      },
      "source": [
        "### データの可視化\n",
        "定義した摂動と自作Datasetクラスによってラベルなしデータへどのような摂動が付与されるのかを確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "zy3f6dD_UTI3",
        "outputId": "a311f740-7acd-47c9-de64-4a0cff03375e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "tmp = unsup_loader.__iter__()  # イテレータを作成\n",
        "data1, data2 = next(tmp)      # ラベルなしデータのDataloaderから摂動を付与した２つのデータを取得\n",
        "\n",
        "# データの可視化\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8,4))\n",
        "\n",
        "ax1.imshow(data1[0].permute(1,2,0))\n",
        "ax2.imshow(data2[0].permute(1,2,0))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwXJszqqkYE3"
      },
      "source": [
        "## ネットワークの定義\n",
        "畳み込みニューラルネットワークを定義します．\\\n",
        "今回は 11.knowledge_distillation や 12.deep_mutual_learning で使用したネットワークと同様の構造を利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj8-_UzOTtz9"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, widen_factor=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16*widen_factor, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16*widen_factor, 32*widen_factor, kernel_size=3, stride=1, padding=1)\n",
        "        self.l1 = nn.Linear(8*8*32*widen_factor, 1024*widen_factor)\n",
        "        self.l2 = nn.Linear(1024*widen_factor, 1024*widen_factor)\n",
        "        self.l3 = nn.Linear(1024*widen_factor, 10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.pool(self.act(self.conv1(x)))\n",
        "        h = self.pool(self.act(self.conv2(h)))\n",
        "        h = h.view(h.size()[0], -1)\n",
        "        h = self.act(self.l1(h))\n",
        "        h = self.act(self.l2(h))\n",
        "        h = self.l3(h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxg52VoDUTI3"
      },
      "source": [
        "## 教師あり学習による学習と評価\n",
        "半教師あり学習の結果と比較するために，ラベルありデータのみを用いた教師あり学習を行います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqK5QGdyUTI3"
      },
      "source": [
        "### 学習条件の設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gNzw4nwUTI3"
      },
      "outputs": [],
      "source": [
        "# 学習回数の設定\n",
        "NUM_EPOCH = 30\n",
        "\n",
        "# ネットワークの用意\n",
        "sup_net = CNN(widen_factor=1).cuda()\n",
        "\n",
        "# 最適化方法の設定\n",
        "optimizer = torch.optim.SGD(sup_net.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEaYI-DyUTI3"
      },
      "source": [
        "### 学習と評価\n",
        "ネットワークを教師あり学習します．\\\n",
        "また，５epochごとに評価用データを用いてネットワークの評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNAzb4D3UTI3",
        "outputId": "22673e59-f9d3-42d6-8d7a-b8ba47012719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.266,            mean accuracy: 0.15,            elapsed_time :7.4\n",
            "epoch: 2,            mean loss: 2.047,            mean accuracy: 0.23,            elapsed_time :8.75\n",
            "epoch: 3,            mean loss: 1.877,            mean accuracy: 0.32,            elapsed_time :10.09\n",
            "epoch: 4,            mean loss: 1.773,            mean accuracy: 0.36,            elapsed_time :11.42\n",
            "epoch: 5,            mean loss: 1.662,            mean accuracy: 0.39,            elapsed_time :12.79\n",
            "test accuracy: 0.3853\n",
            "epoch: 6,            mean loss: 1.448,            mean accuracy: 0.45,            elapsed_time :18.19\n",
            "epoch: 7,            mean loss: 1.28,            mean accuracy: 0.55,            elapsed_time :19.53\n",
            "epoch: 8,            mean loss: 1.05,            mean accuracy: 0.63,            elapsed_time :20.89\n",
            "epoch: 9,            mean loss: 0.792,            mean accuracy: 0.71,            elapsed_time :22.23\n",
            "epoch: 10,            mean loss: 0.637,            mean accuracy: 0.77,            elapsed_time :23.58\n",
            "test accuracy: 0.3945\n",
            "epoch: 11,            mean loss: 0.494,            mean accuracy: 0.82,            elapsed_time :29.58\n",
            "epoch: 12,            mean loss: 0.269,            mean accuracy: 0.91,            elapsed_time :30.9\n",
            "epoch: 13,            mean loss: 0.293,            mean accuracy: 0.89,            elapsed_time :32.24\n",
            "epoch: 14,            mean loss: 0.161,            mean accuracy: 0.93,            elapsed_time :33.58\n",
            "epoch: 15,            mean loss: 0.233,            mean accuracy: 0.91,            elapsed_time :34.89\n",
            "test accuracy: 0.4018\n",
            "epoch: 16,            mean loss: 0.076,            mean accuracy: 0.97,            elapsed_time :40.3\n",
            "epoch: 17,            mean loss: 0.051,            mean accuracy: 0.97,            elapsed_time :41.64\n",
            "epoch: 18,            mean loss: 0.05,            mean accuracy: 0.98,            elapsed_time :43.0\n",
            "epoch: 19,            mean loss: 0.048,            mean accuracy: 0.98,            elapsed_time :44.33\n",
            "epoch: 20,            mean loss: 0.059,            mean accuracy: 0.98,            elapsed_time :45.64\n",
            "test accuracy: 0.4102\n",
            "epoch: 21,            mean loss: 0.048,            mean accuracy: 0.98,            elapsed_time :50.96\n",
            "epoch: 22,            mean loss: 0.036,            mean accuracy: 0.98,            elapsed_time :52.3\n",
            "epoch: 23,            mean loss: 0.052,            mean accuracy: 0.98,            elapsed_time :53.65\n",
            "epoch: 24,            mean loss: 0.038,            mean accuracy: 0.98,            elapsed_time :54.97\n",
            "epoch: 25,            mean loss: 0.002,            mean accuracy: 0.99,            elapsed_time :56.32\n",
            "test accuracy: 0.4156\n",
            "epoch: 26,            mean loss: 0.001,            mean accuracy: 0.99,            elapsed_time :61.71\n",
            "epoch: 27,            mean loss: 0.001,            mean accuracy: 0.99,            elapsed_time :63.05\n",
            "epoch: 28,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :64.4\n",
            "epoch: 29,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :66.2\n",
            "epoch: 30,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :67.55\n",
            "test accuracy: 0.4213\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # ネットワークの学習 ----------------------------------------------------------\n",
        "    # ネットワークを学習モードへ変更\n",
        "    sup_net.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        # 学習用データをGPUへ\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # 学習用データをネットワークへ入力\n",
        "        logits = sup_net(image)\n",
        "        \n",
        "        # 損失の計算(教師付きデータ)\n",
        "        loss = F.cross_entropy(logits, label)\n",
        "        \n",
        "        # パラメータの更新\n",
        "        sup_net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        pred   = torch.argmax(logits, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "        \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch},\\\n",
        "            mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
        "            mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
        "            elapsed_time :{round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークの評価 ----------------------------------------------------------\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
        "        # ネットワークを評価モードへ変更\n",
        "        sup_net.eval()\n",
        "\n",
        "        # 評価の実行\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in test_loader:\n",
        "                # 評価用データをGPUへ\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "                \n",
        "                # 評価用データをネットワークへ入力\n",
        "                logits = sup_net(image)\n",
        "                \n",
        "                # 正解したデータ数を取得\n",
        "                pred   = torch.argmax(logits, dim=1)\n",
        "                count += torch.sum(pred == label)\n",
        "\n",
        "        # 評価結果の表示\n",
        "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW3VIvsZTt0G"
      },
      "source": [
        "## 半教師あり学習による学習と評価\n",
        "logits間の2乗誤差をラベルなしデータに対する損失式として半教師あり学習を行います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9husGpYUTI3"
      },
      "source": [
        "### 学習条件の設定\n",
        "ラベルなしデータに対する損失値に重み付け (w_unlabel) を行うことで，ラベルありデータを用いた学習とラベルなしデータを用いた学習のどちらに重きを置くのかを制御します．\\\n",
        "今回は，重みとして１を使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3n7ec4JUTI3"
      },
      "outputs": [],
      "source": [
        "# 学習回数の設定\n",
        "NUM_EPOCH = 30\n",
        "\n",
        "# 教師なしデータの損失に対する重み付け\n",
        "w_unlabel = 1.0\n",
        "\n",
        "# ネットワークの用意\n",
        "net = CNN(widen_factor=1).cuda()\n",
        "\n",
        "# 最適化方法の設定\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvH9lACsUTI3"
      },
      "source": [
        "### 学習と評価\n",
        "ネットワークを半教師あり学習します．\\\n",
        "ラベルありデータはfor文，ラベルなしデータはイテレータを作成してデータを取り出します，\\\n",
        "また，５epochごとに評価用データを用いてネットワークの評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPUL3binTt0H",
        "outputId": "abb1023e-797d-4f27-97b3-60e42212bf8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,             mean loss: 2.277,             label loss: 2.275,             unlabel loss: 0.002,             mean accuracy: 0.15,             elapsed_time :41.58\n",
            "epoch: 2,             mean loss: 2.108,             label loss: 2.08,             unlabel loss: 0.028,             mean accuracy: 0.25,             elapsed_time :81.21\n",
            "epoch: 3,             mean loss: 2.006,             label loss: 1.963,             unlabel loss: 0.043,             mean accuracy: 0.3,             elapsed_time :120.77\n",
            "epoch: 4,             mean loss: 1.861,             label loss: 1.797,             unlabel loss: 0.063,             mean accuracy: 0.36,             elapsed_time :160.42\n",
            "epoch: 5,             mean loss: 1.754,             label loss: 1.671,             unlabel loss: 0.083,             mean accuracy: 0.4,             elapsed_time :202.5\n",
            "test accuracy: 0.3889\n",
            "epoch: 6,             mean loss: 1.674,             label loss: 1.562,             unlabel loss: 0.113,             mean accuracy: 0.47,             elapsed_time :247.08\n",
            "epoch: 7,             mean loss: 1.505,             label loss: 1.372,             unlabel loss: 0.134,             mean accuracy: 0.52,             elapsed_time :286.83\n",
            "epoch: 8,             mean loss: 1.388,             label loss: 1.226,             unlabel loss: 0.161,             mean accuracy: 0.59,             elapsed_time :326.97\n",
            "epoch: 9,             mean loss: 1.189,             label loss: 0.993,             unlabel loss: 0.196,             mean accuracy: 0.69,             elapsed_time :367.28\n",
            "epoch: 10,             mean loss: 1.048,             label loss: 0.828,             unlabel loss: 0.22,             mean accuracy: 0.78,             elapsed_time :409.63\n",
            "test accuracy: 0.4075\n",
            "epoch: 11,             mean loss: 0.843,             label loss: 0.608,             unlabel loss: 0.235,             mean accuracy: 0.88,             elapsed_time :454.6\n",
            "epoch: 12,             mean loss: 0.672,             label loss: 0.437,             unlabel loss: 0.235,             mean accuracy: 0.94,             elapsed_time :495.28\n",
            "epoch: 13,             mean loss: 0.504,             label loss: 0.286,             unlabel loss: 0.218,             mean accuracy: 0.97,             elapsed_time :535.76\n",
            "epoch: 14,             mean loss: 0.365,             label loss: 0.18,             unlabel loss: 0.186,             mean accuracy: 0.98,             elapsed_time :578.02\n",
            "epoch: 15,             mean loss: 0.279,             label loss: 0.123,             unlabel loss: 0.156,             mean accuracy: 0.99,             elapsed_time :619.27\n",
            "test accuracy: 0.4922\n",
            "epoch: 16,             mean loss: 0.22,             label loss: 0.09,             unlabel loss: 0.13,             mean accuracy: 0.99,             elapsed_time :664.43\n",
            "epoch: 17,             mean loss: 0.174,             label loss: 0.067,             unlabel loss: 0.107,             mean accuracy: 0.99,             elapsed_time :705.52\n",
            "epoch: 18,             mean loss: 0.142,             label loss: 0.053,             unlabel loss: 0.089,             mean accuracy: 0.99,             elapsed_time :746.3\n",
            "epoch: 19,             mean loss: 0.122,             label loss: 0.046,             unlabel loss: 0.076,             mean accuracy: 0.99,             elapsed_time :789.16\n",
            "epoch: 20,             mean loss: 0.102,             label loss: 0.038,             unlabel loss: 0.064,             mean accuracy: 0.99,             elapsed_time :829.79\n",
            "test accuracy: 0.4427\n",
            "epoch: 21,             mean loss: 0.101,             label loss: 0.043,             unlabel loss: 0.057,             mean accuracy: 0.99,             elapsed_time :875.1\n",
            "epoch: 22,             mean loss: 0.069,             label loss: 0.025,             unlabel loss: 0.044,             mean accuracy: 0.99,             elapsed_time :918.3\n",
            "epoch: 23,             mean loss: 0.056,             label loss: 0.02,             unlabel loss: 0.036,             mean accuracy: 0.99,             elapsed_time :961.1\n",
            "epoch: 24,             mean loss: 0.046,             label loss: 0.016,             unlabel loss: 0.03,             mean accuracy: 0.99,             elapsed_time :1001.89\n",
            "epoch: 25,             mean loss: 0.039,             label loss: 0.014,             unlabel loss: 0.025,             mean accuracy: 0.99,             elapsed_time :1042.65\n",
            "test accuracy: 0.3266\n",
            "epoch: 26,             mean loss: 0.034,             label loss: 0.012,             unlabel loss: 0.022,             mean accuracy: 0.99,             elapsed_time :1087.97\n",
            "epoch: 27,             mean loss: 0.03,             label loss: 0.011,             unlabel loss: 0.019,             mean accuracy: 0.99,             elapsed_time :1130.96\n",
            "epoch: 28,             mean loss: 0.027,             label loss: 0.009,             unlabel loss: 0.017,             mean accuracy: 0.99,             elapsed_time :1172.17\n",
            "epoch: 29,             mean loss: 0.023,             label loss: 0.008,             unlabel loss: 0.015,             mean accuracy: 0.99,             elapsed_time :1212.8\n",
            "epoch: 30,             mean loss: 0.02,             label loss: 0.007,             unlabel loss: 0.013,             mean accuracy: 0.99,             elapsed_time :1253.59\n",
            "test accuracy: 0.2554\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # ネットワークの学習 ----------------------------------------------------------\n",
        "    # ネットワークを学習モードへ変更\n",
        "    net.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss_sup   = 0.0\n",
        "    sum_loss_unsup = 0.0\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    # 教師なし用Dataloaderの設定（イテレータの作成）\n",
        "    iter_u = iter(unsup_loader)\n",
        "    \n",
        "    for img, label in train_loader:  # ラベルありデータの取得\n",
        "        # ラベルなしデータの取得\n",
        "        try:\n",
        "            unlabel_img1, unlabel_img2 = next(iter_u)\n",
        "        except StopIteration:\n",
        "            iter_u = iter(unsup_loader)\n",
        "            unlabel_img1, unlabel_img2 = next(iter_u)\n",
        "\n",
        "        # 学習用データをGPUへ\n",
        "        image = torch.cat([img, unlabel_img1, unlabel_img2]).cuda()  # ラベルありデータとラベルなしデータを1つに\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # 学習用データをネットワークへ入力\n",
        "        logits_all = net(image)\n",
        "        \n",
        "        # 損失の計算(ラベルありデータ)\n",
        "        logits = logits_all[:len(img)]  # ラベルありデータに対する出力を取得\n",
        "        loss_sup = F.cross_entropy(logits, label)  # ラベルありデータに対する損失を計算 (Cross Entropy loss)\n",
        "\n",
        "        # 損失の計算(ラベルなしデータ)\n",
        "        logits_unsup = logits_all[len(img):]  # ラベルなしデータに対する出力を取得\n",
        "        logits1, logits2 = torch.chunk(logits_unsup, 2)  # 摂動を付与した2サンプルに対する出力を取得\n",
        "\n",
        "        loss_unsup = F.mse_loss(logits2, logits1)  # ラベルなしデータに対する損失を計算\n",
        "\n",
        "        # 最終的な損失を計算\n",
        "        loss = loss_sup + w_unlabel*loss_unsup\n",
        "        \n",
        "        # パラメータの更新\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()               # 損失の合計値\n",
        "        sum_loss_sup   += loss_sup.item()     # ラベルありデータに対する損失\n",
        "        sum_loss_unsup += loss_unsup.item()   # ラベルなしデータに対する損失\n",
        "        \n",
        "        pred   = torch.argmax(logits, dim=1)  # 値の最も高いクラスの抽出\n",
        "        count += torch.sum(pred == label)     # 正解したデータの数をカウント\n",
        "        \n",
        "    # ログの表示\n",
        "    print( f\"epoch: {epoch},\\\n",
        "             mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
        "             label loss: {round(sum_loss_sup/len(train_loader), 3)},\\\n",
        "             unlabel loss: {round(w_unlabel*sum_loss_unsup/len(train_loader), 3)},\\\n",
        "             mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
        "             elapsed_time :{round(time()-start, 2)}\" )\n",
        "\n",
        "    # ネットワークの評価 ----------------------------------------------------------\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
        "        # ネットワークを評価モードへ変更\n",
        "        net.eval()\n",
        "\n",
        "        # 評価の実行\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in test_loader:\n",
        "                # 評価用データをGPUへ\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "                \n",
        "                # 評価用データをネットワークへ入力\n",
        "                logits = net(image)\n",
        "                \n",
        "                # 正解したデータ数を取得\n",
        "                pred   = torch.argmax(logits, dim=1)\n",
        "                count += torch.sum(pred == label)\n",
        "\n",
        "        # 評価結果の表示\n",
        "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQPab9W0Tt0y"
      },
      "source": [
        "# 課題\n",
        "1. ラベルなしデータに対する損失設計を変更してみましょう\n",
        "2. ラベルありデータとラベルなしデータの比率を変更してみましょう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZZ1AFr2MVIr"
      },
      "source": [
        "#### ラベルなしデータに対する損失設計の例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cseMPETNMVIr"
      },
      "outputs": [],
      "source": [
        "# 確率分布に対するKL-divergence\n",
        "def kl_divergence(logits_1, logits_2):\n",
        "    softmax_1 = F.softmax(logits_1, dim=1)\n",
        "    softmax_2 = F.softmax(logits_2, dim=1)\n",
        "    kl = (softmax_2 * torch.log((softmax_2 / (softmax_1+1e-10)) + 1e-10)).sum(dim=1)\n",
        "    return kl.mean()\n",
        "\n",
        "loss_unsup = (kl_divergence(logits1, logits2)+kl_divergence(logits2, logits1))/2  # KL-divergenceは距離の公理を満たさない指標のため双方向の平均を計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5L4cLDFMVIr"
      },
      "outputs": [],
      "source": [
        "# 確率分布に対する平均2乗誤差\n",
        "loss_unsup = F.mse_loss(F.softmax(logits2, dim=1), F.softmax(logits1, dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm_j5xkhMVIr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "toc-autonumbering": false,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b02a38d315a1425e8d19c6c7f587c2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e590f9980f4c473b92883a38a1b1a344",
              "IPY_MODEL_b591e51f68474d10bb6bfb3d1b210a32",
              "IPY_MODEL_ac83ba10f1784275b062bba314994fcb"
            ],
            "layout": "IPY_MODEL_4e9603078af2483aad04a0f621b8721e"
          }
        },
        "e590f9980f4c473b92883a38a1b1a344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fc43ff7f31442d9cd9f5a2aa67f698",
            "placeholder": "​",
            "style": "IPY_MODEL_5bd6b9a6379645c4a31036b1eca8aa5c",
            "value": "100%"
          }
        },
        "b591e51f68474d10bb6bfb3d1b210a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a412f7d468e4164938fb07c62c67cff",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85c9c84bfbee42fc94a9da6220273f18",
            "value": 170498071
          }
        },
        "ac83ba10f1784275b062bba314994fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d6181299f34052adb9ca74b8de54f2",
            "placeholder": "​",
            "style": "IPY_MODEL_4c431554f4c74956be214bf3f0e28651",
            "value": " 170498071/170498071 [00:13&lt;00:00, 12314043.14it/s]"
          }
        },
        "4e9603078af2483aad04a0f621b8721e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08fc43ff7f31442d9cd9f5a2aa67f698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd6b9a6379645c4a31036b1eca8aa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a412f7d468e4164938fb07c62c67cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c9c84bfbee42fc94a9da6220273f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26d6181299f34052adb9ca74b8de54f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c431554f4c74956be214bf3f0e28651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
