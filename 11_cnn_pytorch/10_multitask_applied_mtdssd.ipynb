{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/10_multitask_applied_mtdssd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSdX7WVvzGVN"
      },
      "source": [
        "# マルチタスク学習応用（検出＋セグメンテーション）\n",
        "---\n",
        "\n",
        "# 目的\n",
        "\n",
        "これまでに学習したSSD・SegNet・マルチタスク学習を応用し，物体検出タスクとセマンティックセグメンテーションタスクを同時に解くネットワークを実際に作成する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6TTO8qBFiB"
      },
      "source": [
        "## 概要\n",
        "物体検出とセマンティックセグメンテーションを行うモデルとして，Multi-task Deconvolutional Single Shot Detector（MT-DSSD） [1] があります．MT-DSSDは，SSD [2] にDeconvolution層を追加したDeconvolutional Single Shot Detector（DSSD） [3] にセグメンテーションを行うブランチを追加したものです．各ブランチで物体識別，物体候補領域の検出，セマンティックセグメンテーションを行うことで，単一のモデルで物体検出とセマンティックセグメンテーションタスクを同時に解くことができます．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3FoISjQduuR"
      },
      "source": [
        "# 準備\n",
        "## Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います． GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdfN8rBnCbzq"
      },
      "source": [
        "# データセットの用意\n",
        "今回もSSDおよびSegNetと同様に，物体検出タスクの性能評価に用いられるARC2017 RGB-Dデータセットを使用します．ARC2017 RGB-Dデータセットは2017年にAmazon Robotics Challengeにて使用された全40クラスの物体を収録する物体検出のデータセットです．\n",
        "ARC2017 RGB-Dデータセットの詳細については以下サイトを参照してください．\n",
        "http://mprg.jp/research/arc_dataset_2017_j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRqMrOVCCjQg"
      },
      "outputs": [],
      "source": [
        "# データのダウンロード\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1oJs_uRkCG2eWBKKY7ekVJp2vlCZmTF1h', 'ARCdataset_png.zip', quiet=True)\n",
        "!unzip -q ARCdataset_png.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvdvyk56KGLd"
      },
      "source": [
        "データセットの中身はSSD・SegNetと同様なので，詳しくは省略します．\n",
        "\n",
        "ダウンロードしたデータの中身を確認します．\n",
        "フォルダARCdataset_pngには以下のフォルダ・ファイルが内包されています． \\\\\n",
        "- train ← 学習用データ\n",
        "- test ← 評価用データ\n",
        "- SSD_pretrained.pth ← 学習済みモデル \\\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXiAu2RKExAG"
      },
      "source": [
        "作業ディレクトリを先程ダウンロードしたフォルダに移動します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp5D2D51wXxB"
      },
      "outputs": [],
      "source": [
        "# 作業ディレクトリの移動\n",
        "%cd ARCdataset_png_for_MTDSSD\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DFz0Hv8Sal"
      },
      "source": [
        "## モジュールのインポート\n",
        "SSDと同様に，プログラムを実行するために必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnEFb8Ef85VA"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "\n",
        "import math # 数学ライブラリ\n",
        "import random # 乱数生成\n",
        "import cv2 as cv    # 画像処理\n",
        "import numpy as np  # 計算\n",
        "import torch        # 機械学習ライブラリ\n",
        "import torchsummary # ネットワーク情報表示\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K16YK_yYOMQN"
      },
      "source": [
        "実験はGPUを使用するので，colaboratory上で使用可能な状態か確認します．\n",
        "`device:cuda`と表示されない場合は，上記の「Google Colaboratoryの設定確認・変更」を確認してください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsIXikhJOmjT"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print('device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sQTzK7VOrFQ"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcEP0IK-9rvW"
      },
      "source": [
        "最初に，評価する画像が入ったフォルダやネットワークのハイパーパラメータを定義します．後に作成するネットワーク定義やテストプログラムからも，値を参照できるようにしておきます．\n",
        "\n",
        "ほとんどSSDと同じですが，セグメンテーション用の教師ラベルリストのパスを指定する`self.augmented_seglabel_list`が追加されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytLrtgmL0NuG"
      },
      "outputs": [],
      "source": [
        "class common:\n",
        "  def __init__(self):\n",
        "    # 学習画像のディレクトリ\n",
        "    self.images_dir = \".\"\n",
        "\n",
        "    # Data Augmentationにより作成されたデータのリストパス（入力画像）\n",
        "    self.augmented_img_list = \"./train/augimg_name_list.txt\"\n",
        "\n",
        "    # Data Augmentationにより作成されたデータのリストパス（セグメンテーション）\n",
        "    self.augmented_seglabel_list = \"./train/seglabel_name_list.txt\"\n",
        "\n",
        "    # SSDの入力画像サイズ\n",
        "    self.insize = 300\n",
        "\n",
        "    # 識別のクラス数 (背景込み)\n",
        "    self.num_of_classes = 41\n",
        "\n",
        "    # Bounding boxのオフセットベクトルの次元数\n",
        "    self.num_of_offset_dims = 4\n",
        "\n",
        "    # Bounding boxのオフセットとクラスを推定する畳み込み層\n",
        "    self.mbox_source_layers = ['conv4_3', 'conv7', 'conv8_2', 'conv9_2', 'conv10_2', 'conv11_2']\n",
        "\n",
        "    # Default boxの最小・最大比率 (in percent %)\n",
        "    self.min_ratio = 20\n",
        "    self.max_ratio = 90\n",
        "\n",
        "    # 各階層における特徴マップの入力画像上のステップ幅\n",
        "    self.steps = [8, 16, 32, 64, 100, 300]\n",
        "\n",
        "    # 各階層のdefault boxの数\n",
        "    self.num_boxes = [4, 6, 6, 6, 4, 4]\n",
        "\n",
        "    # 各階層の特徴マップのDefault boxのアスペクト比\n",
        "    self.aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
        "\n",
        "    # 各階層の特徴マップの解像度\n",
        "    self.map_sizes = [38, 19, 10, 5, 3, 1]\n",
        "\n",
        "    self.loc_var = 0.01\n",
        "\n",
        "    # Data Augmentationで増加させる倍数\n",
        "    self.augmentation_factor = 2\n",
        "\n",
        "    # Data Augmentationのパラメータ\n",
        "    self.jitter = 0.3      # Default : 0.2\n",
        "    self.saturation = 1.5  # Default : 1.5\n",
        "    self.exposure = 1.5    # Default : 1.5\n",
        "    self.hue = 0.03        # Default : 0.03\n",
        "\n",
        "    # choose border type\n",
        "    # BORDER_REPLICATE, BORDER_REFLECT, BORDER_REFLECT_101, BORDER_WRAP, BORDER_CONSTANT\n",
        "    self.border_type = cv.BORDER_CONSTANT\n",
        "\n",
        "    self.border_val = (127, 127, 127)\n",
        "\n",
        "    # ARCクラスラベル (クラス名にはスペース(空白)は禁止)\n",
        "    self.arc_labels = [\n",
        "          \"Background\",             #0\n",
        "          \"Binder\",                 #1\n",
        "          \"Balloons\",               #2\n",
        "          \"Baby_Wipes\",             #3\n",
        "          \"Toilet_Brush\",           #4\n",
        "          \"Toothbrushes\",           #5\n",
        "          \"Crayons\",                #6\n",
        "          \"Salts\",                  #7\n",
        "          \"DVD\",                    #8\n",
        "          \"Glue_Sticks\",            #9\n",
        "          \"Eraser\",                 #10\n",
        "          \"Scissors\",               #11\n",
        "          \"Green_Book\",             #12\n",
        "          \"Socks\",                  #13\n",
        "          \"Irish_Spring\",           #14\n",
        "          \"Paper_Tape\",             #15\n",
        "          \"Touch_Tissues\",          #16\n",
        "          \"Knit_Gloves\",            #17\n",
        "          \"Laugh_Out_Loud_Jokes\",   #18\n",
        "          \"Pencil_Cup\",             #19\n",
        "          \"Mini_Marbles\",           #20\n",
        "          \"Neoprene_Weight\",        #21\n",
        "          \"Wine_Glasses\",           #22\n",
        "          \"Water_Bottle\",           #23\n",
        "          \"Reynolds_Pie\",           #24\n",
        "          \"Reynolds_Wrap\",          #25\n",
        "          \"Robots_Everywhere\",      #26\n",
        "          \"Duct_Tape\",              #27\n",
        "          \"Sponges\",                #28\n",
        "          \"Speed_Stick\",            #29\n",
        "          \"Index_Cards\",            #30\n",
        "          \"Ice_Cube_Tray\",          #31\n",
        "          \"Table_Cover\",            #32\n",
        "          \"Measuring_Spoons\",       #33\n",
        "          \"Bath_Sponge\",            #34\n",
        "          \"Pencils\",                #35\n",
        "          \"Mousetraps\",             #36\n",
        "          \"Face_Cloth\",             #37\n",
        "          \"Tennis_Balls\",           #38\n",
        "          \"Spray_Bottle\",           #39\n",
        "          \"Flashlights\"]            #40\n",
        "\n",
        "    # アイテムIDリスト\n",
        "    self.itemIDList = [\n",
        "                  \"0 BG\",\n",
        "                  \"1\",\n",
        "                  \"2\",\n",
        "                  \"3\",\n",
        "                  \"4\",\n",
        "                  \"5\",\n",
        "                  \"6\",\n",
        "                  \"7\",\n",
        "                  \"8\",\n",
        "                  \"9\",\n",
        "                  \"10\",\n",
        "                  \"11\",\n",
        "                  \"12\",\n",
        "                  \"13\",\n",
        "                  \"14\",\n",
        "                  \"15\",\n",
        "                  \"16\",\n",
        "                  \"17\",\n",
        "                  \"18\",\n",
        "                  \"19\",\n",
        "                  \"20\",\n",
        "                  \"21\",\n",
        "                  \"22\",\n",
        "                  \"23\",\n",
        "                  \"24\",\n",
        "                  \"25\",\n",
        "                  \"26\",\n",
        "                  \"27\",\n",
        "                  \"28\",\n",
        "                  \"29\",\n",
        "                  \"30\",\n",
        "                  \"31\",\n",
        "                  \"32\",\n",
        "                  \"33\",\n",
        "                  \"34\",\n",
        "                  \"35\",\n",
        "                  \"36\",\n",
        "                  \"37\",\n",
        "                  \"38\",\n",
        "                  \"39\",\n",
        "                  \"40\",\n",
        "                  \"41\"]\n",
        "\n",
        "    # クラスの色\n",
        "    self.arc_class_color = np.array([\n",
        "               [  0,   0,   0],\n",
        "               [ 85,   0,   0],\n",
        "               [170,   0,   0],\n",
        "               [255,   0,   0],\n",
        "               [  0,  85,   0],\n",
        "               [ 85,  85,   0],\n",
        "               [170,  85,   0],\n",
        "               [255,  85,   0],\n",
        "               [  0, 170,   0],\n",
        "               [ 85, 170,   0],\n",
        "               [170, 170,   0],\n",
        "               [255, 170,   0],\n",
        "               [  0, 255,   0],\n",
        "               [ 85, 255,   0],\n",
        "               [170, 255,   0],\n",
        "               [255, 255,   0],\n",
        "               [  0,   0,  85],\n",
        "               [ 85,   0,  85],\n",
        "               [170,   0,  85],\n",
        "               [255,   0,  85],\n",
        "               [  0,  85,  85],\n",
        "               [ 85,  85,  85],\n",
        "               [170,  85,  85],\n",
        "               [255,  85,  85],\n",
        "               [  0, 170,  85],\n",
        "               [ 85, 170,  85],\n",
        "               [170, 170,  85],\n",
        "               [255, 170,  85],\n",
        "               [  0, 255,  85],\n",
        "               [ 85, 255,  85],\n",
        "               [170, 255,  85],\n",
        "               [255, 255,  85],\n",
        "               [  0,   0, 170],\n",
        "               [ 85,   0, 170],\n",
        "               [170,   0, 170],\n",
        "               [255,   0, 170],\n",
        "               [  0,  85, 170],\n",
        "               [ 85,  85, 170],\n",
        "               [170,  85, 170],\n",
        "               [255,  85, 170],\n",
        "               [  0, 170, 170]])\n",
        "\n",
        "common_params = common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjhcl8TB-Onr"
      },
      "source": [
        "## データ拡張\n",
        "Data Augmentationの処理を記述します．trainフォルダ内にデータ拡張済みのBoundingBoxファイルが用意されており，データ拡張済みのファイルが読み込まれた際に画像に対してData Augmentation処理が実行されます．\\\n",
        "読み込まれたRGB画像は一度HSV形式に変換され，歪みを加えたRGB画像を新たに生成します．\n",
        "\n",
        "この部分も基本的にはSSDと同じですが，セグメンテーションの教師ラベルを取り扱う処理が追加されています．セグメンテーションの教師ラベルはdata augmentation時に色を変化させてはいけないため，crop処理のみ追加します．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNXjQo_jERu8"
      },
      "outputs": [],
      "source": [
        "# BGR -> HSV\n",
        "def BGR_2_HSV_(img):\n",
        "\n",
        "    h = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "    s = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "    v = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "\n",
        "    img_r = img.copy()\n",
        "    b = img[:, :, 0]\n",
        "    g = img[:, :, 1]\n",
        "    r = img[:, :, 2]\n",
        "    max_v = cv.max(cv.max(b, g), r)\n",
        "    min_v = cv.min(cv.min(b, g), r)\n",
        "    delta = max_v - min_v\n",
        "    v = max_v\n",
        "    zero_m = (max_v == 0.)\n",
        "    zero_m = zero_m.astype(np.float32)\n",
        "    nonzeor_m = (max_v != 0.)\n",
        "    nonzeor_m = nonzeor_m.astype(np.float32)\n",
        "\n",
        "    exp_m = zero_m * 10e-8\n",
        "\n",
        "    s = delta / (max_v + exp_m)\n",
        "    s = s * nonzeor_m\n",
        "\n",
        "    rmax = (r == max_v)\n",
        "    rmax = rmax.astype(np.float32)\n",
        "\n",
        "    gmax = (g == max_v)\n",
        "    gr = (g != r)\n",
        "    gmax = gmax.astype(np.float32)\n",
        "    gr = gr.astype(np.float32)\n",
        "    gmax = gmax * gr\n",
        "\n",
        "    bmax = (b == max_v)\n",
        "    br = (b != r)\n",
        "    bg = (b != g)\n",
        "    bmax = bmax.astype(np.float32)\n",
        "    br = br.astype(np.float32)\n",
        "    bg = bg.astype(np.float32)\n",
        "    bmax = bmax * br * bg\n",
        "\n",
        "    h += ((g - b) / (delta + 10e-8)) * rmax\n",
        "    h += (((b - r) / (delta + 10e-8)) + 2.) * gmax\n",
        "    h += (((r - g) / (delta + 10e-8)) + 4.) * bmax\n",
        "\n",
        "    h = h * (np.pi / 3.)\n",
        "\n",
        "    neg_m = (h < 0.0)\n",
        "    neg_m = neg_m.astype(np.float32)\n",
        "    h += neg_m * (2. * np.pi)\n",
        "\n",
        "    h = h / (2. * np.pi)\n",
        "\n",
        "    img_r[:, :, 0] = h\n",
        "    img_r[:, :, 1] = s\n",
        "    img_r[:, :, 2] = v\n",
        "\n",
        "    return img_r\n",
        "\n",
        "# HSV -> BGR\n",
        "def HSV_2_BGR_(img):\n",
        "\n",
        "    r = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "    g = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "    b = np.zeros((img.shape[0], img.shape[1]), np.float32)\n",
        "\n",
        "    img_r = img.copy()\n",
        "    h = img[:, :, 0]\n",
        "    s = img[:, :, 1]\n",
        "    v = img[:, :, 2]\n",
        "\n",
        "    h = h * (2. * np.pi)\n",
        "\n",
        "    zero_m = (s == 0.)\n",
        "    zero_m = zero_m.astype(np.float32)\n",
        "    nonzeor_m = (s != 0.)\n",
        "    nonzeor_m = nonzeor_m.astype(np.float32)\n",
        "\n",
        "    idx = np.floor(h)\n",
        "    idx = idx.astype(np.int16)\n",
        "\n",
        "    f = h - idx\n",
        "    p = v * (1. - s)\n",
        "    q = v * (1. - s * ((3. / np.pi) * f))\n",
        "    t = v * (1. - s * (1. - ((3. / np.pi) * f)))\n",
        "\n",
        "    idx0 = (idx == 0)\n",
        "    idx0 = idx0.astype(np.float32)\n",
        "\n",
        "    idx1 = (idx == 1)\n",
        "    idx1 = idx1.astype(np.float32)\n",
        "\n",
        "    idx2 = (idx == 2)\n",
        "    idx2 = idx2.astype(np.float32)\n",
        "\n",
        "    idx3 = (idx == 3)\n",
        "    idx3 = idx3.astype(np.float32)\n",
        "\n",
        "    idx4 = (idx == 4)\n",
        "    idx4 = idx4.astype(np.float32)\n",
        "\n",
        "    idxE = idx0 + idx1 + idx2 + idx3 + idx4\n",
        "    idxE = (idxE == 0)\n",
        "    idxE = idxE.astype(np.float32)\n",
        "\n",
        "    r += v * idx0\n",
        "    g += t * idx0\n",
        "    b += p * idx0\n",
        "\n",
        "    r += q * idx1\n",
        "    g += v * idx1\n",
        "    b += p * idx1\n",
        "\n",
        "    r += p * idx2\n",
        "    g += v * idx2\n",
        "    b += t * idx2\n",
        "\n",
        "    r += p * idx3\n",
        "    g += q * idx3\n",
        "    b += v * idx3\n",
        "\n",
        "    r += t * idx4\n",
        "    g += p * idx4\n",
        "    b += v * idx4\n",
        "\n",
        "    r += v * idxE\n",
        "    g += p * idxE\n",
        "    b += q * idxE\n",
        "\n",
        "    r = r * nonzeor_m\n",
        "    g = g * nonzeor_m\n",
        "    b = b * nonzeor_m\n",
        "\n",
        "    r += v * zero_m\n",
        "    g += v * zero_m\n",
        "    b += v * zero_m\n",
        "\n",
        "    img_r[:, :, 0] = b\n",
        "    img_r[:, :, 1] = g\n",
        "    img_r[:, :, 2] = r\n",
        "\n",
        "    return img_r\n",
        "\n",
        "# 歪みをくわえた画像を生成\n",
        "def distortImage(img, dhue, dsat, dexp):\n",
        "\n",
        "    img = BGR_2_HSV_(img)\n",
        "\n",
        "    n = img.shape[0] * img.shape[1]\n",
        "\n",
        "    img[:, :, 0] = img[:, :, 0] + dhue\n",
        "    img[:, :, 1] = img[:, :, 1] * dsat\n",
        "    img[:, :, 2] = img[:, :, 2] * dexp\n",
        "\n",
        "    m = img[:, :, 0] > 1.\n",
        "    m = m.astype(np.float32)\n",
        "    p = img[:, :, 0] < 0.\n",
        "    p = p.astype(np.float32)\n",
        "\n",
        "    img[:, :, 0] = img[:, :, 0] - m\n",
        "    img[:, :, 0] = img[:, :, 0] + p\n",
        "\n",
        "    img = HSV_2_BGR_(img)\n",
        "\n",
        "    img = np.minimum(np.maximum(img, 0.), 1.)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# augmentation処理\n",
        "def trainAugmentation(img, border_pixels, crop_param, hsv_param, flip_type, segimg):\n",
        "    hsv_srand = 0\n",
        "\n",
        "    if hsv_srand == 1:\n",
        "        dhue = hsv_param[0]\n",
        "        dsat = hsv_param[1]\n",
        "        dexp = hsv_param[2]\n",
        "    else:\n",
        "        dhue = random.uniform(-1. * common_params.hue, common_params.hue)\n",
        "        dsat = random.uniform(1., common_params.saturation)\n",
        "        dexp = random.uniform(1., common_params.exposure)\n",
        "        if random.randint(0, 1) == 0:\n",
        "            dsat = 1. / dsat\n",
        "        if random.randint(0, 1) == 0:\n",
        "            dexp = 1. / dexp\n",
        "\n",
        "    if random.randint(0, 4) >= 1:\n",
        "        img = img.astype(np.float32)\n",
        "        img /= 255.\n",
        "        img = distortImage(img, dhue, dsat, dexp)\n",
        "        img *= 255.\n",
        "        img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "    if common_params.border_type == cv.BORDER_CONSTANT:\n",
        "        cropped_img = cv.copyMakeBorder(img, border_pixels[1], border_pixels[3], border_pixels[0], border_pixels[2], common_params.border_type, value = common_params.border_val)\n",
        "        cropped_segimg = cv.copyMakeBorder(segimg, border_pixels[1], border_pixels[3], border_pixels[0], border_pixels[2], cv.BORDER_REPLICATE)\n",
        "    else:\n",
        "        cropped_img = cv.copyMakeBorder(img, border_pixels[1], border_pixels[3], border_pixels[0], border_pixels[2], common_params.border_type)\n",
        "        cropped_segimg = cv.copyMakeBorder(segimg, border_pixels[1], border_pixels[3], border_pixels[0], border_pixels[2], cv.BORDER_REPLICATE)\n",
        "\n",
        "    cropped_img = cv.getRectSubPix(cropped_img, (int(crop_param[2]), int(crop_param[3])), (crop_param[0], crop_param[1]))\n",
        "    cropped_img = cv.resize(cropped_img, (common_params.insize, common_params.insize), interpolation = cv.INTER_CUBIC)\n",
        "\n",
        "\n",
        "    w = crop_param[2]\n",
        "    h = crop_param[3]\n",
        "    cx = crop_param[0]\n",
        "    cy = crop_param[1]\n",
        "    cropped_segimg = cropped_segimg[int(cy-h/2):int(cy+h/2), int(cx-w/2):int(cx+w/2)]\n",
        "\n",
        "    cropped_segimg = cv.resize(cropped_segimg, (common_params.insize, common_params.insize), interpolation = cv.INTER_NEAREST)\n",
        "\n",
        "\n",
        "    if flip_type >= 0:\n",
        "        cropped_img = cv.flip(cropped_img, flip_type)\n",
        "        cropped_segimg = cv.flip(cropped_segimg, flip_type)\n",
        "\n",
        "    return (cropped_img, cropped_segimg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4iklfyAG5pf"
      },
      "source": [
        "## データローダの作成\n",
        "データセットの読み込み方を定義したデータローダを作成します．\n",
        "オリジナルのデータセットを使用する場合に必要です．\\\n",
        "SSDと同様に`transforms.Compose`のデータ拡張機能は使用せずに，MTDatasetクラスを`DataLoader`に与えてデータを読み込みます． \\\n",
        "バッチサイズは5，データセット読み込みに割り当てるスレッド数 (`num_workers`) は4とします．\n",
        "\n",
        "ここもSSDと同様ですが，セグメンテーションの教師ラベルを読み込む処理が追加されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-VTQaSyJulQ"
      },
      "outputs": [],
      "source": [
        "# バッチサイズの設定\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, input_list_path, input_seglabel_list_path, confing_image):\n",
        "        # Open image datalist\n",
        "        f = open(input_list_path, 'r')\n",
        "        input_list = []\n",
        "        for line in f:\n",
        "            ln = line.split('\\n')\n",
        "            input_list.append(ln[0])\n",
        "        input_list.sort()\n",
        "        f.close()\n",
        "\n",
        "        # Open segmentation label datalist\n",
        "        f = open(input_seglabel_list_path, 'r')\n",
        "        input_seglabel_list = []\n",
        "        for line in f:\n",
        "            ln = line.split('\\n')\n",
        "            input_seglabel_list.append(ln[0])\n",
        "        input_seglabel_list.sort()\n",
        "        f.close()\n",
        "\n",
        "        self.input_list = np.array(input_list)\n",
        "        self.input_seglabel_list = np.array(input_seglabel_list)\n",
        "        self.confing_image = confing_image\n",
        "        #perm = np.random.permutation(N) #学習サンプルのシャッフル\n",
        "\n",
        "        print(\"Training images : \", len(self.input_list))\n",
        "        print(\"Segmentation labels : \", len(self.input_seglabel_list))\n",
        "        if len(self.input_list) != len(self.input_seglabel_list):\n",
        "            print(\"[ERROR] Mismatch between #input_images and #segmentation_labels.\")\n",
        "            exit(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_list)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        device_ = \"cpu\"\n",
        "        input_name = self.input_list[i]\n",
        "        input_seglabel_name = self.input_seglabel_list[i]\n",
        "        #\n",
        "        # Detection teach\n",
        "        #\n",
        "\n",
        "        # Augmentation Params\n",
        "        aug_p = open(common_params.images_dir + \"/train/img_aug_param/\" + input_name + \".txt\", 'r')\n",
        "\n",
        "        in_line = aug_p.readline()  #aug_pを1行ずつ読み込み\n",
        "        opath = in_line.split(' \\n')    #改行で区切る\n",
        "        original_img_path = str(opath[0])\n",
        "\n",
        "        in_line = aug_p.readline()\n",
        "        augmentation = int(in_line)\n",
        "\n",
        "        in_line = aug_p.readline()\n",
        "        part = in_line.split(' ')\n",
        "        border_pixels = [int(part[0]), int(part[1]), int(part[2]), int(part[3])]\n",
        "\n",
        "        in_line = aug_p.readline()\n",
        "        part = in_line.split(' ')\n",
        "        crop_param = [float(part[0]), float(part[1]), float(part[2]), float(part[3])]\n",
        "\n",
        "        in_line = aug_p.readline()\n",
        "        part = in_line.split(' ')\n",
        "        hsv_param = [float(part[0]), float(part[1]), float(part[2])]\n",
        "\n",
        "        in_line = aug_p.readline()\n",
        "        flip_type = int(in_line)\n",
        "\n",
        "        #\n",
        "        # Segmentation teach\n",
        "        #\n",
        "\n",
        "        # Segmentation Aug Param (Actually, segmentation image path)\n",
        "        seg_label_p = open(common_params.images_dir + \"/train/seglabel_aug_param/\" + input_seglabel_name + \".txt\", 'r')\n",
        "\n",
        "        in_line = seg_label_p.readline()\n",
        "        slpath = in_line.split(' \\n')\n",
        "        seg_label_path = str(slpath[0])\n",
        "\n",
        "        # Input image\n",
        "        color_img = cv.imread(common_params.images_dir + \"/train/rgb/\" + original_img_path + \".png\", cv.IMREAD_COLOR)\n",
        "\n",
        "        if color_img is None:\n",
        "            print(\"[ERROR] Cannot read input image:\")\n",
        "            print(common_params.images_dir + \"/train/rgb/\" + original_img_path + \".png\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # セグメンテーション画像の読み込み\n",
        "        seg_labelimg = cv.imread(common_params.images_dir + \"/train/seglabel/\" + seg_label_path + \".png\", 0)\n",
        "\n",
        "        if seg_labelimg is None:\n",
        "            print(\"[ERROR] Cannot read segmentation label:\")\n",
        "            print(common_params.images_dir + \"/train/seglabel/\" + seg_label_path + \".png\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # 画像をSSDの入力サイズにリサイズ\n",
        "        input_img = cv.resize(color_img, (common_params.insize, common_params.insize), interpolation = cv.INTER_CUBIC)  #バイキュビック補間\n",
        "\n",
        "        input_seglabel = cv.resize(seg_labelimg, (common_params.insize, common_params.insize), interpolation = cv.INTER_NEAREST)\n",
        "\n",
        "        # Data Augmentationにより作成されたBoundingBox情報か判定\n",
        "        if augmentation == 1:\n",
        "            input_img, input_seglabel = trainAugmentation(input_img, border_pixels, crop_param, hsv_param, flip_type, input_seglabel)   #data augmentation\n",
        "\n",
        "        if self.confing_image:\n",
        "            conf_img = input_img.copy()\n",
        "\n",
        "        # 画像データをfloatに変換\n",
        "        input_img = input_img.astype(np.float32)\n",
        "\n",
        "        # 画像の平均値を引く\n",
        "        input_img -= np.array([103.939, 116.779, 123.68])\n",
        "\n",
        "        # 画像の次元を(高さ，幅，チャンネル数)から(チャンネル数, 高さ，幅)へ転置\n",
        "        input_img = input_img.transpose(2, 0, 1)\n",
        "\n",
        "        gt_boxes = []\n",
        "        df_boxes = []\n",
        "        indices = []\n",
        "        classes = []\n",
        "        idx_tmp = []\n",
        "\n",
        "        # positiveサンプルの読み込み\n",
        "        pos_num = 0\n",
        "        f = open(common_params.images_dir + \"/train/positives/\" + input_name + \".txt\", 'r')\n",
        "        for rw in f:\n",
        "            ln = rw.split(' ')\n",
        "            classes.append(int(ln[1]))\n",
        "            gt_boxes.append([float(ln[2]), float(ln[3]), float(ln[4]), float(ln[5])])\n",
        "            df_boxes.append([float(ln[6]), float(ln[7]), float(ln[8]), float(ln[9])])\n",
        "            indices.append([int(ln[10]), int(ln[11]), int(ln[12]), int(ln[13])])\n",
        "            pos_num += 1\n",
        "        f.close()\n",
        "\n",
        "        # hard negativeサンプルの読み込み (最大でpositiveサンプル数の3倍)\n",
        "        neg_num = 0\n",
        "        f = open(common_params.images_dir + \"/train/negatives/\" + input_name + \".txt\", 'r')\n",
        "        for rw in f:\n",
        "            ln = rw.split(' ')\n",
        "            classes.append(int(ln[1]))\n",
        "            gt_boxes.append([float(ln[2]), float(ln[3]), float(ln[4]), float(ln[5])])\n",
        "            df_boxes.append([float(ln[2]), float(ln[3]), float(ln[4]), float(ln[5])])\n",
        "            idx_tmp.append([int(ln[10]), int(ln[11]), int(ln[12]), int(ln[13])])\n",
        "            neg_num += 1\n",
        "        f.close()\n",
        "\n",
        "        hardneg_size = pos_num * 3 if neg_num > (pos_num * 3) else neg_num\n",
        "\n",
        "        perm = np.random.permutation(len(idx_tmp))\n",
        "\n",
        "        for hn in range(0, hardneg_size):\n",
        "            indices.append(idx_tmp[perm[hn]])\n",
        "\n",
        "        # segmentationのignore class(255)を-1にする\n",
        "        input_seglabel = input_seglabel.astype(np.int64) #uintからintにしないと負の値が入らない\n",
        "        input_seglabel[input_seglabel==255] = -1\n",
        "\n",
        "        # padding処理 ???(random) -> 8732(dfbox max size)\n",
        "        max_boxes = 8732 * 4\n",
        "\n",
        "        if len(gt_boxes) == 0:\n",
        "            gt_boxes = np.ones((max_boxes, 4)) * -100\n",
        "        elif len(gt_boxes) != max_boxes:\n",
        "            gt_boxes = np.pad(np.array(gt_boxes), [(0,max_boxes-len(gt_boxes)), (0,0)], 'constant', constant_values=-100)\n",
        "\n",
        "        if len(df_boxes) == 0:\n",
        "            df_boxes = np.ones((max_boxes, 4)) * -100\n",
        "        elif len(df_boxes) != max_boxes:\n",
        "            df_boxes = np.pad(np.array(df_boxes), [(0,max_boxes-len(df_boxes)), (0,0)], 'constant', constant_values=-100)\n",
        "\n",
        "        if len(indices) == 0:\n",
        "            indices = np.ones((max_boxes, 4)) * -100\n",
        "        elif len(indices) != max_boxes:\n",
        "            indices = np.pad(np.array(indices), [(0,max_boxes-len(indices)), (0,0)], 'constant', constant_values=-100)\n",
        "\n",
        "        if len(classes) == 0:\n",
        "            classes = np.ones((max_boxes,)) * -100\n",
        "        elif len(classes) != max_boxes:\n",
        "            classes = np.pad(np.array(classes), (0,max_boxes-len(classes)), 'constant', constant_values=-100)\n",
        "\n",
        "        # list to tensor\n",
        "        input_img = torch.tensor(input_img, device=device_)\n",
        "        gt_boxes = torch.tensor(gt_boxes, dtype=torch.float32, device=device_)\n",
        "        df_boxes = torch.tensor(df_boxes, dtype=torch.float32, device=device_)\n",
        "        indices = torch.tensor(indices, dtype=torch.int64, device=device_)\n",
        "        classes = torch.tensor(classes, dtype=torch.int64, device=device_)\n",
        "        conf_img = torch.tensor(conf_img, device=device_)\n",
        "        input_seglabel = torch.tensor(input_seglabel, device=device_)\n",
        "\n",
        "        return input_img, gt_boxes, df_boxes, indices, classes, conf_img, input_seglabel\n",
        "\n",
        "# Dataset import\n",
        "train_dataset = MTDataset(common_params.augmented_img_list, common_params.augmented_seglabel_list, True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__FbTYANd5S"
      },
      "source": [
        "# ネットワークモデルの定義\n",
        "次に，MT-DSSDのネットワークを定義します．MT-DSSDはSSD300をベースにしており，特徴抽出部分はVGGです．\n",
        "\n",
        "ネットワークのうち， `self.conv9_1` `self.conv9_2` まではSSDと同じです．その後，畳み込みと逆の演算を行うDeconvolutionを用いて，特徴マップのサイズを拡大します．Deconvolutionは `self.deconv1` 〜 `self.deconv5` の5回行います．この構造はセマンティックセグメンテーションのネットワークから着想を得ています．SSDと同じ部分をエンコーダ，5層のDeconvolution層をデコーダと呼びます．\n",
        "\n",
        "各Deconvolution層から得られた特徴マップと同じサイズの特徴マップは，エンコーダ内の対応する特徴マップと要素和を取ります．例えば， `yy5 = F.relu(y5 + k5)` は1回目のDeconvolutionである `self.deconv1(k6)` と， `conv8_2` から得られた特徴マップ `k5 = F.relu(self.conv8_2(k5))` を足しています．この部分をDeconvolution moduleと呼びます．\n",
        "\n",
        "Deconvolution moduleで得られた特徴マップは，Detection branchとSegmentation branchに入力されます．ただし，例外的に `self.conv9_2` から得られた特徴マップは，Deconvolution moduleを介さず，そのまま各branchに入力されます．\n",
        "\n",
        "Detection branchは， `self.?_?_mbox_loc(?)` と `self.?_?_mbox_cls(?)` の2つの畳み込み層で構成されます．前者は物体候補領域を推定し，後者は各候補領域の物体クラスを推定します．これはSSDと同じ仕組みです．各Detection branchで得られた推定結果は数が多く冗長なため，Non-maximum Suppression処理で適切な数に削減されます（ネットワーク内では行いません）．\n",
        "\n",
        "Segmentation branchは， `self.deconv?_?_1`, `self.deconv?_?_2`, `self.deconv?_?_3` の3つのDeconvolution層で構成されます．3回のDeconvolutionを繰り返して，セマンティックセグメンテーションのクラス確率を推定します．各Segmentation branchで得られた推定結果は連結 (`concatenate`) され，最後に1度だけ `self.segconv` で畳み込み処理を行い，最終的なセマンティックセグメンテーションの結果を得ます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmI9DjEpzE4y"
      },
      "outputs": [],
      "source": [
        "class MTDSSDNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MTDSSDNet, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0, ceil_mode = True)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.maxpool5 = nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.fc6 = nn.Conv2d(512, 1024, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.fc7 = nn.Conv2d(1024, 1024, kernel_size = 1, stride = 1, padding = 0)\n",
        "\n",
        "        self.conv6_1 = nn.Conv2d(1024, 256, kernel_size = 1, stride = 1, padding = 0)\n",
        "        self.conv6_2 = nn.Conv2d(256, 512, kernel_size = 3, stride = 2, padding = 1)\n",
        "\n",
        "        self.conv7_1 = nn.Conv2d(512, 128, kernel_size = 1, stride = 1, padding = 0)\n",
        "        self.conv7_2 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
        "\n",
        "        self.conv8_1 = nn.Conv2d(256, 128, kernel_size = 1, stride = 1, padding = 0)\n",
        "        self.conv8_2 = nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 0)\n",
        "\n",
        "        self.conv9_1 = nn.Conv2d(256, 128, kernel_size = 1, stride = 1, padding = 0)\n",
        "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 0)\n",
        "\n",
        "        self.conv9_2_mbox_loc = nn.Conv2d(256, common_params.num_of_offset_dims * common_params.num_boxes[5], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv9_2_mbox_cls = nn.Conv2d(256, common_params.num_of_classes * common_params.num_boxes[5], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconv9_2_1 = nn.ConvTranspose2d(256, common_params.num_of_classes, kernel_size = 10, stride = 1, padding = 0) #10×10\n",
        "        self.deconv9_2_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 11, stride = 8, padding = 4) #75×75\n",
        "        self.deconv9_2_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 1) #300×300\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 256, kernel_size = 3, stride = 1, padding = 0) #3×3\n",
        "\n",
        "        self.conv8_2_mbox_loc = nn.Conv2d(256, common_params.num_of_offset_dims * common_params.num_boxes[4], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv8_2_mbox_cls = nn.Conv2d(256, common_params.num_of_classes * common_params.num_boxes[4], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconv8_2_1 = nn.ConvTranspose2d(256, common_params.num_of_classes, kernel_size = 9, stride = 6, padding = 1) #19×19\n",
        "        self.deconv8_2_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 7, stride = 4, padding = 2) #75×75\n",
        "        self.deconv8_2_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 1) #300×300\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 256, kernel_size = 3, stride = 2, padding = 1) #5×5\n",
        "\n",
        "        self.conv7_2_mbox_loc = nn.Conv2d(256, common_params.num_of_offset_dims * common_params.num_boxes[3], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv7_2_mbox_cls = nn.Conv2d(256, common_params.num_of_classes * common_params.num_boxes[3], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconv7_2_1 = nn.ConvTranspose2d(256, common_params.num_of_classes, kernel_size = 7, stride = 4, padding = 2) #19×19\n",
        "        self.deconv7_2_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 7, stride = 4, padding = 2) #75×75\n",
        "        self.deconv7_2_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 1) #300×300\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(256, 512, kernel_size = 4, stride = 2, padding = 1) #10×10\n",
        "\n",
        "        self.conv6_2_mbox_loc = nn.Conv2d(512, common_params.num_of_offset_dims * common_params.num_boxes[2], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv6_2_mbox_cls = nn.Conv2d(512, common_params.num_of_classes * common_params.num_boxes[2], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconv6_2_1 = nn.ConvTranspose2d(512, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 2) #38×38\n",
        "        self.deconv6_2_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 3, stride = 2, padding = 1) #75×75\n",
        "        self.deconv6_2_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 1) #300×300\n",
        "\n",
        "        self.deconv4 = nn.ConvTranspose2d(512, 1024, kernel_size = 3, stride = 2, padding = 1) #19×19\n",
        "\n",
        "        self.fc7_mbox_loc = nn.Conv2d(1024, common_params.num_of_offset_dims * common_params.num_boxes[1], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.fc7_mbox_cls = nn.Conv2d(1024, common_params.num_of_classes * common_params.num_boxes[1], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconvfc7_1 = nn.ConvTranspose2d(1024, common_params.num_of_classes, kernel_size = 4, stride = 2, padding = 1) #38×38\n",
        "        self.deconvfc7_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 3, stride = 2, padding = 1) #75×75\n",
        "        self.deconvfc7_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 6, stride = 4, padding = 1) #300×300\n",
        "\n",
        "        self.deconv5 = nn.ConvTranspose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1) #38×38\n",
        "\n",
        "        self.conv4_3_norm_mbox_loc = nn.Conv2d(512, common_params.num_of_offset_dims * common_params.num_boxes[0], kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv4_3_norm_mbox_cls = nn.Conv2d(512, common_params.num_of_classes * common_params.num_boxes[0], kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "        self.deconv4_3_1 = nn.ConvTranspose2d(512, common_params.num_of_classes, kernel_size = 3, stride = 2, padding = 1) #75×75\n",
        "        self.deconv4_3_2 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 4, stride = 2, padding = 1) #150×150\n",
        "        self.deconv4_3_3 = nn.ConvTranspose2d(common_params.num_of_classes, common_params.num_of_classes, kernel_size = 4, stride = 2, padding = 1)  #300×300\n",
        "\n",
        "        self.segconv = nn.Conv2d(common_params.num_of_classes*6, common_params.num_of_classes, kernel_size = 1, stride = 1, padding = 0)\n",
        "\n",
        "\n",
        "    def forward(self, x, train=False):\n",
        "\n",
        "        k1 = F.relu(self.conv1_1(x))\n",
        "        k1 = F.relu(self.conv1_2(k1))\n",
        "\n",
        "        k1 = self.maxpool1(k1)\n",
        "\n",
        "        k1 = F.relu(self.conv2_1(k1))\n",
        "        k1 = F.relu(self.conv2_2(k1))\n",
        "\n",
        "        k1 = self.maxpool2(k1)\n",
        "\n",
        "        k1 = F.relu(self.conv3_1(k1))\n",
        "        k1 = F.relu(self.conv3_2(k1))\n",
        "        k1 = F.relu(self.conv3_3(k1))\n",
        "\n",
        "        k1 = self.maxpool3(k1)\n",
        "\n",
        "        k1 = F.relu(self.conv4_1(k1))\n",
        "        k1 = F.relu(self.conv4_2(k1))\n",
        "        k1 = F.relu(self.conv4_3(k1))\n",
        "\n",
        "        k2 = self.maxpool4(k1)\n",
        "\n",
        "        k2 = F.relu(self.conv5_1(k2))\n",
        "        k2 = F.relu(self.conv5_2(k2))\n",
        "        k2 = F.relu(self.conv5_3(k2))\n",
        "\n",
        "        k2 = self.maxpool5(k2)\n",
        "\n",
        "        k2 = F.relu(self.fc6(k2))\n",
        "        k2 = F.relu(self.fc7(k2))\n",
        "\n",
        "        k3 = F.relu(self.conv6_1(k2))\n",
        "        k3 = F.relu(self.conv6_2(k3))\n",
        "\n",
        "        k4 = F.relu(self.conv7_1(k3))\n",
        "        k4 = F.relu(self.conv7_2(k4))\n",
        "\n",
        "        k5 = F.relu(self.conv8_1(k4))\n",
        "        k5 = F.relu(self.conv8_2(k5))\n",
        "\n",
        "        k6 = F.relu(self.conv9_1(k5))\n",
        "        k6 = F.relu(self.conv9_2(k6))\n",
        "\n",
        "        Loc6 = self.conv9_2_mbox_loc(k6)  # Box_Estimator_6\n",
        "        Cls6 = self.conv9_2_mbox_cls(k6) # Class_Classifier_6\n",
        "\n",
        "        Seg1 = self.deconv9_2_1(k6)\n",
        "        Seg1 = self.deconv9_2_2(Seg1)\n",
        "        Seg1 = self.deconv9_2_3(Seg1)\n",
        "\n",
        "        y5 = self.deconv1(k6) #3×3\n",
        "\n",
        "        yy5 = F.relu(y5 + k5)\n",
        "\n",
        "        Loc5 = self.conv8_2_mbox_loc(yy5)  # Box_Estimator_5\n",
        "        Cls5 = self.conv8_2_mbox_cls(yy5) # Class_Classifier_5\n",
        "\n",
        "        Seg2 = self.deconv8_2_1(yy5)\n",
        "        Seg2 = self.deconv8_2_2(Seg2)\n",
        "        Seg2 = self.deconv8_2_3(Seg2)\n",
        "\n",
        "        y4 = self.deconv2(yy5)\n",
        "\n",
        "        yy4 = F.relu(y4 + k4)\n",
        "\n",
        "        Loc4 = self.conv7_2_mbox_loc(yy4)  # Box_Estimator_4\n",
        "        Cls4 = self.conv7_2_mbox_cls(yy4) # Class_Classifier_4\n",
        "\n",
        "        Seg3 = self.deconv7_2_1(yy4)\n",
        "        Seg3 = self.deconv7_2_2(Seg3)\n",
        "        Seg3 = self.deconv7_2_3(Seg3)\n",
        "\n",
        "        y3 = self.deconv3(yy4)\n",
        "\n",
        "        yy3 = F.relu(y3 + k3)\n",
        "\n",
        "        Loc3 = self.conv6_2_mbox_loc(yy3)  # Box_Estimator_3\n",
        "        Cls3 = self.conv6_2_mbox_cls(yy3) # Class_Classifier_3\n",
        "\n",
        "        Seg4 = self.deconv6_2_1(yy3)\n",
        "        Seg4 = self.deconv6_2_2(Seg4)\n",
        "        Seg4 = self.deconv6_2_3(Seg4)\n",
        "\n",
        "        y2 = self.deconv4(yy3)\n",
        "\n",
        "        yy2 = F.relu(y2 + k2)\n",
        "\n",
        "        Loc2 = self.fc7_mbox_loc(yy2)  # Box_Estimator_2\n",
        "        Cls2 = self.fc7_mbox_cls(yy2) # Class_Classifier_2\n",
        "\n",
        "        Seg5 = self.deconvfc7_1(yy2)\n",
        "        Seg5 = self.deconvfc7_2(Seg5)\n",
        "        Seg5 = self.deconvfc7_3(Seg5)\n",
        "\n",
        "        y1 = self.deconv5(yy2)\n",
        "\n",
        "        yy1 = F.relu(y1 + k1)\n",
        "\n",
        "        Loc1 = self.conv4_3_norm_mbox_loc(yy1)  # Box_Estimator_1\n",
        "        Cls1 = self.conv4_3_norm_mbox_cls(yy1) # Class_Classifier_1\n",
        "\n",
        "        Seg6 = self.deconv4_3_1(yy1) #75×75\n",
        "        Seg6 = self.deconv4_3_2(Seg6) #150×150\n",
        "        Seg6 = self.deconv4_3_3(Seg6) #300×300\n",
        "\n",
        "        Seg = torch.cat([Seg1, Seg2, Seg3, Seg4, Seg5, Seg6], dim = 1)\n",
        "        #Seg = Seg1 + Seg2 + Seg3 + Seg4 + Seg5 + Seg6\n",
        "\n",
        "        Seg = self.segconv(Seg)\n",
        "\n",
        "        if train:\n",
        "            Loc1 = Loc1.permute(0, 2, 3, 1) #(バッチ数,高さ,幅,チャンネル数)\n",
        "            Cls1 = Cls1.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc2 = Loc2.permute(0, 2, 3, 1)\n",
        "            Cls2 = Cls2.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc3 = Loc3.permute(0, 2, 3, 1)\n",
        "            Cls3 = Cls3.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc4 = Loc4.permute(0, 2, 3, 1)\n",
        "            Cls4 = Cls4.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc5 = Loc5.permute(0, 2, 3, 1)\n",
        "            Cls5 = Cls5.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc6 = Loc6.permute(0, 2, 3, 1)\n",
        "            Cls6 = Cls6.permute(0, 2, 3, 1)\n",
        "\n",
        "            Loc1 = Loc1.contiguous()\n",
        "            Loc2 = Loc2.contiguous()\n",
        "            Loc3 = Loc3.contiguous()\n",
        "            Loc4 = Loc4.contiguous()\n",
        "            Loc5 = Loc5.contiguous()\n",
        "            Loc6 = Loc6.contiguous()\n",
        "\n",
        "            Cls1 = Cls1.contiguous()\n",
        "            Cls2 = Cls2.contiguous()\n",
        "            Cls3 = Cls3.contiguous()\n",
        "            Cls4 = Cls4.contiguous()\n",
        "            Cls5 = Cls5.contiguous()\n",
        "            Cls6 = Cls6.contiguous()\n",
        "\n",
        "            Loc1 = Loc1.view(Loc1.data.shape[0] * Loc1.data.shape[1] * Loc1.data.shape[2] * common_params.num_boxes[0], int(Loc1.data.shape[3] / common_params.num_boxes[0]))\n",
        "            Cls1 = Cls1.view(Cls1.data.shape[0] * Cls1.data.shape[1] * Cls1.data.shape[2] * common_params.num_boxes[0], int(Cls1.data.shape[3] / common_params.num_boxes[0]))\n",
        "\n",
        "            Loc2 = Loc2.view(Loc2.data.shape[0] * Loc2.data.shape[1] * Loc2.data.shape[2] * common_params.num_boxes[1], int(Loc2.data.shape[3] / common_params.num_boxes[1]))\n",
        "            Cls2 = Cls2.view(Cls2.data.shape[0] * Cls2.data.shape[1] * Cls2.data.shape[2] * common_params.num_boxes[1], int(Cls2.data.shape[3] / common_params.num_boxes[1]))\n",
        "\n",
        "            Loc3 = Loc3.view(Loc3.data.shape[0] * Loc3.data.shape[1] * Loc3.data.shape[2] * common_params.num_boxes[2], int(Loc3.data.shape[3] / common_params.num_boxes[2]))\n",
        "            Cls3 = Cls3.view(Cls3.data.shape[0] * Cls3.data.shape[1] * Cls3.data.shape[2] * common_params.num_boxes[2], int(Cls3.data.shape[3] / common_params.num_boxes[2]))\n",
        "\n",
        "            Loc4 = Loc4.view(Loc4.data.shape[0] * Loc4.data.shape[1] * Loc4.data.shape[2] * common_params.num_boxes[3], int(Loc4.data.shape[3] / common_params.num_boxes[3]))\n",
        "            Cls4 = Cls4.view(Cls4.data.shape[0] * Cls4.data.shape[1] * Cls4.data.shape[2] * common_params.num_boxes[3], int(Cls4.data.shape[3] / common_params.num_boxes[3]))\n",
        "\n",
        "            Loc5 = Loc5.view(Loc5.data.shape[0] * Loc5.data.shape[1] * Loc5.data.shape[2] * common_params.num_boxes[4], int(Loc5.data.shape[3] / common_params.num_boxes[4]))\n",
        "            Cls5 = Cls5.view(Cls5.data.shape[0] * Cls5.data.shape[1] * Cls5.data.shape[2] * common_params.num_boxes[4], int(Cls5.data.shape[3] / common_params.num_boxes[4]))\n",
        "\n",
        "            Loc6 = Loc6.view(Loc6.data.shape[0] * Loc6.data.shape[1] * Loc6.data.shape[2] * common_params.num_boxes[5], int(Loc6.data.shape[3] / common_params.num_boxes[5]))\n",
        "            Cls6 = Cls6.view(Cls6.data.shape[0] * Cls6.data.shape[1] * Cls6.data.shape[2] * common_params.num_boxes[5], int(Cls6.data.shape[3] / common_params.num_boxes[5]))\n",
        "\n",
        "            return (Loc1, Cls1, Loc2, Cls2, Loc3, Cls3, Loc4, Cls4, Loc5, Cls5, Loc6, Cls6, Seg)\n",
        "\n",
        "        else:\n",
        "            return (Loc1, Cls1, Loc2, Cls2, Loc3, Cls3, Loc4, Cls4, Loc5, Cls5, Loc6, Cls6, Seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3qxW4OiL5Hs"
      },
      "source": [
        "# 損失関数の定義\n",
        "学習に使用する損失関数と最適化手法を定義します．Loss計算はGPUよりもCPUの方が高速に処理できるため，CPU上で処理を行います．\n",
        "\n",
        "SSDと同様に，最初に各層のクラス推定と位置推定を行う層と教師データのBounding box情報を照らし合わせるために，同等の特徴マップを用意して教師データを格納しています．\n",
        "クラス推定にはクロスエントロピー誤差，位置推定にはsmooth L1誤差，\n",
        "セマンティックセグメンテーションにはクロスエントロピー誤差を用いて誤差を計算します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lavy4riMm6K"
      },
      "outputs": [],
      "source": [
        "class MTLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MTLoss, self).__init__()\n",
        "\n",
        "    def forward(self, Loc, Cls, Seg, gt_box_batch, df_box_batch, idx_batch, cls_batch, bat_s, mining, seg_label):\n",
        "        device_ = \"cpu\"\n",
        "\n",
        "        if mining:\n",
        "            # hard negative mining有効時のクラスラベル\n",
        "            cls_t1 = torch.ones((bat_s, common_params.num_boxes[0], common_params.map_sizes[0], common_params.map_sizes[0]), dtype = torch.int64, device = device_) * -1\n",
        "            cls_t2 = torch.ones((bat_s, common_params.num_boxes[1], common_params.map_sizes[1], common_params.map_sizes[1]), dtype = torch.int64, device = device_) * -1\n",
        "            cls_t3 = torch.ones((bat_s, common_params.num_boxes[2], common_params.map_sizes[2], common_params.map_sizes[2]), dtype = torch.int64, device = device_) * -1\n",
        "            cls_t4 = torch.ones((bat_s, common_params.num_boxes[3], common_params.map_sizes[3], common_params.map_sizes[3]), dtype = torch.int64, device = device_) * -1\n",
        "            cls_t5 = torch.ones((bat_s, common_params.num_boxes[4], common_params.map_sizes[4], common_params.map_sizes[4]), dtype = torch.int64, device = device_) * -1\n",
        "            cls_t6 = torch.ones((bat_s, common_params.num_boxes[5], common_params.map_sizes[5], common_params.map_sizes[5]), dtype = torch.int64, device = device_) * -1\n",
        "        else:\n",
        "            # hard negative mining無効時のクラスラベル\n",
        "            cls_t1 = torch.zeros((bat_s, common_params.num_boxes[0], common_params.map_sizes[0], common_params.map_sizes[0]), dtype = torch.int64, device = device_)\n",
        "            cls_t2 = torch.zeros((bat_s, common_params.num_boxes[1], common_params.map_sizes[1], common_params.map_sizes[1]), dtype = torch.int64, device = device_)\n",
        "            cls_t3 = torch.zeros((bat_s, common_params.num_boxes[2], common_params.map_sizes[2], common_params.map_sizes[2]), dtype = torch.int64, device = device_)\n",
        "            cls_t4 = torch.zeros((bat_s, common_params.num_boxes[3], common_params.map_sizes[3], common_params.map_sizes[3]), dtype = torch.int64, device = device_)\n",
        "            cls_t5 = torch.zeros((bat_s, common_params.num_boxes[4], common_params.map_sizes[4], common_params.map_sizes[4]), dtype = torch.int64, device = device_)\n",
        "            cls_t6 = torch.zeros((bat_s, common_params.num_boxes[5], common_params.map_sizes[5], common_params.map_sizes[5]), dtype = torch.int64, device = device_)\n",
        "\n",
        "        # bounding boxのオフセットベクトルの教示データ\n",
        "        loc_t1 = torch.zeros((bat_s, common_params.num_boxes[0] * common_params.num_of_offset_dims, common_params.map_sizes[0], common_params.map_sizes[0]), dtype = torch.float32, device = device_)\n",
        "        loc_t2 = torch.zeros((bat_s, common_params.num_boxes[1] * common_params.num_of_offset_dims, common_params.map_sizes[1], common_params.map_sizes[1]), dtype = torch.float32, device = device_)\n",
        "        loc_t3 = torch.zeros((bat_s, common_params.num_boxes[2] * common_params.num_of_offset_dims, common_params.map_sizes[2], common_params.map_sizes[2]), dtype = torch.float32, device = device_)\n",
        "        loc_t4 = torch.zeros((bat_s, common_params.num_boxes[3] * common_params.num_of_offset_dims, common_params.map_sizes[3], common_params.map_sizes[3]), dtype = torch.float32, device = device_)\n",
        "        loc_t5 = torch.zeros((bat_s, common_params.num_boxes[4] * common_params.num_of_offset_dims, common_params.map_sizes[4], common_params.map_sizes[4]), dtype = torch.float32, device = device_)\n",
        "        loc_t6 = torch.zeros((bat_s, common_params.num_boxes[5] * common_params.num_of_offset_dims, common_params.map_sizes[5], common_params.map_sizes[5]), dtype = torch.float32, device = device_)\n",
        "\n",
        "        cls_t = ([cls_t1, cls_t2, cls_t3, cls_t4, cls_t5, cls_t5])\n",
        "        loc_t = ([loc_t1, loc_t2, loc_t3, loc_t4, loc_t5, loc_t6])\n",
        "\n",
        "        for b in range(0, len(idx_batch)):\n",
        "            for i in range(0, len(idx_batch[b])):\n",
        "                if int(idx_batch[b][i][1]) == -100: break\n",
        "                fmap_layer = idx_batch[b][i][1]\n",
        "                fmap_position = idx_batch[b][i][2]\n",
        "                df_box_num = idx_batch[b][i][3]\n",
        "                st_box_idx = df_box_num * common_params.num_of_offset_dims\n",
        "                ed_box_idx = st_box_idx + common_params.num_of_offset_dims\n",
        "\n",
        "                c = int(fmap_position % common_params.map_sizes[fmap_layer])\n",
        "                r = int(fmap_position / common_params.map_sizes[fmap_layer])\n",
        "\n",
        "                item_class_id = cls_batch[b][i]\n",
        "\n",
        "                # 1〜6番目のdefault boxのクラスとオフセットの教示データを格納\n",
        "                gt_box_batch_idx = gt_box_batch[b][i]\n",
        "                df_box_batch_idx = df_box_batch[b][i]\n",
        "\n",
        "                cls_t[fmap_layer][b, df_box_num, r, c] = cls_batch[b][i]\n",
        "                loc_t[fmap_layer][b, st_box_idx : ed_box_idx, r, c] = (gt_box_batch_idx - df_box_batch_idx) / common_params.loc_var\n",
        "\n",
        "\n",
        "        # 1〜6階層目の教示confidence mapの次元を(バッチ数, DF box数, 高さ, 幅)から(バッチ数, 高さ, 幅, DF box数)に転置\n",
        "        cls_t1 = cls_t1.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_t2 = cls_t2.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_t3 = cls_t3.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_t4 = cls_t4.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_t5 = cls_t5.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_t6 = cls_t6.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # 1〜6階層目の教示confidence mapの各次元数を(バッチ数, 高さ, 幅, DF box数)から(バッチ数 * 高さ * 幅 * DF box数)にreshape\n",
        "        cls_t1 = cls_t1.view(cls_t1.data.shape[0] * cls_t1.data.shape[1] * cls_t1.data.shape[2] * common_params.num_boxes[0])\n",
        "        cls_t2 = cls_t2.view(cls_t2.data.shape[0] * cls_t2.data.shape[1] * cls_t2.data.shape[2] * common_params.num_boxes[1])\n",
        "        cls_t3 = cls_t3.view(cls_t3.data.shape[0] * cls_t3.data.shape[1] * cls_t3.data.shape[2] * common_params.num_boxes[2])\n",
        "        cls_t4 = cls_t4.view(cls_t4.data.shape[0] * cls_t4.data.shape[1] * cls_t4.data.shape[2] * common_params.num_boxes[3])\n",
        "        cls_t5 = cls_t5.view(cls_t5.data.shape[0] * cls_t5.data.shape[1] * cls_t5.data.shape[2] * common_params.num_boxes[4])\n",
        "        cls_t6 = cls_t6.view(cls_t6.data.shape[0] * cls_t6.data.shape[1] * cls_t6.data.shape[2] * common_params.num_boxes[5])\n",
        "\n",
        "        # 1〜6階層目の教示localization mapの次元を(バッチ数, オフセット次元数 * DF box数, 高さ, 幅)から(バッチ数, 高さ, 幅, オフセット次元数 * DF box数)に転置\n",
        "        loc_t1 = loc_t1.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_t2 = loc_t2.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_t3 = loc_t3.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_t4 = loc_t4.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_t5 = loc_t5.permute(0, 2, 3, 1).contiguous()\n",
        "        loc_t6 = loc_t6.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # 1〜6階層目の教示localization mapの各次元数を(バッチ数, 高さ, 幅, オフセット次元数 * DF box数)から(バッチ数 * 高さ * 幅 * DF box数, オフセット次元数)にreshape\n",
        "        loc_t1 = loc_t1.view(loc_t1.data.shape[0] * loc_t1.data.shape[1] * loc_t1.data.shape[2] * common_params.num_boxes[0], int(loc_t1.data.shape[3] / common_params.num_boxes[0]))\n",
        "        loc_t2 = loc_t2.view(loc_t2.data.shape[0] * loc_t2.data.shape[1] * loc_t2.data.shape[2] * common_params.num_boxes[1], int(loc_t2.data.shape[3] / common_params.num_boxes[1]))\n",
        "        loc_t3 = loc_t3.view(loc_t3.data.shape[0] * loc_t3.data.shape[1] * loc_t3.data.shape[2] * common_params.num_boxes[2], int(loc_t3.data.shape[3] / common_params.num_boxes[2]))\n",
        "        loc_t4 = loc_t4.view(loc_t4.data.shape[0] * loc_t4.data.shape[1] * loc_t4.data.shape[2] * common_params.num_boxes[3], int(loc_t4.data.shape[3] / common_params.num_boxes[3]))\n",
        "        loc_t5 = loc_t5.view(loc_t5.data.shape[0] * loc_t5.data.shape[1] * loc_t5.data.shape[2] * common_params.num_boxes[4], int(loc_t5.data.shape[3] / common_params.num_boxes[4]))\n",
        "        loc_t6 = loc_t6.view(loc_t6.data.shape[0] * loc_t6.data.shape[1] * loc_t6.data.shape[2] * common_params.num_boxes[5], int(loc_t6.data.shape[3] / common_params.num_boxes[5]))\n",
        "\n",
        "        # 1〜6階層目の教示confidence mapを結合\n",
        "        Cls_T = torch.cat([cls_t1, cls_t2, cls_t3, cls_t4, cls_t5, cls_t6], dim = 0)\n",
        "\n",
        "        # 1〜6階層目の教示localization mapを結合\n",
        "        Loc_T = torch.cat([loc_t1, loc_t2, loc_t3, loc_t4, loc_t5, loc_t6], dim = 0)\n",
        "\n",
        "        # loss計算\n",
        "        x_entropy = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "        x_entropy_seg = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "        MSE = nn.MSELoss()\n",
        "        smooth_L1 = nn.SmoothL1Loss(reduction='none')\n",
        "\n",
        "        # confidence mapのloss\n",
        "        loss_cls = x_entropy(Cls.to(device), Cls_T.to(device))\n",
        "        # localization mapのloss\n",
        "        loss_loc = smooth_L1(Loc.to(device), Loc_T.to(device))\n",
        "        loss_loc = torch.sum(loss_loc, dim=-1)\n",
        "        positive_samples = torch.tensor(Cls_T > 0, dtype=torch.float32, device = device)\n",
        "        loss_loc *= positive_samples\n",
        "        n_positives = positive_samples.sum()\n",
        "        loss_loc = torch.sum(loss_loc) / n_positives\n",
        "\n",
        "        #segmentationのloss\n",
        "        loss_seg = x_entropy_seg(Seg.to(device), seg_label.to(device))\n",
        "\n",
        "        return loss_cls, loss_loc, loss_seg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELuHJ5lGOmmv"
      },
      "source": [
        "# 学習\n",
        "損失関数の定義まで操作が完了したら，学習を行います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmE767JNUe9s"
      },
      "source": [
        "## ネットワークと最適化方法の設定\n",
        "先ほど作成したネットワーク (MTDSSDNet) を定義し，`to(device)`によりGPU上に展開します．GPUにネットワークモデルをのせることで，並列演算の得意なGPUによる高速な学習が可能です．\n",
        "\n",
        "最適化手法とその設定はSSDと同じで，SGD（確率的勾配降下法）を用い，学習率を0.001，モーメンタムを0.9，重み減衰率を0.0005として引数に与えます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMLV95OcQuaL"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 10e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "# MTDSSDNetの読み込み\n",
        "ssd_model = MTDSSDNet()\n",
        "ssd_model.to(device)\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = optim.SGD(ssd_model.parameters(), lr = LEARNING_RATE, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "\n",
        "# loss function\n",
        "loss_function = MTLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds56MjEnUqFA"
      },
      "source": [
        "## モデルの学習\n",
        "ネットワークと最適化方法の設定完了後，学習を開始します．\n",
        "学習時は，入力された画像の推論が実行され，confidence， localization, segmentationの各特徴マップに対してそれぞれlossを計算して，誤差逆伝播によるパラメータの更新が行われます．\n",
        "また，マルチタスク学習の基礎で学んだように，各lossには重みをもたせてあります． `(loss_cls * 0.45 + loss_loc * 0.1 + loss_seg * 0.45 )` なので，confidenceとsegmentationに0.45，localizationに0.1の重みを与えます．\n",
        "\n",
        "SSDと同様に，MT-DSSDの学習には時間を要するため，ここではエポック数を30とします．学習には25分ほどかかります．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYiQ951ZMT0q"
      },
      "outputs": [],
      "source": [
        "# エポック数の設定\n",
        "EPOCH = 30\n",
        "\n",
        "def train():\n",
        "    all_iter = 0\n",
        "    num_report_iter = 50\n",
        "\n",
        "    begin_at = time.time()\n",
        "    for num_epoch in tqdm(range(EPOCH)):\n",
        "        running_loss_cls = 0.0\n",
        "        running_loss_loc = 0.0\n",
        "        running_loss_seg = 0.0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # epochが4回に1回の割合でhard negative mining有効\n",
        "        mining = True if (num_epoch + 1) % 4 == 0 else False\n",
        "\n",
        "        now_datetime = datetime.datetime.today()\n",
        "        now_datetime = str(now_datetime.year) + '-' + str('%02d' % now_datetime.month) + '-' + str('%02d' % now_datetime.day) + '@' + str('%02d' % now_datetime.hour) + ':' + str('%02d' % now_datetime.minute) + ':' + str('%02d' % now_datetime.second)\n",
        "        # print(\"\\n[Epoch: {} start] Learning rate: {} ({})\".format(num_epoch + 1, now_lr, now_datetime))\n",
        "        if mining: print(\"[Do HARD NEGATIVE MINING in this epoch!!]\")\n",
        "\n",
        "        for num_batch, data in enumerate(train_dataloader):\n",
        "            all_iter += 1\n",
        "\n",
        "            img_batch, gt_box_batch, df_box_batch, idx_batch, cls_batch, conf_img_batch, seglabel_batch = data\n",
        "\n",
        "            train_img = img_batch.to(device)\n",
        "            seg_label = seglabel_batch\n",
        "\n",
        "            # SSD net forward\n",
        "            #ssd_model.zero_grad()\n",
        "            Loc1, Cls1, Loc2, Cls2, Loc3, Cls3, Loc4, Cls4, Loc5, Cls5, Loc6, Cls6, Seg = ssd_model(train_img, train=True)\n",
        "\n",
        "            # ネットワークから出力されたconfidence mapを1〜6階層目まで結合\n",
        "            Loc = torch.cat([Loc1, Loc2, Loc3, Loc4, Loc5, Loc6], dim = 0)\n",
        "            # ネットワークから出力されたlocalization mapを1〜6階層目まで結合\n",
        "            Cls = torch.cat([Cls1, Cls2, Cls3, Cls4, Cls5, Cls6], dim = 0)\n",
        "\n",
        "            # lossを計算\n",
        "            loss_cls, loss_loc, loss_seg = loss_function(Loc, Cls, Seg, gt_box_batch, df_box_batch, idx_batch, cls_batch, BATCH_SIZE, mining, seg_label)\n",
        "            loss = (loss_cls * 0.45 + loss_loc * 0.1 + loss_seg * 0.45 )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            elapsed_time = datetime.timedelta(seconds = time.time() - begin_at)\n",
        "            sys.stdout.write(\"\\rEpoch: {}, Iter: {}, Loss:{} (cls: {}, loc: {}, seg: {})\".format(num_epoch+1, all_iter, loss.data, loss_cls.data, loss_loc.data, loss_seg.data))\n",
        "            sys.stdout.flush()\n",
        "            time.sleep(0.01)\n",
        "\n",
        "            running_loss_cls += loss_cls\n",
        "            running_loss_loc += loss_loc\n",
        "            running_loss_seg += loss_seg\n",
        "            running_loss += loss\n",
        "\n",
        "        running_loss /= all_iter\n",
        "        running_loss_cls /= all_iter\n",
        "        running_loss_loc /= all_iter\n",
        "        running_loss_seg /= all_iter\n",
        "        print(\"\\r\\r\\r[Epoch:{} Iter:{}] Time: {}, Ave_Loss:{} (c: {}, l: {}, s: {}) mining: {}\".format(num_epoch+1, all_iter, elapsed_time, running_loss, running_loss_cls, running_loss_loc, running_loss_seg, mining))\n",
        "\n",
        "    print(\"\\nExit Training\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEH7qzvi5RJE"
      },
      "source": [
        "# 評価\n",
        "以下の評価コードを実行することで，物体検出後の画像とそのバウンディングボックス情報，セグメンテーション結果が保存されます．\n",
        "\n",
        "`IN_DIR`は評価対象の画像が含まれているフォルダのパス，`OUT_DIR`は結果を保存するフォルダのパスです．\n",
        "\n",
        "物体検出の評価方法はSSDと同じですが，セマンティックセグメンテーションを物体検出結果に反映させる仕組みが追加されています．セマンティックセグメンテーションの各クラスの外接矩形を用いて，物体候補領域のスコアを次の式のように更新しています．\n",
        "\n",
        "$\n",
        "\\text { Box }_{\\text {score }}=\\left(\\operatorname{cls}\\left(B_{d}\\right)+\\left(\\frac{\\operatorname{area}\\left(B_{d}\\right) \\cap \\operatorname{area}\\left(B_{s}\\right)}{\\operatorname{area}\\left(B_{d}\\right) \\cup \\operatorname{area}\\left(B_{s}\\right)}\\right)\\right) / 2.0\n",
        "$\n",
        "\n",
        "ここで，$B_d$は物体候補領域のbounding box，$B_s$はセマンティックセグメンテーションの外接矩形，$\\operatorname{cls}(\\cdot)$はbounding boxのクラス確率，$\\operatorname{area}(\\cdot)$はbounding boxの面積を示します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc6dy-OG5x1u"
      },
      "outputs": [],
      "source": [
        "IN_DIR = \"./val/rgb\"                #  評価データのパス\n",
        "OUT_DIR = \"./out\"                   #  結果保存先\n",
        "SAVE_DATA = True                    #  画像を保存\n",
        "\n",
        "# クラスラベル (クラス名にはスペース(空白)は禁止)\n",
        "labels = common_params.arc_labels\n",
        "class_color = common_params.arc_class_color[:, ::-1]\n",
        "print(len(labels))\n",
        "\n",
        "# confidence mapのsoftmaxを計算\n",
        "def mboxSoftmax(confidence_maps, num_classes, num_boxes):\n",
        "    s = np.zeros((confidence_maps.shape[0], confidence_maps.shape[1], confidence_maps.shape[2]), np.float32)\n",
        "\n",
        "    bs = 0\n",
        "    be = num_classes\n",
        "    for b in range(0, num_boxes):\n",
        "\n",
        "        t = confidence_maps.detach().numpy()[bs : be, :, :]\n",
        "\n",
        "        total = 0\n",
        "        for i in range(0, t.shape[0]):\n",
        "            total += np.exp(t[i, :, :])\n",
        "\n",
        "        for i in range(0, t.shape[0]):\n",
        "            s[bs + i, :, :] = np.exp(t[i, :, :]) / total\n",
        "\n",
        "        bs = be\n",
        "        be += num_classes\n",
        "    return s\n",
        "\n",
        "\n",
        "# LocとClsをCPUで扱える形式に変換\n",
        "def to_CPU(Loc, Cls):\n",
        "    return Loc.to(\"cpu\"), Cls.to(\"cpu\")\n",
        "\n",
        "\n",
        "# クラス確率の高いdefault boxを検出\n",
        "def multiBoxDetection(cls_score_maps, localization_maps, num_dbox, num_class, offset_dim, min_size, max_size, step, aspect_ratio):\n",
        "\n",
        "    box_offsets = []\n",
        "    default_boxes = []\n",
        "    class_labels = []\n",
        "    class_scores = []\n",
        "\n",
        "    img_width = common_params.insize\n",
        "    img_height = common_params.insize\n",
        "\n",
        "    map_size = cls_score_maps.shape[1] * cls_score_maps.shape[2]\n",
        "    for i in range(0, map_size):\n",
        "\n",
        "        c = int(i % cls_score_maps.shape[1])\n",
        "        r = int(i / cls_score_maps.shape[1])\n",
        "\n",
        "        mbox_max_val = 0\n",
        "        mbox_max_idx = 0\n",
        "        mbox_num = 0\n",
        "\n",
        "        bs = 0\n",
        "        be = num_class\n",
        "        for b in range(0, num_dbox):\n",
        "\n",
        "            max_val = np.max(cls_score_maps[bs : be, r, c])\n",
        "            max_idx = int(np.argmax(cls_score_maps[bs : be, r, c]))\n",
        "\n",
        "            if max_val > mbox_max_val and max_idx != 0:\n",
        "                mbox_max_val = max_val\n",
        "                mbox_max_idx = max_idx\n",
        "                mbox_num = b\n",
        "\n",
        "            bs = be\n",
        "            be += num_class\n",
        "\n",
        "        bs = mbox_num * offset_dim\n",
        "        be = bs + offset_dim\n",
        "        b_offset = localization_maps[bs : be, r, c]\n",
        "\n",
        "        offset_ = 0.5\n",
        "\n",
        "        if mbox_max_val >= 0.1:\n",
        "            center_x = float((c + offset_) * step)\n",
        "            center_y = float((r + offset_) * step)\n",
        "\n",
        "            if mbox_num == 0:\n",
        "                box_width = box_height = min_size\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "            elif mbox_num == 1:\n",
        "                box_width = box_height = np.sqrt(min_size * max_size)\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "            elif mbox_num == 2:\n",
        "                box_width = min_size * np.sqrt(float(aspect_ratio[0]))\n",
        "                box_height = min_size / np.sqrt(float(aspect_ratio[0]))\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "            elif mbox_num == 3:\n",
        "                box_width = min_size * np.sqrt(1. / float(aspect_ratio[0]))\n",
        "                box_height = min_size / np.sqrt(1. / float(aspect_ratio[0]))\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "            elif mbox_num == 4:\n",
        "                box_width = min_size * np.sqrt(float(aspect_ratio[1]))\n",
        "                box_height = min_size / np.sqrt(float(aspect_ratio[1]))\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "            elif mbox_num == 5:\n",
        "                box_width = min_size * np.sqrt(1. / float(aspect_ratio[1]))\n",
        "                box_height = min_size / np.sqrt(1. / float(aspect_ratio[1]))\n",
        "                xmin = (center_x - box_width / 2.) / img_width\n",
        "                ymin = (center_y - box_height / 2.) / img_height\n",
        "                xmax = (center_x + box_width / 2.) / img_width\n",
        "                ymax = (center_y + box_height / 2.) / img_height\n",
        "\n",
        "            box_offsets.append(b_offset)\n",
        "            default_boxes.append([min(max(xmin, 0.), 1.), min(max(ymin, 0.), 1.), min(max(xmax, 0.), 1.), min(max(ymax, 0.), 1.), mbox_num])\n",
        "            class_labels.append(mbox_max_idx)\n",
        "            class_scores.append(mbox_max_val)\n",
        "\n",
        "    return (box_offsets, default_boxes, class_labels, class_scores)\n",
        "\n",
        "# bounding box候補を検出 (default boxの補正)\n",
        "def candidatesDetection(offsets, default_boxes, class_labels, class_scores, num_classes, color_img, variance):\n",
        "\n",
        "    img_width = color_img.shape[1]\n",
        "    img_height = color_img.shape[0]\n",
        "\n",
        "    candidates = []\n",
        "    for i in range(0, num_classes):\n",
        "        candidates.append([])\n",
        "\n",
        "    for det in range(0, len(class_labels)):\n",
        "\n",
        "        pred_xmin = (default_boxes[det][0] + offsets[det][0] * variance)\n",
        "        pred_ymin = (default_boxes[det][1] + offsets[det][1] * variance)\n",
        "        pred_xmax = (default_boxes[det][2] + offsets[det][2] * variance)\n",
        "        pred_ymax = (default_boxes[det][3] + offsets[det][3] * variance)\n",
        "\n",
        "        pred_xmin = min(max(pred_xmin, 0.), 1.) * img_width\n",
        "        pred_ymin = min(max(pred_ymin, 0.), 1.) * img_height\n",
        "        pred_xmax = min(max(pred_xmax, 0.), 1.) * img_width\n",
        "        pred_ymax = min(max(pred_ymax, 0.), 1.) * img_height\n",
        "        candidates[class_labels[det]].append([pred_xmin, pred_ymin, pred_xmax, pred_ymax, class_scores[det]])\n",
        "    return candidates\n",
        "\n",
        "# 同じクラスに属するbounding boxの重なり率を計算\n",
        "def jaccardOverlap(bbox1, bbox2):\n",
        "\n",
        "    if (bbox2[0] > bbox1[2]) or (bbox2[2] < bbox1[0]) or (bbox2[1] > bbox1[3]) or (bbox2[3] < bbox1[1]):\n",
        "        overlap = 0.\n",
        "    else:\n",
        "        inter_xmin = max(bbox1[0], bbox2[0])\n",
        "        inter_ymin = max(bbox1[1], bbox2[1])\n",
        "        inter_xmax = min(bbox1[2], bbox2[2])\n",
        "        inter_ymax = min(bbox1[3], bbox2[3])\n",
        "\n",
        "        inter_width = inter_xmax - inter_xmin\n",
        "        inter_height = inter_ymax - inter_ymin\n",
        "        inter_size = inter_width * inter_height\n",
        "\n",
        "        bbox1_size = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "        bbox2_size = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "\n",
        "        overlap = inter_size / (bbox1_size + bbox2_size - inter_size)\n",
        "\n",
        "    return overlap\n",
        "\n",
        "def seg_jaccardOverlap(bbox1, bbox2):\n",
        "\n",
        "    x_max = bbox2[0] + bbox2[2]\n",
        "    y_max = bbox2[1] + bbox2[3]\n",
        "    if (bbox2[0] > bbox1[2]) or (x_max < bbox1[0]) or (bbox2[1] > bbox1[3]) or (y_max < bbox1[1]):\n",
        "        overlap = 0.\n",
        "    else:\n",
        "        x_max = bbox2[0] + bbox2[2]\n",
        "        y_max = bbox2[1] + bbox2[3]\n",
        "        inter_xmin = max(bbox1[0], bbox2[0])\n",
        "        inter_ymin = max(bbox1[1], bbox2[1])\n",
        "        inter_xmax = min(bbox1[2], x_max)\n",
        "        inter_ymax = min(bbox1[3], y_max)\n",
        "\n",
        "        inter_width = inter_xmax - inter_xmin\n",
        "        inter_height = inter_ymax - inter_ymin\n",
        "        inter_size = inter_width * inter_height\n",
        "\n",
        "        bbox1_size = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "        bbox2_size = (x_max - bbox2[0]) * (y_max - bbox2[1])\n",
        "\n",
        "        overlap = inter_size / (bbox1_size + bbox2_size - inter_size)\n",
        "\n",
        "    return overlap\n",
        "\n",
        "\n",
        "# bounding box候補のnon-maximum suppresion\n",
        "def nonMaximumSuppresion(candidates, sbox):\n",
        "\n",
        "    overlap_th = 0.2\n",
        "\n",
        "    for i in range(0, len(candidates)):\n",
        "        box_num = len(candidates[i])\n",
        "\n",
        "        for j in range(0, box_num):\n",
        "            for s in range(0, len(sbox)):\n",
        "                if i == sbox[s][4]:\n",
        "                    seg_overlap = seg_jaccardOverlap(candidates[i][j], sbox[s])\n",
        "                    seg_overlap = (seg_overlap + candidates[i][j][4]) / 2\n",
        "                    candidates[i][j][4] = seg_overlap\n",
        "\n",
        "\n",
        "    for i in range(0, len(candidates)):\n",
        "\n",
        "        box_num = len(candidates[i])\n",
        "\n",
        "        js = 0\n",
        "        for j in range(0, box_num):\n",
        "            ks = js + 1\n",
        "            for k in range(j + 1, box_num):\n",
        "\n",
        "                if ks >= len(candidates[i]) or js >= len(candidates[i]):\n",
        "                    continue\n",
        "\n",
        "                overlap = jaccardOverlap(candidates[i][js], candidates[i][ks])\n",
        "\n",
        "                if overlap >= overlap_th and candidates[i][js][4] >= candidates[i][ks][4]:\n",
        "                    candidates[i].pop(ks)\n",
        "                    ks -= 1\n",
        "                elif overlap >= overlap_th and candidates[i][js][4] < candidates[i][ks][4]:\n",
        "                    candidates[i][js], candidates[i][ks] = candidates[i][ks], candidates[i][js]\n",
        "                    candidates[i].pop(ks)\n",
        "                    ks -= 1\n",
        "\n",
        "                ks += 1\n",
        "            js += 1\n",
        "\n",
        "    return candidates\n",
        "\n",
        "# box検出結果を画像に描画してテキストとともに保存\n",
        "def saveDetection(final_detections, out_img, filename):\n",
        "    # 検出したbounding boxを画像に描画\n",
        "    offset_ = 0.5\n",
        "    font = cv.FONT_HERSHEY_SIMPLEX\n",
        "    for i in range(0, len(final_detections)):\n",
        "            class_name = labels[i]\n",
        "            color = class_color[i]\n",
        "            for j in range(0, len(final_detections[i])):\n",
        "                p1 = int(final_detections[i][j][0] + offset_)\n",
        "                p2 = int(final_detections[i][j][1] + offset_)\n",
        "                p3 = int(final_detections[i][j][2] + offset_)\n",
        "                p4 = int(final_detections[i][j][3] + offset_)\n",
        "                colors = (int(color[0]), int(color[1]), int(color[2]))\n",
        "                cv.rectangle(out_img, (p1, p2), (p3, p4), colors, 5)\n",
        "                q1 = p1\n",
        "                q2 = p4\n",
        "                # クラス名表示\n",
        "                cv.rectangle(out_img, (q1, q2 - 30), (q1 + len(class_name) * 25, q2), colors, -1)\n",
        "                cv.putText(out_img, str(i) + \" \" + class_name + \": \" + str(('%.2f' % final_detections[i][j][4])),\n",
        "                           (q1, q2 - 8), font, 0.7, (0, 0, 0), 2, cv.LINE_AA)\n",
        "                cv.putText(out_img, str(i) + \" \" + class_name + \": \" + str(('%.2f' % final_detections[i][j][4])),\n",
        "                           (q1, q2 - 8), font, 0.7, (255, 255, 255), 1, cv.LINE_AA)\n",
        "\n",
        "                # 検出したクラスのラベル、スコア、座標を出力\n",
        "                if SAVE_DATA:\n",
        "                    f = open(OUT_DIR + '/detection_txt/' + filename + 'res.txt', 'a')\n",
        "                    f.write(\"{} {} {} {} {} {}\\n\".format(str(i), final_detections[i][j][4], p1, p2, p3, p4))\n",
        "                    f.close()\n",
        "    # 画像保存\n",
        "    if SAVE_DATA:\n",
        "        cv.imwrite(OUT_DIR + '/detection/' + filename + '.png', out_img)\n",
        "\n",
        "    return out_img\n",
        "\n",
        "def detection(img, ssd_model, filename, min_sizes, max_sizes):\n",
        "    # タイマーリセット\n",
        "    total_time = 0.0\n",
        "    processing_time = 0.0\n",
        "    drawing_time = 0.0\n",
        "    classes = []\n",
        "\n",
        "    # 入力画像をSSDの入力サイズにリサイズ\n",
        "    start = time.time()\n",
        "    input_img = cv.resize(img, (common_params.insize, common_params.insize), interpolation = cv.INTER_CUBIC)\n",
        "    out_img = img.copy()\n",
        "\n",
        "    input_img = input_img.astype(np.float32)\n",
        "    input_img -= np.array([103.939, 116.779, 123.68])\n",
        "    input_img = input_img.transpose(2, 0, 1)\n",
        "    input_data = []\n",
        "    input_data.append(input_img)\n",
        "\n",
        "    x_data = torch.tensor(input_data, dtype=torch.float32, device=device)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # SSDのforward\n",
        "    start = time.time()\n",
        "    Loc1, Cls1, Loc2, Cls2, Loc3, Cls3, Loc4, Cls4, Loc5, Cls5, Loc6, Cls6, Seg = ssd_model(x_data, train=False)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # CPUで処理\n",
        "    start = time.time()\n",
        "    Loc1, Cls1 = to_CPU(Loc1, Cls1)\n",
        "    Loc2, Cls2 = to_CPU(Loc2, Cls2)\n",
        "    Loc3, Cls3 = to_CPU(Loc3, Cls3)\n",
        "    Loc4, Cls4 = to_CPU(Loc4, Cls4)\n",
        "    Loc5, Cls5 = to_CPU(Loc5, Cls5)\n",
        "    Loc6, Cls6 = to_CPU(Loc6, Cls6)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # セグメンテーション推論\n",
        "    start = time.time()\n",
        "    pred = Seg.to(\"cpu\").detach().numpy()\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    seg_result_max = np.squeeze(pred[0,:,:])\n",
        "\n",
        "    # セグメンテーション結果をグレー画像でもっておく(保存用/外接矩形抽出用)\n",
        "    gray = seg_result_max.astype(np.uint8)\n",
        "    gray_large = cv.resize(gray, (1280, 960), interpolation = cv.INTER_NEAREST)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # セグメンテーション保存\n",
        "    start = time.time()\n",
        "    # セグメンテーション結果をカラーに戻す\n",
        "    rgb_r = seg_result_max.copy()\n",
        "    rgb_g = seg_result_max.copy()\n",
        "    rgb_b = seg_result_max.copy()\n",
        "\n",
        "    for k in range(0, class_color.shape[0]):\n",
        "        rgb_r[seg_result_max==k] = class_color[k,0]\n",
        "        rgb_g[seg_result_max==k] = class_color[k,1]\n",
        "        rgb_b[seg_result_max==k] = class_color[k,2]\n",
        "\n",
        "    rgb = np.zeros((seg_result_max.shape[0], seg_result_max.shape[1], 3))\n",
        "    rgb[:,:,0] = rgb_r\n",
        "    rgb[:,:,1] = rgb_g\n",
        "    rgb[:,:,2] = rgb_b\n",
        "\n",
        "    rgb = rgb.astype(np.uint8)\n",
        "    if SAVE_DATA: cv.imwrite(OUT_DIR + '/segmentation_results/' + filename + '.png', rgb)\n",
        "    rgb = cv.resize(rgb, (1280, 960), interpolation = cv.INTER_NEAREST)\n",
        "    if SAVE_DATA: cv.imwrite(OUT_DIR + '/segmentation_results_large/' + filename + '.png', rgb)\n",
        "    if SAVE_DATA: cv.imwrite(OUT_DIR + '/segmentation_results_gray/' + filename + '.png', gray)\n",
        "\n",
        "    rgb_s = rgb.copy()\n",
        "    elapsed_time = time.time() - start\n",
        "    drawing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    sbox = []\n",
        "\n",
        "    # セグメンテーションの外接矩形を取得\n",
        "    start = time.time()\n",
        "    for g in range(1, len(labels)+1):\n",
        "        gray_extract = gray_large.copy()\n",
        "        gray_extract[gray_extract != g] = 0\n",
        "        ret, thresh = cv.threshold(gray_extract, g-1, 255, 0)\n",
        "        contours, _ = cv.findContours(thresh, 1, 2)\n",
        "\n",
        "        max_contours = 0\n",
        "        max_i = 0\n",
        "        for i in range(len(contours)):\n",
        "            if max_contours <= len(contours[i]):\n",
        "                max_contours = len(contours[i])\n",
        "                max_i = i\n",
        "\n",
        "        if len(contours) != 0:\n",
        "            x,y,w,h = cv.boundingRect(contours[max_i])\n",
        "            if SAVE_DATA: cv.rectangle(rgb_s,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "            sbox.append([x, y, w, h, g])\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # 各階層のconfidence mapのsoftmaxを計算\n",
        "    start = time.time()\n",
        "    cls_score1 = mboxSoftmax(Cls1[0], common_params.num_of_classes, common_params.num_boxes[0])\n",
        "    cls_score2 = mboxSoftmax(Cls2[0], common_params.num_of_classes, common_params.num_boxes[1])\n",
        "    cls_score3 = mboxSoftmax(Cls3[0], common_params.num_of_classes, common_params.num_boxes[2])\n",
        "    cls_score4 = mboxSoftmax(Cls4[0], common_params.num_of_classes, common_params.num_boxes[3])\n",
        "    cls_score5 = mboxSoftmax(Cls5[0], common_params.num_of_classes, common_params.num_boxes[4])\n",
        "    cls_score6 = mboxSoftmax(Cls6[0], common_params.num_of_classes, common_params.num_boxes[5])\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # クラス確率の高いdefault boxの検出\n",
        "    start = time.time()\n",
        "    Loc1 = Loc1.detach().numpy()\n",
        "    Loc2 = Loc2.detach().numpy()\n",
        "    Loc3 = Loc3.detach().numpy()\n",
        "    Loc4 = Loc4.detach().numpy()\n",
        "    Loc5 = Loc5.detach().numpy()\n",
        "    Loc6 = Loc6.detach().numpy()\n",
        "\n",
        "    offsets1, default_boxes1, class_labels1, class_scores1 = multiBoxDetection(cls_score1, Loc1[0], common_params.num_boxes[0], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[0], max_sizes[0], common_params.steps[0], common_params.aspect_ratios[0])\n",
        "    offsets2, default_boxes2, class_labels2, class_scores2 = multiBoxDetection(cls_score2, Loc2[0], common_params.num_boxes[1], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[1], max_sizes[1], common_params.steps[1], common_params.aspect_ratios[1])\n",
        "    offsets3, default_boxes3, class_labels3, class_scores3 = multiBoxDetection(cls_score3, Loc3[0], common_params.num_boxes[2], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[2], max_sizes[2], common_params.steps[2], common_params.aspect_ratios[2])\n",
        "    offsets4, default_boxes4, class_labels4, class_scores4 = multiBoxDetection(cls_score4, Loc4[0], common_params.num_boxes[3], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[3], max_sizes[3], common_params.steps[3], common_params.aspect_ratios[3])\n",
        "    offsets5, default_boxes5, class_labels5, class_scores5 = multiBoxDetection(cls_score5, Loc5[0], common_params.num_boxes[4], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[4], max_sizes[4], common_params.steps[4], common_params.aspect_ratios[4])\n",
        "    offsets6, default_boxes6, class_labels6, class_scores6 = multiBoxDetection(cls_score6, Loc6[0], common_params.num_boxes[5], common_params.num_of_classes, common_params.num_of_offset_dims, min_sizes[5], max_sizes[5], common_params.steps[5], common_params.aspect_ratios[5])\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # オフセットベクトルによりdefault boxを補正\n",
        "    start = time.time()\n",
        "    candidates1 = candidatesDetection(offsets1, default_boxes1, class_labels1, class_scores1, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    candidates2 = candidatesDetection(offsets2, default_boxes2, class_labels2, class_scores2, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    candidates3 = candidatesDetection(offsets3, default_boxes3, class_labels3, class_scores3, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    candidates4 = candidatesDetection(offsets4, default_boxes4, class_labels4, class_scores4, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    candidates5 = candidatesDetection(offsets5, default_boxes5, class_labels5, class_scores5, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    candidates6 = candidatesDetection(offsets6, default_boxes6, class_labels6, class_scores6, common_params.num_of_classes, img, common_params.loc_var)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # 各階層のbounding box候補を統合\n",
        "    start = time.time()\n",
        "    all_candidate = []\n",
        "    for i in range(0, common_params.num_of_classes):\n",
        "        all_candidate.append([])\n",
        "        all_candidate[i].extend(candidates1[i])\n",
        "        all_candidate[i].extend(candidates2[i])\n",
        "        all_candidate[i].extend(candidates3[i])\n",
        "        all_candidate[i].extend(candidates4[i])\n",
        "        all_candidate[i].extend(candidates5[i])\n",
        "        all_candidate[i].extend(candidates6[i])\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # non-maximum suppresionによりbounding boxの最終結果を出力\n",
        "    start = time.time()\n",
        "    final_detections = nonMaximumSuppresion(all_candidate, sbox)\n",
        "    elapsed_time = time.time() - start\n",
        "    processing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "\n",
        "    # 画像保存\n",
        "    start = time.time()\n",
        "    out_img = saveDetection(final_detections, out_img, filename)\n",
        "    elapsed_time = time.time() - start\n",
        "    drawing_time += elapsed_time\n",
        "    total_time += elapsed_time\n",
        "    print(\"----- Detection Done -----\")\n",
        "\n",
        "def test_out():\n",
        "    # ディレクトリの作成\n",
        "    if SAVE_DATA:\n",
        "        make_dir_list = [\n",
        "            OUT_DIR,\n",
        "            OUT_DIR + '/detection',                     # 検出結果のb-box画像\n",
        "            OUT_DIR + '/segmentation_results',          # セグメンテーション結果\n",
        "            OUT_DIR + '/segmentation_results_large',    # セグメンテーション結果(入力サイズに拡大)\n",
        "            OUT_DIR + '/segmentation_results_gray',     # セグメンテーション結果(グレースケール)\n",
        "            OUT_DIR + '/detection_txt'                  # 検出結果のb-boxテキスト\n",
        "        ]\n",
        "        for dirname in make_dir_list:\n",
        "            if not(os.path.exists(dirname)): os.mkdir(dirname)\n",
        "\n",
        "\n",
        "    # default boxのサイズリスト計算\n",
        "    step = int(math.floor((common_params.max_ratio - common_params.min_ratio) / (len(common_params.mbox_source_layers) - 2)))\n",
        "    min_sizes = []\n",
        "    max_sizes = []\n",
        "    for ratio in range(common_params.min_ratio, common_params.max_ratio + 1, step):\n",
        "            min_sizes.append(common_params.insize * ratio / 100.)\n",
        "            max_sizes.append(common_params.insize * (ratio + step) / 100.)\n",
        "    min_sizes = [common_params.insize * 10 / 100.] + min_sizes\n",
        "    max_sizes = [common_params.insize * 20 / 100.] + max_sizes\n",
        "\n",
        "    # 入力画像の読み込み\n",
        "    color_img = glob(os.path.join(IN_DIR, '*' + '.png'))\n",
        "    color_img.sort()\n",
        "    print(\"Evaluate images : \", len(color_img))\n",
        "\n",
        "    # 読み込んだリストを順次検出\n",
        "    ssd_model.eval()\n",
        "    for lf in range(len(color_img)):\n",
        "        print(\"{} / {}\".format(lf+1, len(color_img)))\n",
        "        img = cv.imread(color_img[lf])\n",
        "        filename, ext = os.path.splitext(os.path.basename(color_img[lf]))\n",
        "        detection(img, ssd_model, filename, min_sizes, max_sizes)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvI8wPyRJbQ"
      },
      "source": [
        "出力された検出結果の画像を確認してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f_rjVFnb_cs"
      },
      "outputs": [],
      "source": [
        "!ls # ディレクトリの確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW_y_Ex2bPNo"
      },
      "outputs": [],
      "source": [
        "!ls out/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTwjYdeqRz6n"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image,display_png\n",
        "display_png(Image('./out/detection/2017-016-2.png'))\n",
        "display_png(Image('./out/segmentation_results/2017-016-2.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd1eUsTxTN1S"
      },
      "source": [
        "Bounding boxが表示されていますが，クラスが間違っていることがわかります．何度か試すと正しいクラスが出ることがありますが，これでは良い結果ではありません．さらに，セマンティックセグメンテーションの結果も理想とは言えません．30epochのみの学習のため，学習が足りずうまく検出・識別できていないことがわかります．\n",
        "\n",
        "次に，物体検出結果を定量的に評価するためにスコアを計算します．この部分はSSDと同じなので，説明は割愛します．スコアは下記の3つを用います．\n",
        "- 識別率: 検出された物体のうち，クラスが正しかった割合\n",
        "- 未検出率: 全ての物体のうち，検出できなかった割合\n",
        "- 平均IoU: 全てのバウンディングボックスのIoUの平均"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jTTNF1ltmIx"
      },
      "outputs": [],
      "source": [
        "%pwd\n",
        "!ls ./val/rgb/\n",
        "!ls ./out/detection_txt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kROAFFaygfWd"
      },
      "outputs": [],
      "source": [
        "image_path = \"./val/rgb/\"\n",
        "teach_path = \"./val/boundingbox/\"\n",
        "result_path = \"./out/detection_txt/\"\n",
        "\n",
        "# IoU matching results path\n",
        "match_result_path = result_path + \"/matchingResults/\"\n",
        "\n",
        "# evaluation output(totalresult, confusion_matrix) path\n",
        "eval_result_path = result_path + \"/eval/\"\n",
        "\n",
        "# wait time of cv2.waitKey (if 0 then no wait)\n",
        "WAITTIME = 0\n",
        "\n",
        "# if your detection results have a classlabel(e.g.: DVD, avery_binder...), set 1\n",
        "LABEL_FLAG = 1\n",
        "\n",
        "# if your detection results are normalized, set 1\n",
        "NORMALIZED = 0\n",
        "\n",
        "#IOU Threshold\n",
        "# IOU_THRESH = 0.55\n",
        "IOU_THRESH = 0.2\n",
        "\n",
        "# if teach labels have category and color classification results, set 1\n",
        "# normally need not change\n",
        "CAT_PASS_FLAG = 0\n",
        "\n",
        "# normally need not change\n",
        "THRESH = 0.35\n",
        "NCLASS = 40\n",
        "\n",
        "COLOR_TABLE = common_params.arc_class_color\n",
        "itemIDList = common_params.itemIDList\n",
        "\n",
        "def convNormalizedCord(data, height, width):\n",
        "    x = float(data[1]) * width\n",
        "    y = float(data[2]) * height\n",
        "    w = float(data[3]) * width\n",
        "    h = float(data[4]) * height\n",
        "    x1 = x - (w/2.)\n",
        "    y1 = y - (h/2.)\n",
        "    x2 = x + (w/2.)\n",
        "    y2 = y + (h/2.)\n",
        "    return(int(data[0]), int(x1), int(y1), int(x2), int(y2))\n",
        "\n",
        "def convResCord(data, height, width, normalized):\n",
        "    if normalized == 1:\n",
        "        data = convNormalizedCord(data, height, width)\n",
        "    if int(data[0]) == 0:\n",
        "        classID = 41\n",
        "    else:\n",
        "        classID = int(data[0])\n",
        "    return(classID, int(data[1]), int(data[2]), int(data[3]), int(data[4]) )\n",
        "\n",
        "def getIOU(boxA, boxB):\n",
        "    # if the length of between boxAcenter and boxBcenter is too far, return 0\n",
        "    center_boxA = np.array([(boxA[0] + boxA[2]) / 2.0, (boxA[1] + boxA[3]) / 2.0])\n",
        "    center_boxB = np.array([(boxB[0] + boxB[2]) / 2.0, (boxB[1] + boxB[3]) / 2.0])\n",
        "    if np.linalg.norm(center_boxA - center_boxB) >= 500:\n",
        "        return 0\n",
        "\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    inter_area = (xB - xA + 1) * (yB - yA + 1)\n",
        "    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\n",
        "    iou = inter_area / float(boxA_area + boxB_area - inter_area)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def readTxt(file_path, d_type, label_flag=0, cat_pass_flag=0):\n",
        "    coordinate = []\n",
        "    f = open(file_path, 'r')\n",
        "\n",
        "    if f != None:\n",
        "        for row in f:\n",
        "            data = row.split()\n",
        "            if(label_flag==1 and d_type == \"result\"):\n",
        "                data = [data[0], data[2], data[3], data[4], data[5]]\n",
        "            elif(d_type == \"result\"):\n",
        "                data = [data[0], data[1], data[2], data[3], data[4]]\n",
        "            elif(cat_pass_flag==1 and d_type == \"teach\"):\n",
        "                data = [data[0], data[3], data[4], data[5], data[6]]\n",
        "            elif(d_type == \"teach\" or d_type == \"evaluate\"):\n",
        "                data = data\n",
        "                # DO NOTHING\n",
        "            else:\n",
        "                print(\"[ERROR] Unexpected text data type:\" + d_type)\n",
        "                return 1\n",
        "            coordinate.append(data)\n",
        "        f.close()\n",
        "        return coordinate\n",
        "    else:\n",
        "        print(\"[ERROR] Can't read:\" + file_path)\n",
        "        return 1\n",
        "\n",
        "def drawBB(img, data):\n",
        "    color = [ COLOR_TABLE[int(data[0])][2], COLOR_TABLE[int(data[0])][1], COLOR_TABLE[int(data[0])][0] ]\n",
        "    height, width = img.shape[:2]\n",
        "    x1 = data[1]\n",
        "    y1 = data[2]\n",
        "    x2 = data[3]\n",
        "    y2 = data[4]\n",
        "    cv.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "    cv.putText(img, itemIDList[int(data[0])], (x1, y1-2), cv.FONT_HERSHEY_TRIPLEX, 1, (255,255,255), 2)\n",
        "    cv.putText(img, itemIDList[int(data[0])], (x1, y1), cv.FONT_HERSHEY_TRIPLEX, 1, (0,0,0), 1)\n",
        "    return img\n",
        "\n",
        "def matching(file_list):\n",
        "    for file_path in file_list:\n",
        "        print(file_path)\n",
        "        file_name, ext = os.path.splitext(os.path.basename(file_path) )\n",
        "        file_name = file_name.replace(\"res\", \"\")\n",
        "\n",
        "        result_data = readTxt(file_path, \"result\", label_flag=LABEL_FLAG)\n",
        "        teach_data = readTxt(teach_path + file_name + \".txt\", \"teach\", cat_pass_flag=CAT_PASS_FLAG)\n",
        "        img = cv.imread(image_path + file_name + \".png\")\n",
        "        height, width = img.shape[:2]\n",
        "        img_backup = img.copy()\n",
        "\n",
        "        # Convert to Full(teach) coordinates\n",
        "        for i in range(0, len(teach_data)):\n",
        "            teach_data[i] = convNormalizedCord(teach_data[i], height, width)\n",
        "\n",
        "        # Convert for result coordinates\n",
        "        for i in range(0, len(result_data)):\n",
        "            result_data[i] = convResCord(result_data[i], height, width, NORMALIZED)\n",
        "\n",
        "        # init result lists\n",
        "        hit = [False] * len(teach_data)\n",
        "        success = [False] * len(result_data)\n",
        "        max_IoU_list = [0] * len(result_data)\n",
        "        true_category = [0] * len(result_data)\n",
        "\n",
        "        # search box which have highest IoU value\n",
        "        for j in range(0, len(result_data)):\n",
        "            max_IoU = 0.0\n",
        "            max_index = 0\n",
        "            for i in range(0, len(teach_data)):\n",
        "                iou = getIOU(teach_data[i][1:], result_data[j][1:])\n",
        "                if(max_IoU < iou) and (iou <= 1.0):\n",
        "                    max_IoU = iou\n",
        "                    max_index = i\n",
        "                if WAITTIME != 0:\n",
        "                    img = img_backup.copy()\n",
        "                    img = drawBB(img, teach_data[i])\n",
        "                    img = drawBB(img, result_data[j])\n",
        "                    cv.imshow(\"\", img)\n",
        "                    cv.waitKey(WAITTIME)\n",
        "\n",
        "            # found\n",
        "            max_IoU_list[j] = max_IoU\n",
        "            img = img_backup.copy()\n",
        "            if(max_IoU > THRESH):\n",
        "                hit[max_index] = True\n",
        "                success[j] = (teach_data[max_index][0] == result_data[j][0])\n",
        "                true_category[j] = teach_data[max_index][0]\n",
        "                if WAITTIME != 0:\n",
        "                    img = drawBB(img, teach_data[max_index])\n",
        "                    img = drawBB(img, result_data[j])\n",
        "                    cv.putText(img, \"Max IoU:\" + str(max_IoU), (0, 25) , cv.FONT_HERSHEY_TRIPLEX, 1, (255,255,255), 2)\n",
        "                    cv.putText(img, \"Class Match:\" + str(success[j]), (0, 50) , cv.FONT_HERSHEY_TRIPLEX, 1, (255,255,255), 2)\n",
        "            else:\n",
        "                if WAITTIME != 0:\n",
        "                    img = drawBB(img, result_data[j])\n",
        "                    cv.putText(img, \"No Matching\", (0, 25) , cv.FONT_HERSHEY_TRIPLEX, 1, (255,255,255), 2)\n",
        "            if WAITTIME != 0:\n",
        "                cv.imshow(\"\", img)\n",
        "                cv.waitKey(WAITTIME*5)\n",
        "\n",
        "        print(\"Result\")\n",
        "        print(\"Matching Success\")\n",
        "        for i in range(0, len(result_data)):\n",
        "            if(success[i] == True):\n",
        "                print(str(result_data[i]))\n",
        "\n",
        "        print(\"Matching Failed\")\n",
        "        for i in range(0, len(result_data)):\n",
        "            if(success[i] == False):\n",
        "                print(str(result_data[i]) + str(max_IoU_list[i]))\n",
        "\n",
        "        print(\"No Match\")\n",
        "        for i in range(0, len(teach_data)):\n",
        "            if(hit[i] == False):\n",
        "                print(str(teach_data[i]))\n",
        "\n",
        "        print(\"File output\")\n",
        "        f = open(result_path + \"matchingResults/\" + file_name + \".txt\", 'w')\n",
        "        for i in range(0, len(result_data)):\n",
        "            write_data = str(result_data[i][0]) + \" \" + str(true_category[i]) + \" \" + str(max_IoU_list[i]) + '\\n'\n",
        "            f.writelines(write_data)\n",
        "        # miss boxes\n",
        "        for i in range(0, len(teach_data)):\n",
        "            if(hit[i] == False):\n",
        "                write_data = \"0\" + \" \" + str(teach_data[i][0]) + \" 0.0\" + '\\n'\n",
        "                f.writelines(write_data)\n",
        "        f.close()\n",
        "\n",
        "    if WAITTIME != 0:\n",
        "        cv.destroyAllWindows()\n",
        "\n",
        "def evaluate(file_list):\n",
        "    conv_ID_table = [i for i in range(0, NCLASS+1)]\n",
        "\n",
        "    total_boxes = 0             # number of all(detected + not detected) boxes\n",
        "    total_true_boxes = 0        # number of true class boxes\n",
        "    total_detected_boxes = 0    # number of detected boxes\n",
        "    total_false_boxes = 0       # number of false class boxes\n",
        "    total_undetected_boxes = 0  # number of not detected boxes\n",
        "    total_IoU = 0.0\n",
        "\n",
        "    confusion_mat = [[0 for i in range(NCLASS)] for j in range(NCLASS)]\n",
        "\n",
        "    for filePath in file_list:\n",
        "        boxes = 0\n",
        "        true_boxes = 0\n",
        "        detected_boxes = 0\n",
        "        false_boxes = 0\n",
        "        undetected_boxes = 0\n",
        "\n",
        "        result_data = readTxt(filePath, \"evaluate\")\n",
        "\n",
        "        for i in range(0, len(result_data)):\n",
        "            boxes += 1\n",
        "            if(result_data[i][0] != '0' and result_data[i][1] != '0'):\n",
        "                #detected\n",
        "                detected_boxes += 1\n",
        "                total_IoU += float(result_data[i][2])\n",
        "                confusion_mat[conv_ID_table[int(result_data[i][1])]-1][conv_ID_table[int(result_data[i][0])]-1] += 1\n",
        "                if(float(result_data[i][2]) >= IOU_THRESH):\n",
        "                    #IOU >= Threshold\n",
        "                    if (result_data[i][0] == result_data[i][1]):\n",
        "                        #true class\n",
        "                        true_boxes += 1\n",
        "                    else:\n",
        "                        #false class\n",
        "                        false_boxes += 1\n",
        "                else:\n",
        "                    #low IOU\n",
        "                    false_boxes += 1\n",
        "            else:\n",
        "                #not detected\n",
        "                undetected_boxes += 1\n",
        "\n",
        "        if boxes != (detected_boxes + undetected_boxes):\n",
        "            print(\"[Error] missmatch: boxes != detected_boxes + undetected_boxes\")\n",
        "            print(boxes, detected_boxes, undetected_boxes)\n",
        "            sys.exit()\n",
        "        if detected_boxes != (true_boxes + false_boxes):\n",
        "            print(\"[Error] missmatch: detected_boxes != true_boxes + false_boxes\")\n",
        "            print(detected_boxes, true_boxes, false_boxes)\n",
        "            sys.exit()\n",
        "\n",
        "        total_boxes += boxes\n",
        "        total_true_boxes += true_boxes\n",
        "        total_detected_boxes += detected_boxes\n",
        "        total_false_boxes += false_boxes\n",
        "        total_undetected_boxes += undetected_boxes\n",
        "\n",
        "    print(\"Total Result\")\n",
        "    print(\"Matching Rate: \"+str(float(total_true_boxes) / float(total_detected_boxes)))\n",
        "    print(\"Miss(No detection box) Rate: \"+str(float(total_undetected_boxes) / float(total_boxes)))\n",
        "    print(\"Mean IoU:\"+ str(total_IoU / total_detected_boxes))\n",
        "\n",
        "    #confusion_matrix normalization\n",
        "    normalized_matrix = []\n",
        "    for i in confusion_mat:\n",
        "        a = 0\n",
        "        temp_matrix = []\n",
        "        a = sum(i,0)\n",
        "        for j in i:\n",
        "            if a == 0:\n",
        "                temp_matrix.append(0.0)\n",
        "            else:\n",
        "                temp_matrix.append(float(j) / float(a))\n",
        "        normalized_matrix.append(temp_matrix)\n",
        "\n",
        "    #draw confusion_matrix\n",
        "    plt.clf()\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    res = ax.imshow(array(normalized_matrix), cmap=cm.jet, interpolation='nearest')\n",
        "    cb = fig.colorbar(res)\n",
        "    cb.ax.set_yticklabels([str(i)+'%' for i in range(0, 101, 10)])\n",
        "    confusion_mat = np.array(confusion_mat)\n",
        "    width, height = confusion_mat.shape\n",
        "    item_list = [str(i) for i in range(1, NCLASS+1)]\n",
        "    plt.xticks(range(width), item_list[:width],rotation=90)\n",
        "    plt.yticks(range(height), item_list[:height])\n",
        "    plt.tick_params(labelsize=7)\n",
        "    plt.subplots_adjust(left=0.05, bottom=0.10, right=0.95, top=0.95)\n",
        "    plt.ylabel(\"True Class\")\n",
        "    plt.xlabel(\"Predicted Class\")\n",
        "\n",
        "    for i in range(0, NCLASS):\n",
        "        print(str(i+1) + \": \" + str(normalized_matrix[i][i]))\n",
        "\n",
        "    #file output\n",
        "    f = open(eval_result_path + \"/totalresult.txt\", 'w')\n",
        "    f.writelines(\"Total Result\" + '\\n')\n",
        "    f.writelines(\"Matching Rate: \"+str(float(total_true_boxes) / float(total_detected_boxes)) + '\\n')\n",
        "    f.writelines(\"Miss(No detection box) Rate: \"+str(float(total_undetected_boxes) / float(total_boxes)) + '\\n')\n",
        "    f.writelines(\"Mean IoU:\"+ str(total_IoU / total_detected_boxes) + '\\n')\n",
        "    for i in range(0, NCLASS):\n",
        "        f.writelines(str(i+1) + \": \" + str(normalized_matrix[i][i]) + '\\n')\n",
        "    f.close()\n",
        "\n",
        "    savefig(eval_result_path + \"/confusion_matrix.pdf\", format=\"pdf\")\n",
        "    savefig(eval_result_path + \"/confusion_matrix.png\", format=\"png\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # mkdir for Matching results\n",
        "    if not os.path.exists(result_path + \"/matchingResults\"): os.mkdir(result_path + \"/matchingResults\")\n",
        "    if not os.path.exists(result_path + \"/eval\"): os.mkdir(result_path + \"/eval\")\n",
        "\n",
        "    # IoU matching\n",
        "    file_list = glob(result_path + \"*.txt\")\n",
        "    if len(file_list) == 0:\n",
        "        print(\"[Error] Detection results file list is empty. Check this dir:\" + result_path)\n",
        "        file_list.sort()\n",
        "    else:\n",
        "        matching(file_list)\n",
        "\n",
        "    # evaluate and output\n",
        "    file_list = glob(match_result_path + \"/*.txt\")\n",
        "    if len(file_list) == 0:\n",
        "        print(\"[Error] Evaluation file list is empty. Check this dir:\" + match_result_path)\n",
        "    else:\n",
        "        file_list.sort()\n",
        "        evaluate(file_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1T2VOVsVpqK"
      },
      "source": [
        "`Total Result`以降が学習済みモデルの評価結果を表しています．\n",
        "\n",
        "```\n",
        "Matching Rate: 0.09146341463414634\n",
        "Miss(No detection box) Rate: 0.5260115606936416\n",
        "Mean IoU:0.7627907384890055\n",
        "```\n",
        "\n",
        "30epochのみの学習のため，良い結果とは言えません．本来は150epochほど学習を進める必要があります．\n",
        "\n",
        "また，Confusion matrixも画像として表示されています．対角線上に高い値があることが理想ですが，残念ながら全く実現できていません．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul3zRM37pOdC"
      },
      "source": [
        "セマンティックセグメンテーションは，SegNetと同様にスコアを計算します．ネットワークから出力された300×300のクラスマップと，300×300にリサイズされた教師ラベルを1ピクセルずつ比較してaccuracyとmIoUを計算します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9Cf8MgHNd9i"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
        "\n",
        "evaluator = Evaluator(common_params.num_of_classes)\n",
        "evaluator.reset()\n",
        "OUT_DIR = \"./out\"\n",
        "\n",
        "gt_file = glob(\"./val/seglabel300/*.png\")\n",
        "pred_file = glob(OUT_DIR + \"/segmentation_results_gray/*.png\")\n",
        "gt_file.sort()\n",
        "pred_file.sort()\n",
        "\n",
        "for num in range(len(gt_file)):\n",
        "    gt_img = cv.imread(gt_file[num], cv.IMREAD_GRAYSCALE)\n",
        "    pred_img = cv.imread(pred_file[num], cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if gt_img is None:\n",
        "      print(\"Not found: \" + str(gt_file[num]))\n",
        "      exit()\n",
        "    if pred_img is None:\n",
        "      print(\"Not found: \" + str(pred_file[num]))\n",
        "      exit()\n",
        "    evaluator.add_batch(gt_img, pred_img)\n",
        "\n",
        "mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "Acc = evaluator.Pixel_Accuracy()\n",
        "print(\"mean accuracy: {}，　mean IoU: {}\".format(Acc, mIoU))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RhFP7qepzD8"
      },
      "source": [
        "```\n",
        "mean accuracy: 0.4601791111111111，　mean IoU: 0.03262478278088939\n",
        "```\n",
        "となりました．こちらも低い値です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7poEDe7UO0"
      },
      "source": [
        "## 学習済みモデルによる評価\n",
        "30エポックの学習では高い精度が得られないことがわかりました．もう少しデータ量を増やして，150エポックほど学習させると高い精度を得られます．しかし，それを行うには3日ほど必要なため，ここでは予め学習しておいた学習済みモデルによる評価を行います．\n",
        "\n",
        "`MODEL_PATH`に学習済みモデルを指定して実行します． また，  `ssd_pretrained.load_state_dict(torch.load(MODEL_PATH))` で保存済みのモデルをネットワークに反映させます．その他は，先程の評価コードと同じです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqGk0knk6sLo"
      },
      "outputs": [],
      "source": [
        "IN_DIR = \"./val/rgb\"                 # 評価データのパス\n",
        "OUT_DIR = \"./out_pretrained\"        # 結果保存先\n",
        "MODEL_PATH = \"./MTDSSD_pretrained.pth\"  # モデルのパス\n",
        "\n",
        "# クラスラベル\n",
        "print(len(labels))\n",
        "def test_out2():\n",
        "    # ディレクトリの作成\n",
        "    if SAVE_DATA:\n",
        "        make_dir_list = [\n",
        "            OUT_DIR,\n",
        "            OUT_DIR + '/detection',                     # 検出結果のb-box画像\n",
        "            OUT_DIR + '/segmentation_results',          # セグメンテーション結果\n",
        "            OUT_DIR + '/segmentation_results_large',    # セグメンテーション結果(入力サイズに拡大)\n",
        "            OUT_DIR + '/segmentation_results_gray',     # セグメンテーション結果(グレースケール)\n",
        "            OUT_DIR + '/detection_txt'                  # 検出結果のb-boxテキスト\n",
        "        ]\n",
        "        for dirname in make_dir_list:\n",
        "            if not(os.path.exists(dirname)): os.mkdir(dirname)\n",
        "    # 学習モデル読み込み\n",
        "    ssd_pretrained = MTDSSDNet()\n",
        "    ssd_pretrained.load_state_dict(torch.load(MODEL_PATH))\n",
        "    ssd_pretrained.to(device)\n",
        "\n",
        "    # default boxのサイズリスト計算\n",
        "    step = int(math.floor((common_params.max_ratio - common_params.min_ratio) / (len(common_params.mbox_source_layers) - 2)))\n",
        "    min_sizes = []\n",
        "    max_sizes = []\n",
        "    for ratio in range(common_params.min_ratio, common_params.max_ratio + 1, step):\n",
        "            min_sizes.append(common_params.insize * ratio / 100.)\n",
        "            max_sizes.append(common_params.insize * (ratio + step) / 100.)\n",
        "    min_sizes = [common_params.insize * 10 / 100.] + min_sizes\n",
        "    max_sizes = [common_params.insize * 20 / 100.] + max_sizes\n",
        "\n",
        "    # 入力画像の読み込み\n",
        "    color_img = glob(os.path.join(IN_DIR, '*' + '.png'))\n",
        "    color_img.sort()\n",
        "\n",
        "    # 読み込んだリストを順次検出\n",
        "    ssd_pretrained.eval()\n",
        "    for lf in range(len(color_img)):\n",
        "        print(\"{} / {}\".format(lf+1, len(color_img)))\n",
        "        img = cv.imread(color_img[lf])\n",
        "        filename, ext = os.path.splitext(os.path.basename(color_img[lf]))\n",
        "        detection(img, ssd_pretrained, filename, min_sizes, max_sizes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_out2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0EdoA_tq-aB"
      },
      "source": [
        "先程と同様に物体検出の評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4pPt6rBNUOm"
      },
      "outputs": [],
      "source": [
        "result_path = \"./out_pretrained/detection_txt/\"\n",
        "\n",
        "# IoU matching results path\n",
        "match_result_path = result_path + \"/matchingResults/\"\n",
        "\n",
        "# evaluation output(totalresult, confusion_matrix) path\n",
        "eval_result_path = result_path + \"/eval/\"\n",
        "\n",
        "def main():\n",
        "    # mkdir for Matching results\n",
        "    if not os.path.exists(result_path + \"/matchingResults\"): os.mkdir(result_path + \"/matchingResults\")\n",
        "    if not os.path.exists(result_path + \"/eval\"): os.mkdir(result_path + \"/eval\")\n",
        "\n",
        "    # IoU matching\n",
        "    file_list = glob(result_path + \"*.txt\")\n",
        "    if len(file_list) == 0:\n",
        "        print(\"[Error] Detection results file list is empty. Check this dir:\" + result_path)\n",
        "        file_list.sort()\n",
        "    else:\n",
        "        matching(file_list)\n",
        "\n",
        "    # evaluate and output\n",
        "    file_list = glob(match_result_path + \"/*.txt\")\n",
        "    if len(file_list) == 0:\n",
        "        print(\"[Error] Evaluation file list is empty. Check this dir:\" + match_result_path)\n",
        "    else:\n",
        "        file_list.sort()\n",
        "        evaluate(file_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F8y33QjZ_ml"
      },
      "source": [
        "```\n",
        "Matching Rate: 0.9131832797427653\n",
        "Miss(No detection box) Rate: 0.028125\n",
        "Mean IoU:0.9116157335649024\n",
        "```\n",
        "\n",
        "になりました．今回よりも大きなデータセットで，かつ多くの時間をかけて学習しているため，かなり精度が向上しています．\n",
        "\n",
        "Confusion matrixも，対角線上に高い値が出ていることがわかります．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py28jD33xu4h"
      },
      "source": [
        "セマンティックセグメンテーションの評価も行いましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu5zRWDPcyY3"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(common_params.num_of_classes)\n",
        "evaluator.reset()\n",
        "OUT_DIR = \"./out_pretrained\"\n",
        "\n",
        "gt_file = glob(\"./val/seglabel300/*.png\")\n",
        "pred_file = glob(OUT_DIR + \"/segmentation_results_gray/*.png\")\n",
        "gt_file.sort()\n",
        "pred_file.sort()\n",
        "\n",
        "\n",
        "for num in range(len(gt_file)):\n",
        "    gt_img = cv.imread(gt_file[num], cv.IMREAD_GRAYSCALE)\n",
        "    pred_img = cv.imread(pred_file[num], cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if gt_img is None:\n",
        "      print(\"Not found: \" + str(gt_file[num]))\n",
        "      exit()\n",
        "    if pred_img is None:\n",
        "      print(\"Not found: \" + str(pred_file[num]))\n",
        "      exit()\n",
        "    evaluator.add_batch(gt_img, pred_img)\n",
        "\n",
        "mIoU = evaluator.Mean_Intersection_over_Union()\n",
        "Acc = evaluator.Pixel_Accuracy()\n",
        "print(\"mean accuracy: {}，　mean IoU: {}\".format(Acc, mIoU))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mco4t-6Ix6Ze"
      },
      "source": [
        "```\n",
        "mean accuracy: 0.9516948888888889，　mean IoU: 0.8729181565476937\n",
        "```\n",
        "となりました．セマンティックセグメンテーションも大きく精度が向上しています．\n",
        "\n",
        "このモデルは，学習画像枚数を1,210枚とし，data augmentationも30倍行っています．また，学習エポック数は150とかなり長い時間をかけて作成したものです．深層学習を行うときに「データ数」と「学習時間」が重要であることもわかります．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbePfeZIcwZo"
      },
      "source": [
        "# 課題\n",
        "1. エポック数を変えて実験し，精度の変化を確認しましょう．\n",
        "2. 誤差関数の重みを変更して，精度の変化を確認しましょう．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hT3k18MZwlH"
      },
      "source": [
        "# 参考文献\n",
        " - [1] R. Araki, T. Onishi, T. Hirakawa, T. Yamashita and H. Fujiyoshi, “MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection”. In 2020 IEEE International Conference on Robotics and Automation, pp. 10487-10493, 2020.\n",
        " - [2] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “SSD: Single Shot Multibox Detector,” in European Conference on Computer Vision. Springer, 2016, pp. 21-37.\n",
        " - [3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “DSSD: De- convolutional Single Shot Detector,” arXiv preprint arXiv:1701.06659, 2017."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "マルチタスク学習（応用編）",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}