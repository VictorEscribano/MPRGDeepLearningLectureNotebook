{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/14_self_supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpXJ4V8D2Cc_"
      },
      "source": [
        "# 自己教師あり学習\n",
        "\n",
        "---\n",
        "\n",
        "深層学習ネットワーク (Deep Neural Network, DNN) は，大量のラベルありデータを用いて学習を行うことで高い認識性能を発揮します．\\\n",
        "しかし，教師ラベルは人手によって付与を行うため，データ数に応じて人的・時間的コストが増加します．\\\n",
        "また，問題設定によって必要となるラベル情報が異なります．\\\n",
        "例としてセマンティックセグメンテーションは，画像に対して１ピクセルごとのラベル付けを必要とします．\\\n",
        "これらのことから，多くの場合では理想的なデータセット（大量のラベルありデータ）を用意するのは困難です．\\\n",
        "このような問題を解決する学習方法の１つとして「自己教師あり学習」があります．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/cw3nd3iq0ggb01g/sup.png\" width = 55%>\n",
        "\n",
        "自己教師あり学習（Self-supervised Learning）は，ラベルなしデータのみを用いたネットワークの事前学習方法です．\\\n",
        "ラベルなしデータに対して，独自の問題 (Pretext task) を設計することで疑似的な教師ラベルを作成し，ネットワークの学習を行います．\\\n",
        "自己教師あり学習後のネットワークは，事前学習モデルとして解きたいタスクへ転移学習・fine-tuningされます．\\\n",
        "自己教師あり学習では，転移学習・fine-tuning先のタスクを「下流タスク (Downstream task)」と呼びます．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/6qlgcye8hr4blaq/self_sup.png\" width = 55%>\n",
        "\n",
        "自己教師あり学習として様々なPretext taskやPretext taskの学習方法が提案されていますが，ここではContrastive Learningという枠組みについて紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYc1x_5sNcOI"
      },
      "source": [
        "# Contrastive Learning\n",
        "\n",
        "Contrastive Learningは，「元画像が同じペアを見つける」というPretext taskに対して元画像が同じ特徴量を近づけ，異なる特徴量を離すように学習する手法です．\\\n",
        "Contrastive Learningでは，近づける関係にあるペアを「ポジティブペア」，離す関係にあるペアを「ネガティブペア」と呼びます．\\\n",
        "今回はContrastive Learningの中でも「Simple Framework for Contrastive Learning (SimCLR)」という手法について紹介すると共に，簡単な問題設定から実際に学習を行います．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/r7yrz62wtt0g7iw/semi_sup_CR.png\" width = 55%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68eUqPV3NcOI"
      },
      "source": [
        "# SimCLR\n",
        "\n",
        "SimCLRの学習の流れは以下のようになります．\n",
        "\n",
        "1. ミニバッチを作成し，各データへデータ増幅を適用することでPretext taskを作成\n",
        "2. 各データをネットワークに入力して特徴量を抽出\n",
        "3. 全ての特徴量間で類似度を計算してポジティブペア・ネガティブペアの関係に基づいて損失を計算\n",
        "4. 損失値が小さくなるようにネットワークを学習\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/rte7jfpzb3md7tg/SimCLR_2.png\" width = 55%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJlzf7mYNcOI"
      },
      "source": [
        "## Pretext taskの作成\n",
        "ミニバッチ内のデータに対してデータ増幅を適用して，1サンプルにつき2サンプルのデータを作成することで，「元画像が同じペア（ポジティブペア）」を作成します．\\\n",
        "また，ポジティブペア以外のデータとの関係をネガティブペアとします．\n",
        "\n",
        "SimCLRでは，データ増幅として「ランダムクロップ」と「色変換」を適用します．\\\n",
        "各変換は，元画像が同じペアを探す際に異なる問題を解くことを促します．\n",
        "\n",
        "ランダムクロップは，「同一位置の予測問題」と「近接位置の予測問題」を作成します．\\\n",
        "色変換は，「色の予測問題」を作成します．\\\n",
        "また，ランダムクロップと色変換を組み合わせることで，位置の予測問題を色情報に基づいて解くことを抑制します．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/8785c0qxjpqaqor/data_aug.png.png\" width = 70%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X401GI2xNcOJ"
      },
      "source": [
        "## ネットワーク構造\n",
        "SimCLRにおけるネットワーク構造は，「自己教師あり学習により学習を行いたいネットワーク」と「2層のMLP」から構成されます．\\\n",
        "自己教師あり学習では，自己教師あり学習により学習を行いたいネットワークを「エンコーダ」と呼びます．\\\n",
        "自己教師あり学習後は，エンコーダのみを下流タスクへ利用します．\n",
        "\n",
        "エンコーダは，本来の構造から出力層を取り除いた形とし，自己教師あり学習によって入力層から中間層の学習が行われます．\\\n",
        "また，MLPの出力特徴量を用いて損失計算を行うことで，エンコーダの出力特徴量がPretext taskに特化した特徴量とならないことを促します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8s9d9b_NcOJ"
      },
      "source": [
        "## 損失式\n",
        "ミニバッチサイズをNサンプルとした場合，Pretext task作成後のデータ数は2Nサンプルとなります．\\\n",
        "そのため，1サンプルにつき1組のポジティブペアと(2N-2)組のネガティブペアがあります．\n",
        "\n",
        "特徴量間の類似度としてコサイン類似度を使用し，以下の式から損失を計算します．\\\n",
        "このとき，$z_i$は基準となるデータの特徴量，$z_j$はポジティブペアの特徴量，$T$は温度パラメータ，$N$はバッチサイズを表します．\\\n",
        "SimCLRでは，この損失式をNT-Xent（Normalized Temperature-scaled CROSS entropy）損失と呼びます．\\\n",
        "このNT-Xent損失を基準となるデータを変えて計算を行い，その平均値を最終的な損失とします．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/zztz70av6dgmwdj/SimCLR_loss.png\" width = 55%>\n",
        "\n",
        "この損失式を式変形すると以下のようになります．\\\n",
        "ポジティブペアの類似度が分母，ネガティブペアの類似度が分子となることがわかります．\\\n",
        "コサイン類似度は，-1から＋1の範囲の指標であり，値が１に近いほど類似度が高いことを表します．\\\n",
        "logは，括弧の中の値が１の場合に０となります．\\\n",
        "そのため，損失値を小さくするために，分母（ポジティブペアの類似度）を大きく，分子（ネガティブペアの類似度）を小さくするような学習が行われます．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/6v0hob1q5r6szl9/SimCLR_loss2.png\" width = 55%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq8iq2LCNcOJ"
      },
      "source": [
        "## 学習の評価\n",
        "自己教師あり学習の論文内では，大きく以下の2つの実験から学習後のエンコーダの評価を行います．\n",
        "\n",
        "1. 自己教師あり学習により獲得した特徴量を評価\\\n",
        "・線形評価：自己教師あり学習に用いたデータセットを用いて，出力層を教師あり学習した場合の認識性能から評価\\\n",
        "・K-NN評価：自己教師あり学習に用いたデータセットを用いて，エンコーダの出力特徴量からK-NN法を適用した場合の認識性能から評価\n",
        "2. 下流タスクへの転移性を評価\\\n",
        "・転移学習：自己教師あり学習と異なるデータセットを用いて，出力層を教師あり学習した場合の認識性能から評価\\\n",
        "・fine-tuning：自己教師あり学習と異なるデータセットを用いて，エンコーダと出力層を教師あり学習した場合の認識性能から評価\n",
        "\n",
        "「1. 自己教師あり学習により獲得した特徴量を評価」では，入力層と中間層は自己教師あり学習でのみ学習を行なっています．\\\n",
        "そのため，良い特徴量を抽出できていない場合は認識性能が低くなるため，間接的に自己教師あり学習した入力層と中間層を評価していると言えます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzQjzxih2CdA"
      },
      "source": [
        "# 畳み込みニューラルネットワークの自己教師付き学習\n",
        "クラス分類問題において，自己教師あり学習 (SimCLR) によって畳み込みニューラルネットワーク (CNN) を学習し，fine-tuningと転移学習により評価を行います．\\\n",
        "今回は，CIFAR-100で自己教師あり学習したCNNを，CIFAR-10でfine-tuningと転移学習します．\n",
        "\n",
        "プログラムの構成は以下の通りです．\n",
        "1. データセットの定義　　　：ラベルなしデータのデータセットを定義します．\n",
        "2. ネットワークの定義　　　：エンコーダ (CNN) + MLPを定義します．\n",
        "3. 損失設計の定義　　　　　：NT-Xent損失を定義します．\n",
        "4. 自己教師あり学習　　　　：自己教師あり学習によりネットワークを学習します．\n",
        "5. 乱数初期化したネットワークの学習　　　：乱数で初期化したネットワークを教師あり学習します．\n",
        "6. 自己教師あり学習したネットワークの学習：自己教師あり学習したネットワークをfine-tuning・転移学習します．\n",
        "\n",
        "５と６の結果を比較することで，自己教師あり学習の有無による精度の違いを比較します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lpmx0gO2CdA"
      },
      "source": [
        "## モジュールの読み込み\n",
        "まず初めにGoogle Colaboratory上にインストールされていないライブラリのインストールを行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO-PhSaPS1QE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b11a92-e35a-4705-bb57-1de677796958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchlars\n",
            "  Downloading torchlars-0.1.2.tar.gz (6.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchlars) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchlars) (4.1.1)\n",
            "Building wheels for collected packages: torchlars\n",
            "  Building wheel for torchlars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchlars: filename=torchlars-0.1.2-cp37-cp37m-linux_x86_64.whl size=1188042 sha256=7723de42a45f8b5b224f6a46578a0d6ebf7c5952c4bb93ee2d0430e5760d0e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/a9/96/62995a5abfd27e2f46671e17b99a52b6dccf46391d2b0e1c17\n",
            "Successfully built torchlars\n",
            "Installing collected packages: torchlars\n",
            "Successfully installed torchlars-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchlars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkOJk9mW2CdB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import SubsetRandomSampler, Subset, Dataset\n",
        "\n",
        "from torchlars import LARS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPuOra882CdC"
      },
      "source": [
        "## データセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlrNwkXU2CdC"
      },
      "source": [
        "### データ増幅の定義\n",
        "Contrastive Learningを行うには，教師なしデータ１サンプルにつき，データ増幅を適用した２サンプル（ポジティブペア）の画像を必要とします．\\\n",
        "今回は，この挙動を自作のデータ増幅クラスによって実現します．\\\n",
        "また，データ増幅後の2Nサンプルのデータは，損失計算を簡単にするためにNサンプルずつの2つのTensorとして返すように設定します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP9BaWl12CdC"
      },
      "outputs": [],
      "source": [
        "# 自己教師あり学習におけるデータ増幅(自作のデータ増幅クラス)\n",
        "class TransformsSimCLR:\n",
        "    def __init__(self):\n",
        "        c_w = 0.5   # color_jitterの強さの調整\n",
        "        color_jitter = transforms.ColorJitter(0.8*c_w, 0.8*c_w, 0.8*c_w, 0.2*c_w)  # ランダムに明るさ,コントラスト,彩度,色相を変化\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(32, scale=(0.2, 1.)),  # ランダムにクロップして32×32のサイズで出力\n",
        "            transforms.RandomApply([color_jitter], p=0.8),  # 80%の確率でcolor_jitterを適用\n",
        "            transforms.RandomGrayscale(p=0.2),  # 20%の確率でグレースケール化を適用\n",
        "            transforms.RandomHorizontalFlip(),  # ランダムに左右反転\n",
        "            transforms.ToTensor(),  # 画像をtensor化\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "        ])\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        # 1つの画像に対して２つのデータ増幅を適用した画像を取得\n",
        "        q = self.train_transform(x)\n",
        "        k = self.train_transform(x)\n",
        "        return [q, k]\n",
        "    \n",
        "ssl_transform = TransformsSimCLR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bx-_wKYNcOU"
      },
      "source": [
        "### データセットの定義\n",
        "自己教師あり学習に使用するデータセットを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "6e0947fa7bad4b2cae2b4aa09f4ecaf7",
            "ee3795016a6b499d9f0bbe57efdd058a",
            "f423634827844f9595ceac78410dab9d",
            "6d706aa25de740f48e6e07805a3657ac",
            "861eef61304b4514b065199250c14834",
            "9a5eeceb72e043dfb69f80931a94090a",
            "4b3de3aadd284a8c8cc10ffbbbd2e3c2",
            "4b990ad419d847f9b5a51584e9ea7d4f",
            "b44381e2091d4626a097667478719c07",
            "084c9ce83646445bb1934c09c7edf0e1",
            "13f88eb28dcf458582fae9c4bcbe504b"
          ]
        },
        "id": "5BeXhEE1NcOU",
        "outputId": "b19728ec-f752-4035-f7b1-e3b2db55f83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/CIFAR-100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e0947fa7bad4b2cae2b4aa09f4ecaf7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/CIFAR-100/cifar-100-python.tar.gz to ./dataset/CIFAR-100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "unsup_trainset = torchvision.datasets.CIFAR100(root=\"./dataset/CIFAR-100\", train=True,  download=True, transform=ssl_transform)\n",
        "unsup_loader   = torch.utils.data.DataLoader(unsup_trainset, \n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=16, \n",
        "                                             pin_memory=True, \n",
        "                                             drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ-oBfYoct9W"
      },
      "source": [
        "### データの可視化\n",
        "定義したデータ増幅と自作データ増幅クラスによって，1つのラベルなしデータからどのようなポジティブペアが作成されるのかを確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "kydSU88e2CdD",
        "outputId": "4003bbf6-d762-4a19-fa2e-8b9c81001903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "tmp = unsup_loader.__iter__()\n",
        "(image_i, image_y), _ = next(tmp)\n",
        "\n",
        "# データの可視化\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8,4))\n",
        "\n",
        "ax1.imshow(image_i[0].permute(1,2,0))\n",
        "ax2.imshow(image_y[0].permute(1,2,0))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPXvYKZ2CdD"
      },
      "source": [
        "## ネットワークの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWXNl_9A2CdD"
      },
      "source": [
        "### ネットワークの定義\n",
        "畳み込みニューラルネットワークの定義をします．\\\n",
        "今回は，11.knowledge_distillation，12.deep_mutual_learning，13.半教師付き学習で使用したネットワークと同様のものを利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns6Yv3lK2CdD"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, widen_factor=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16*widen_factor, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16*widen_factor, 32*widen_factor, kernel_size=3, stride=1, padding=1)\n",
        "        self.l1 = nn.Linear(8*8*32*widen_factor, 1024*widen_factor)\n",
        "        self.l2 = nn.Linear(1024*widen_factor, 1024*widen_factor)\n",
        "        self.l3 = nn.Linear(1024*widen_factor, 10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.pool(self.act(self.conv1(x)))\n",
        "        h = self.pool(self.act(self.conv2(h)))\n",
        "        h = h.view(h.size()[0], -1)\n",
        "        h = self.act(self.l1(h))\n",
        "        h = self.act(self.l2(h))\n",
        "        h = self.l3(h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_D6lBect9Z"
      },
      "source": [
        "### エンコーダ＋MLPの定義\n",
        "学習対象のネットワークから出力層を取り除いた形のエンコーダとMLPから構成されるネットワークを定義します．\\\n",
        "今回は，学習対象のネットワークの「2層の畳み込み層＋3層の全結合層」における3層目の全結合層を出力層と捉えて，「2層の畳み込み層＋2層の全結合層」を自己教師あり学習により学習します．\n",
        "\n",
        "まず，学習対象のネットワークの3層目の全結合層を，入力値をそのまま出力値とする層に置き換えます．\\\n",
        "次に，2層のMLPを定義します．\\\n",
        "また，各データの特徴量は，損失計算を簡単にするためにNサンプルずつの2つのTensorとして返します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlKSoszs2CdD"
      },
      "outputs": [],
      "source": [
        "# 入力をそのまま出力とする層の定義\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# MLPを含めたネットワークの定義\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, encoder, projection_dim=128):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder    = encoder     # ネットワーク\n",
        "        self.n_features = encoder.l3.in_features  # ネットワークの３層目の全結合層の入力サイズ\n",
        "        self.encoder.l3 = Identity()  # ネットワークの3層目の全結合層を変更（入力値をそのまま返すように変更）\n",
        "        \n",
        "        # MLPの定義\n",
        "        self.projection_dim = projection_dim  # MLPの出力サイズ\n",
        "        self.projector = nn.Sequential( nn.Linear(self.n_features, self.n_features, bias=False),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(self.n_features, self.projection_dim, bias=False) )\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        # ネットワークへ入力\n",
        "        h_i = self.encoder(x_i)\n",
        "        h_j = self.encoder(x_j)\n",
        "        # ネットワークの出力した特徴量をMLPへ入力\n",
        "        z_i = self.projector(h_i)\n",
        "        z_j = self.projector(h_j)\n",
        "        # ネットワークによる特徴量とMLPによる特徴量を出力するように設計\n",
        "        return z_i, z_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l6kK-4K2CdD"
      },
      "source": [
        "## 損失関数の定義\n",
        "NT-Xent (Normalized Temperature-scaled CROSS entropy) 損失を実装します．\n",
        "損失式は以下のようになります．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/zztz70av6dgmwdj/SimCLR_loss.png\" width = 40%>\n",
        "\n",
        "このとき，この損失式は１つのデータ$z_i$に対する損失値を計算するものでした．\\\n",
        "基準とするデータ$z_i$によって分母の構成が変わるため，データ毎にこの損失式を計算する必要があります．\\\n",
        "プログラム上では，この損失式をforward関数内で３段階の工程から実現しています．\\\n",
        "ここでは例として，ミニバッチサイズを３として解説します．\n",
        "\n",
        "データ増幅適用前のデータを(A, B, C)，データ増幅適用後のデータを(A1, A2, B1, B2, C1, C2)とします．\\\n",
        "自作データ増幅クラスとSimCLRクラスの設定から，最終的なネットワークの出力特徴量は，(A1, B1, C1)と(A2, B2, C2)の２つに分かれます．\n",
        "\n",
        "---\n",
        "１段階目では，全ての特徴量間の類似度を計算します．\\\n",
        "まず，２つに分かれている特徴量のTensorを結合します．\\\n",
        "結合は，「torch.cat」で行い，第一引数に結合したいTensor「(z_i, z_j)」を指定します．\\\n",
        "このとき，第一引数の１つ目に指定したTensor「z_i」の一番後ろに２つ目に指定したTensor「z_j」が結合されます．\\\n",
        "そのため，結合した後の特徴量の順番は，(A1，B1，C1，A2，B2，C2)となります．\\\n",
        "次に「self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature」で全ての特徴量間のコサイン類似度を計算します．\\\n",
        "これによって損失式内の「${\\rm sim}()/T$」が計算されます．\\\n",
        "ここで作成されるTensorは，以下のような形で特徴量間の類似度が格納されています．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/pzsr045caryivm1/loss_1.png\" width = 55%>\n",
        "\n",
        "---\n",
        "２段階目では，損失計算に必要な類似度のみを取り出します．\\\n",
        "ポジティブペアの類似度は，(A1, A2)，(B1, B2)，(C1, C2)，(A2, A1)，(B2, B1)，(C2, C1)です．\\\n",
        "ポジティブペアの類似度の格納位置を確認すると，左上から右下にかけて斜めに格納されています．\\\n",
        "「torch.diag」を利用することで，第一引数で指定したTensorから，第二引数で指定した位置を基準として左上から右下にかけて斜めにTensor内の類似度を取り出します．\\\n",
        "第二引数によってどのように取り出し位置が変化するのかを各色で表します．\\\n",
        "Tensorの要素指定は「0」から始まるため，ミニバッチサイズと同じ値を指定することで，取り出したい要素を指定できます．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/ztw8u2ya3b2y1lz/loss_2.png.png\" width = 58%>\n",
        "\n",
        "取り出した２つのTensor torch.diag(sim, 3)とtorch.diag(sim, -3)は，「torch.cat」によって１つのTensorにします．\\\n",
        "結合した後の類似度の順番は ( (A1, A2)，(B1, B2)，(C1, C2)，(A2, A1)，(B2, B1)，(C2, C1) ) となります．\n",
        "\n",
        "次に「sim[self.mask].reshape(N, -1)」によって，全ての類似度を収納したTensorからポジティブペアの類似度や損失計算に不要な同じ特徴量間の類似度（例：A1とA1の類似度）を削除します．\\\n",
        "これにより，このTensorはネガティブペアの類似度のみを格納したTensorとなります．\n",
        "\n",
        "最後に「torch.cat((positive_samples, negative_samples), dim=1)」によってポジティブペアの類似度とネガティブペアの類似度を１つのTensorにします．\\\n",
        "作成されたTensorは以下のような形となります．\\\n",
        "(0,0)，(1,0)，(2,0)といった縦の０番目の要素がポジティブペアの類似度（オレンジ背景），０番目以外の要素がネガティブペアの類似度（白背景）となっています．\\\n",
        "これにより，０番目の要素を取り出すことでNT-Xent損失の分子にあたる「${\\rm sim}(z_i,z_j)/T$」，０～４番目の要素を取り出すことで分母にあたる「${\\rm sim}(z_i,z_k)/T$」の計算結果を取り出すことができます．\n",
        "\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/g4izlda80i14696/loss_3.png.png.png\" width = 55%>\n",
        "\n",
        "---\n",
        "３段階目では，損失計算を行います．\\\n",
        "損失計算には「nn.CrossEntropyLoss()」を利用します．\n",
        "このクラスは，クロスエントロピー損失を計算します．\\\n",
        "クロスエントロピー損失の式は以下の通りです．\n",
        "\n",
        "\\begin{equation}\n",
        "E=-\\sum_i^{nclass} t_i \\log y_i\n",
        "\\end{equation}\n",
        "\n",
        "そのため，NT-Xent損失の式と見比べてどの計算に利用するのか疑問に思うかもしれません．\\\n",
        "「nn.CrossEntropyLoss()」クラスは，上記のようなクロスエントロピー損失のみを計算するクラスではなく，ソフトマックス関数の計算を内包したクラスです．\n",
        "そのため，数式で表すと以下のようになります．\n",
        "\n",
        "\\begin{equation}\n",
        "E=-\\log(\\frac{\\exp(y_{label})}{\\sum_i^{nclass} \\exp(y_i)})\n",
        "\\end{equation}\n",
        "\n",
        "このとき，$y_{label}$は正解クラス（第二引数により指定されたクラス）の確率値を表します．\\\n",
        "２段階目で作成したTensorは，０番目の要素がポジティブペアの類似度（NT-Xent損失の分子）であるため，第二引数として０を指定することでNT-Xent損失が計算されます．\\\n",
        "「torch.zeros(N).to(positive_samples.cuda()).long()」ではNT-Xent損失における基準となるデータ「$z_i$」の数分の０を指定するTensorを作成し，「self.criterion(logits, labels)」では損失を計算しています．\\\n",
        "このとき，「nn.CrossEntropyLoss(reduction=\"sum\")」とinit関数で宣言しているため，基準となるデータ数分の損失値の総和となっています．\\\n",
        "今回は，平均値を最終的な損失とするため，データ数で除算します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aiTXn_2CdD"
      },
      "outputs": [],
      "source": [
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size=128, temperature=1):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size  = batch_size  # ミニバッチサイズ\n",
        "        self.temperature = temperature  # 温度パラメータ\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)  # マスクの作成\n",
        "        self.criterion    = nn.CrossEntropyLoss(reduction=\"sum\")  # クロスエントロピー(softmaxを内包)\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)  # コサイン類似度\n",
        "\n",
        "    # 同一特徴量間の類似度とポジティブペアの類似度を削除するためのマスクの作成\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N    = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)  # 同一特徴量間の類似度が入っている位置の値を0に\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size+i] = 0  # ポジティブペアの類似度が入っている位置の値を0に\n",
        "            mask[batch_size+i, i] = 0  # ポジティブペアの類似度が入っている位置の値を0に\n",
        "        return mask\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        # ------ 1 --------------------------------------------------------------------------\n",
        "        N = 2 * self.batch_size\n",
        "        z = torch.cat((z_i, z_j), dim=0)  # ネットワークの２つの出力を1つのTensorに\n",
        "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature  # 全ての特徴量間の類似度を計算\n",
        "\n",
        "        # ------ 2 --------------------------------------------------------------------------\n",
        "        sim_i_j = torch.diag(sim,  self.batch_size)  # ポジティブペアの類似度(i->j))を抽出\n",
        "        sim_j_i = torch.diag(sim, -self.batch_size)  # ポジティブペアの類似度(j->i))を抽出\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)  # ポジティブペアの類似度を1つのTensorに\n",
        "        \n",
        "        negative_samples = sim[self.mask].reshape(N, -1)  # ネガティブペアの類似度のみのTensorを作成\n",
        "\n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)  # ポジティブペアとネガティブペアの類似度を1つのテンソルに\n",
        "\n",
        "        # ------ 3 --------------------------------------------------------------------------\n",
        "        labels = torch.zeros(N).to(positive_samples.cuda()).long()  # ポジティブペアの類似度の位置を表すTensorを作成\n",
        "\n",
        "        loss = self.criterion(logits, labels)  # 損失計算（総和）\n",
        "        loss /= N  # データ数で除算（平均）\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HizwoyXf0-ag"
      },
      "source": [
        "## 学習条件の設定\n",
        "ミニバッチサイズが大きい場合，１epochあたりのiteration数（ネットワークの更新回数）が少なくなります．\\\n",
        "そのため，一般的には，学習率や学習回数を大きく設定します．\\\n",
        "そこで，SimCLRではミニバッチサイズに合わせて学習率を調整しています．\\\n",
        "また，学習率が大きい場合，学習が不安定になります．\\\n",
        "そこで，最適化方法としてLARS (Layerwise Adaptive Rate Scaling)，学習率の調整テクニックとしてWarmupを利用します．\n",
        "\n",
        "LARSは，ネットワークの層ごとに勾配の大きさに合わせて重みパラメータの更新量を調整する最適化方法です．\\\n",
        "Warmupは，学習初期から高い学習率とするのではなく，学習初期は小さな値として徐々に一定の値まで向上させるテクニックです．\\\n",
        "また，自己教師あり学習では，コサイン関数に基づいて学習率の減衰が行われます．\n",
        "\n",
        "今回は，LARSは最初にインストールを行ったライブラリのものを使用し，Warmupとコサイン関数に基づいた学習率の調整は自作の関数により実現します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxu8R_5rNcOV"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, init_lr, epoch, max_epoch, warmup_epoch):\n",
        "    if epoch <= warmup_epoch:\n",
        "        # Warmupによる学習率の増幅\n",
        "        cur_lr = init_lr * (epoch/warmup_epoch)\n",
        "    else:\n",
        "        # コサイン関数に基づいた学習率の減衰\n",
        "        cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * (epoch-warmup_epoch) / (max_epoch-warmup_epoch)))\n",
        "\n",
        "    # optimizerに格納されている学習率の値の変更\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
        "            param_group['lr'] = init_lr\n",
        "        else:\n",
        "            param_group['lr'] = cur_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyybHHZNcOW"
      },
      "source": [
        "今回は，学習回数を100 epoch，Warmupの期間を10 epochとします．\\\n",
        "Google colaboratory上では1 epochあたり約1分弱（100 epochで約2時間）の時間がかかります．\\\n",
        "そのため，学習回数は適宜調整して実行してください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgriDGQOTGYJ"
      },
      "outputs": [],
      "source": [
        "# ネットワークの用意\n",
        "encoder = CNN(widen_factor=1)  # ネットワークの定義\n",
        "net = SimCLR(encoder)  # MLPを含めたネットワークの定義\n",
        "\n",
        "# 学習回数の設定\n",
        "NUM_EPOCH = 100\n",
        "\n",
        "# Warmupの期間の設定\n",
        "WARMUP_EPOCH = 10\n",
        "\n",
        "# 学習率の設定\n",
        "LR = 0.3*(BATCH_SIZE/256)\n",
        "\n",
        "# 自己教師あり学習の損失式の設定\n",
        "criterion = NT_Xent(BATCH_SIZE, 0.5)\n",
        "\n",
        "# 最適化方法の設定\n",
        "base_optimizer = torch.optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=10e-6)\n",
        "optimizer = LARS(optimizer=base_optimizer, trust_coef=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6yLJvmTca31"
      },
      "source": [
        "### 学習率の推移の可視化\n",
        "Warmupとコサイン関数に基づいた調整により，どのように学習率がどのように変化するのかを可視化します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "OQ1EPwuQcfty",
        "outputId": "a57d7fb7-5f45-46f9-a866-e97bdfcf4c56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 学習率の推移の可視化\n",
        "def print_lr(LR, NUM_EPOCH):\n",
        "    model = torch.nn.Linear(10, 3) # 仮のネットワークを作成\n",
        " \n",
        "    # 可視化したい条件\n",
        "    lr_list = list()\n",
        "    base_optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=10e-6)\n",
        "    optimizer = LARS(optimizer=base_optimizer, trust_coef=0.001)\n",
        "\n",
        "    # 学習率の取得\n",
        "    for epoch in range(1, NUM_EPOCH+1):\n",
        "        adjust_learning_rate(optimizer, LR, epoch, NUM_EPOCH, WARMUP_EPOCH)\n",
        "        pg = optimizer.param_groups[0]\n",
        "        lr_list.append(pg['lr'])\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "\n",
        "    # 学習率の推移を描画\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot([i+1 for i in range(len(lr_list))], lr_list)\n",
        "    # 10 epoch刻みで縦線を描画\n",
        "    plt.vlines([10*i for i in range(int((NUM_EPOCH/10)+1))], 0, np.array(lr_list).max(), \"green\", linestyles='dashed')\n",
        "    # x軸とy軸のラベルを描画\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('lr')\n",
        "    plt.show()\n",
        "\n",
        "# 学習率の推移の確認\n",
        "print_lr(LR, NUM_EPOCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvcU3vH-1JTP"
      },
      "source": [
        "## ネットワークの学習\n",
        "ネットワークを自己教師あり学習します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfVDPUaj2CdD",
        "outputId": "56606623-bd95-43ce-bdce-f22f48b34146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchlars/lars.py:140: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  p.grad.add_(weight_decay, p.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5,                mean loss: 5.273,                elapsed_time :388.55\n",
            "epoch: 10,                mean loss: 5.17,                elapsed_time :777.28\n",
            "epoch: 15,                mean loss: 5.103,                elapsed_time :1167.04\n",
            "epoch: 20,                mean loss: 5.054,                elapsed_time :1555.77\n",
            "epoch: 25,                mean loss: 5.024,                elapsed_time :1936.92\n",
            "epoch: 30,                mean loss: 5.001,                elapsed_time :2313.25\n",
            "epoch: 35,                mean loss: 4.98,                elapsed_time :2688.25\n",
            "epoch: 40,                mean loss: 4.969,                elapsed_time :3072.51\n",
            "epoch: 45,                mean loss: 4.954,                elapsed_time :3451.83\n",
            "epoch: 50,                mean loss: 4.945,                elapsed_time :3826.79\n",
            "epoch: 55,                mean loss: 4.933,                elapsed_time :4202.7\n",
            "epoch: 60,                mean loss: 4.926,                elapsed_time :4577.59\n",
            "epoch: 65,                mean loss: 4.919,                elapsed_time :4951.18\n",
            "epoch: 70,                mean loss: 4.915,                elapsed_time :5325.14\n",
            "epoch: 75,                mean loss: 4.908,                elapsed_time :5702.39\n",
            "epoch: 80,                mean loss: 4.905,                elapsed_time :6077.79\n",
            "epoch: 85,                mean loss: 4.901,                elapsed_time :6452.55\n",
            "epoch: 90,                mean loss: 4.901,                elapsed_time :6826.59\n",
            "epoch: 95,                mean loss: 4.899,                elapsed_time :7202.11\n",
            "epoch: 100,                mean loss: 4.899,                elapsed_time :7577.42\n"
          ]
        }
      ],
      "source": [
        "# ネットワークをGPUへ\n",
        "net = net.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "net.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "\n",
        "    # 学習率の調整\n",
        "    adjust_learning_rate(optimizer, LR, epoch, NUM_EPOCH, 10)\n",
        "    \n",
        "    for (image_i, image_y), _ in unsup_loader:\n",
        "        # ラベルなしデータをGPUへ\n",
        "        image_i = image_i.cuda()\n",
        "        image_y = image_y.cuda()\n",
        "        \n",
        "        # ラベルなしデータをネットワークへ入力\n",
        "        z_i, z_j = net(image_i, image_y)\n",
        "        \n",
        "        # 損失の計算\n",
        "        loss = criterion(z_i, z_j)\n",
        "        \n",
        "        # パラメータの更新\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # ログ用に損失値を取得\n",
        "        sum_loss += loss.item()\n",
        "       \n",
        "    # ログの表示\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):\n",
        "        print(f\"epoch: {epoch},\\\n",
        "                mean loss: {round(sum_loss/len(unsup_loader), 3)},\\\n",
        "                elapsed_time :{round(time()-start, 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2qnV17HNcOW"
      },
      "source": [
        "## 自己教師あり学習したネットワークの保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAWqCwfGNcOW"
      },
      "outputs": [],
      "source": [
        "torch.save(net.encoder.to('cpu').state_dict(), './cifar100_pre_train.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Pf4VNhNcOW"
      },
      "source": [
        "## 自己教師あり学習したネットワークの評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8rcAGIfNcOX"
      },
      "source": [
        "### データセットの用意\n",
        "下流タスクとして使用するデータセットを定義します．\\\n",
        "今回は，下流タスクとしてランダムに選んだ1,000サンプルのデータから学習します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "c40b085b3d3349e88fd45b995107f498",
            "27018c26614c4e6794bf8a128fd25c41",
            "f63326f318f2453fb7268619ac326aab",
            "c273b568f817436f89c2c2dde06a95a5",
            "507698b71c7242eab1e15e64ccd7a454",
            "2f6a6d3721b140ff879b77d80d9bc542",
            "8815f5f93b4c43c7a314d7d93ea77f7a",
            "2cca9ccbf8204b249782f978b408f72c",
            "c36bd147b6134248bd8b7d5b0be0bb19",
            "671dd49d9b834c2e97b375f95bb26e25",
            "b475a1854e584023a2da3fe6805c5420"
          ]
        },
        "id": "rxLyfdQ9NcOX",
        "outputId": "e301bbd6-c48c-4934-8a3d-69116eeec5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/CIFAR-10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c40b085b3d3349e88fd45b995107f498"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/CIFAR-10/cifar-10-python.tar.gz to ./dataset/CIFAR-10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# 学習用データに対するデータ増幅\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# 評価用データに対するデータ増幅\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# 学習用データ\n",
        "trainset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=True,  download=True, transform=train_transform)\n",
        "\n",
        "# 評価用データ\n",
        "testset  = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=False, download=True, transform=test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M5mUmd1NcOX"
      },
      "outputs": [],
      "source": [
        "# StratifiedShuffleSplit：データをシャッフルして分割\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=0)\n",
        "\n",
        "# list(range(len(total_trainset)))：データidのリスト，total_trainset.targets：データidに対応するラベル\n",
        "sss = sss.split(list(range(len(trainset))), trainset.targets)\n",
        "\n",
        "# 分割後のデータidを取得\n",
        "_, label_idx = next(sss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TirRDkMMNcOX"
      },
      "outputs": [],
      "source": [
        "# 学習用データセットを作成\n",
        "train_subset = Subset(trainset, label_idx)\n",
        "labels = [trainset.targets[idx] for idx in label_idx]\n",
        "train_subset.train_labels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmCG9JLfNcOX",
        "outputId": "7bcebeb8-93b1-4505-9121-f7b2455b2d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データ数\n",
            "教師ありデータ： 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"データ数\")\n",
        "print(\"教師ありデータ：\", len(train_subset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiyVIPwbNcOX",
        "outputId": "3d7bb8fb-8626-4a2e-dea8-926d396a38ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# 学習用データのDataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, \n",
        "                                           batch_size=16, \n",
        "                                           shuffle=True,\n",
        "                                           num_workers=16,\n",
        "                                           pin_memory=True,\n",
        "                                           drop_last=True)\n",
        "\n",
        "# 評価用データのDataloader\n",
        "test_loader  = torch.utils.data.DataLoader(testset, \n",
        "                                           batch_size=16,\n",
        "                                           shuffle=False, \n",
        "                                           num_workers=16, \n",
        "                                           pin_memory=True, \n",
        "                                           drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kTkHNWMNcOX"
      },
      "source": [
        "### 乱数により初期化したネットワーク\n",
        "自己教師あり学習による事前学習した場合の結果と比較するために，乱数により初期化したネットワークを教師あり学習します．\\\n",
        "また，５epochごとに評価用データを用いてネットワークの評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovcv4vU7NcOX",
        "outputId": "3b5d4059-676e-4b55-eb5f-7c8df5019de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 2.267,            mean accuracy: 0.15,            elapsed_time :1.6\n",
            "epoch: 2,            mean loss: 2.064,            mean accuracy: 0.23,            elapsed_time :3.18\n",
            "epoch: 3,            mean loss: 1.912,            mean accuracy: 0.3,            elapsed_time :4.82\n",
            "epoch: 4,            mean loss: 1.761,            mean accuracy: 0.35,            elapsed_time :6.42\n",
            "epoch: 5,            mean loss: 1.594,            mean accuracy: 0.42,            elapsed_time :8.04\n",
            "test accuracy: 0.3635\n",
            "epoch: 6,            mean loss: 1.4,            mean accuracy: 0.47,            elapsed_time :14.05\n",
            "epoch: 7,            mean loss: 1.193,            mean accuracy: 0.54,            elapsed_time :15.67\n",
            "epoch: 8,            mean loss: 1.013,            mean accuracy: 0.64,            elapsed_time :17.24\n",
            "epoch: 9,            mean loss: 0.719,            mean accuracy: 0.74,            elapsed_time :18.83\n",
            "epoch: 10,            mean loss: 0.496,            mean accuracy: 0.81,            elapsed_time :20.42\n",
            "test accuracy: 0.3848\n",
            "epoch: 11,            mean loss: 0.3,            mean accuracy: 0.89,            elapsed_time :26.54\n",
            "epoch: 12,            mean loss: 0.416,            mean accuracy: 0.86,            elapsed_time :28.12\n",
            "epoch: 13,            mean loss: 0.15,            mean accuracy: 0.94,            elapsed_time :29.74\n",
            "epoch: 14,            mean loss: 0.212,            mean accuracy: 0.93,            elapsed_time :31.31\n",
            "epoch: 15,            mean loss: 0.22,            mean accuracy: 0.93,            elapsed_time :32.92\n",
            "test accuracy: 0.3879\n",
            "epoch: 16,            mean loss: 0.088,            mean accuracy: 0.97,            elapsed_time :39.07\n",
            "epoch: 17,            mean loss: 0.032,            mean accuracy: 0.98,            elapsed_time :40.66\n",
            "epoch: 18,            mean loss: 0.016,            mean accuracy: 0.99,            elapsed_time :42.24\n",
            "epoch: 19,            mean loss: 0.004,            mean accuracy: 0.99,            elapsed_time :43.81\n",
            "epoch: 20,            mean loss: 0.001,            mean accuracy: 0.99,            elapsed_time :45.41\n",
            "test accuracy: 0.417\n",
            "epoch: 21,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :51.63\n",
            "epoch: 22,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :53.25\n",
            "epoch: 23,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :54.83\n",
            "epoch: 24,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :56.39\n",
            "epoch: 25,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :57.94\n",
            "test accuracy: 0.4226\n",
            "epoch: 26,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :64.96\n",
            "epoch: 27,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :66.95\n",
            "epoch: 28,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :68.57\n",
            "epoch: 29,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :70.21\n",
            "epoch: 30,            mean loss: 0.0,            mean accuracy: 0.99,            elapsed_time :71.82\n",
            "test accuracy: 0.423\n"
          ]
        }
      ],
      "source": [
        "# ネットワークの設定\n",
        "sup_net = CNN(widen_factor=1)\n",
        "\n",
        "# 最適化方法の設定\n",
        "optimizer = torch.optim.SGD(sup_net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 損失式の設定\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# エポック数の設定\n",
        "NUM_EPOCH = 30\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "sup_net = sup_net.cuda()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # ネットワークの学習 ----------------------------------------------------------\n",
        "    # ネットワークを学習モードへ変更\n",
        "    sup_net.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        # 学習用データをGPUへ\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # 学習用データをネットワークへ入力\n",
        "        logits = sup_net(image)\n",
        "\n",
        "        # 損失の計算\n",
        "        loss = criterion(logits, label)\n",
        "        \n",
        "        # パラメータの更新\n",
        "        sup_net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        pred   = torch.argmax(logits, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "        \n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch},\\\n",
        "            mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
        "            mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
        "            elapsed_time :{round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワークの評価 ----------------------------------------------------------\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
        "        # ネットワークを評価モードへ変更\n",
        "        sup_net.eval()\n",
        "\n",
        "        # 評価の実行\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in test_loader:\n",
        "                # 評価用データをGPUへ\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "                # 評価用データをネットワークへ入力\n",
        "                logits = sup_net(image)\n",
        "\n",
        "                # 正解したデータ数を取得\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                count += torch.sum(pred == label)\n",
        "\n",
        "        # 評価結果の表示\n",
        "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl2oehfINcOY"
      },
      "source": [
        "### 自己教師あり学習により事前学習したネットワーク\n",
        "自己教師あり学習により事前学習したネットワークを学習します．\n",
        "\n",
        "自己教師あり学習は，学習に多くの時間を必要とするため，いくつかの自己教師あり学習した重みパラメータを用意しました．\n",
        "使用したい場合は，以下のプログラムから重みパラメータのダウンロードを行い，次の「自己教師あり学習した重みパラメータの指定」のPATHを変更して実行してください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkD33kf7NcOY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# ネットワーク：widen_factor=1, エポック数：300, ミニバッチサイズ：256\n",
        "#res = requests.get(\"https://www.dropbox.com/s/ux6f8gz88bzg9qe/cifar100_wf1_300epoch_256batch.pth?dl=1\")\n",
        "#with open(f\"cifar100_wf1_300epoch_256batch.pth\", 'wb') as f:\n",
        "#    f.write(res.content)\n",
        "\n",
        "# ネットワーク：widen_factor=1, エポック数：300, ミニバッチサイズ：512\n",
        "#res = requests.get(\"https://www.dropbox.com/s/e31vr8o57zyqlfe/cifar100_wf1_300epoch_512batch.pth?dl=1\")\n",
        "#with open(f\"cifar100_wf1_300epoch_512batch.pth\", 'wb') as f:\n",
        "#    f.write(res.content)\n",
        "\n",
        "# ネットワーク：widen_factor=1, エポック数：100, ミニバッチサイズ：1024\n",
        "#res = requests.get(\"https://www.dropbox.com/s/a6h9n4m8007biwm/cifar100_wf1_100epoch_1024batch.pth?dl=1\")\n",
        "#with open(f\"cifar100_wf1_100epoch_1024batch.pth\", 'wb') as f:\n",
        "#    f.write(res.content)\n",
        "\n",
        "# ネットワーク：widen_factor=1, エポック数：300, ミニバッチサイズ：1024\n",
        "#res = requests.get(\"https://www.dropbox.com/s/oi1p4kwd1mjij53/cifar100_wf1_300epoch_1024batch.pth?dl=1\")\n",
        "#with open(f\"cifar100_wf1_300epoch_1024batch.pth\", 'wb') as f:\n",
        "#    f.write(res.content)\n",
        "\n",
        "# ネットワーク：widen_factor=1, エポック数：1000, ミニバッチサイズ：1024\n",
        "#res = requests.get(\"https://www.dropbox.com/s/u46wsbgtgwqr7k2/cifar100_wf1_1000epoch_1024batch.pth?dl=1\")\n",
        "#with open(f\"cifar100_wf1_1000epoch_1024batch.pth\", 'wb') as f:\n",
        "#    f.write(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFAknQX2NcOY"
      },
      "source": [
        "#### 自己教師あり学習した重みパラメータの指定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiXEpPm2NcOY"
      },
      "outputs": [],
      "source": [
        "weight_path = './cifar100_pre_train.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqK1j2bNcOY"
      },
      "source": [
        "#### fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFAWjwWQNcOY",
        "outputId": "469c817b-e0f5-4a36-dd06-42a7f84df350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_IncompatibleKeys(missing_keys=['l3.weight', 'l3.bias'], unexpected_keys=[])\n",
            "epoch: 1,            mean loss: 2.221,            mean accuracy: 0.16,            elapsed_time :1.67\n",
            "epoch: 2,            mean loss: 1.914,            mean accuracy: 0.3,            elapsed_time :3.24\n",
            "epoch: 3,            mean loss: 1.722,            mean accuracy: 0.37,            elapsed_time :4.83\n",
            "epoch: 4,            mean loss: 1.519,            mean accuracy: 0.43,            elapsed_time :6.4\n",
            "epoch: 5,            mean loss: 1.288,            mean accuracy: 0.53,            elapsed_time :7.95\n",
            "test accuracy: 0.4036\n",
            "epoch: 6,            mean loss: 1.085,            mean accuracy: 0.6,            elapsed_time :14.13\n",
            "epoch: 7,            mean loss: 0.892,            mean accuracy: 0.69,            elapsed_time :15.7\n",
            "epoch: 8,            mean loss: 0.598,            mean accuracy: 0.8,            elapsed_time :17.31\n",
            "epoch: 9,            mean loss: 0.379,            mean accuracy: 0.86,            elapsed_time :18.91\n",
            "epoch: 10,            mean loss: 0.319,            mean accuracy: 0.9,            elapsed_time :20.49\n",
            "test accuracy: 0.4085\n",
            "epoch: 11,            mean loss: 0.267,            mean accuracy: 0.9,            elapsed_time :26.63\n",
            "epoch: 12,            mean loss: 0.164,            mean accuracy: 0.95,            elapsed_time :28.22\n",
            "epoch: 13,            mean loss: 0.027,            mean accuracy: 0.99,            elapsed_time :29.85\n",
            "epoch: 14,            mean loss: 0.015,            mean accuracy: 0.99,            elapsed_time :31.47\n",
            "epoch: 15,            mean loss: 0.035,            mean accuracy: 0.98,            elapsed_time :33.07\n",
            "test accuracy: 0.4129\n",
            "epoch: 16,            mean loss: 0.113,            mean accuracy: 0.97,            elapsed_time :39.2\n",
            "epoch: 17,            mean loss: 0.19,            mean accuracy: 0.94,            elapsed_time :40.75\n",
            "epoch: 18,            mean loss: 0.092,            mean accuracy: 0.96,            elapsed_time :42.36\n",
            "epoch: 19,            mean loss: 0.061,            mean accuracy: 0.97,            elapsed_time :43.96\n",
            "epoch: 20,            mean loss: 0.091,            mean accuracy: 0.96,            elapsed_time :45.62\n",
            "test accuracy: 0.3888\n",
            "epoch: 21,            mean loss: 0.056,            mean accuracy: 0.98,            elapsed_time :51.7\n",
            "epoch: 22,            mean loss: 0.115,            mean accuracy: 0.95,            elapsed_time :53.32\n",
            "epoch: 23,            mean loss: 0.09,            mean accuracy: 0.97,            elapsed_time :54.89\n",
            "epoch: 24,            mean loss: 0.036,            mean accuracy: 0.98,            elapsed_time :56.53\n",
            "epoch: 25,            mean loss: 0.04,            mean accuracy: 0.98,            elapsed_time :58.12\n",
            "test accuracy: 0.4289\n",
            "epoch: 26,            mean loss: 0.006,            mean accuracy: 0.99,            elapsed_time :64.14\n",
            "epoch: 27,            mean loss: 0.075,            mean accuracy: 0.98,            elapsed_time :65.73\n",
            "epoch: 28,            mean loss: 0.136,            mean accuracy: 0.95,            elapsed_time :67.37\n",
            "epoch: 29,            mean loss: 0.067,            mean accuracy: 0.97,            elapsed_time :69.03\n",
            "epoch: 30,            mean loss: 0.013,            mean accuracy: 0.99,            elapsed_time :70.65\n",
            "test accuracy: 0.4113\n"
          ]
        }
      ],
      "source": [
        "# ネットワークの設定\n",
        "ssl_net = CNN(widen_factor=1)  # ネットワークの用意\n",
        "msg = ssl_net.load_state_dict(torch.load(weight_path), strict=False)  # 重みパラメータの読み込み\n",
        "print(msg)\n",
        "\n",
        "# 最適化方法の設定\n",
        "optimizer = torch.optim.SGD(ssl_net.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# エポック数の設定\n",
        "NUM_EPOCH = 30\n",
        "\n",
        "# ネットワークをGPUへ\n",
        "ssl_net = ssl_net.cuda()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # ネットワークの学習 ----------------------------------------------------------\n",
        "    # ネットワークを学習モードへ変更\n",
        "    ssl_net.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        # 学習用データをGPUへ\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        # 学習用データをネットワークへ入力\n",
        "        logits = ssl_net(image)\n",
        "\n",
        "        # 損失の計算\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        # パラメータの更新\n",
        "        ssl_net.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        pred   = torch.argmax(logits, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch},\\\n",
        "            mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
        "            mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
        "            elapsed_time :{round(time()-start, 2)}\")\n",
        "\n",
        "\n",
        "    # ネットワークの評価 ----------------------------------------------------------\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
        "        # ネットワークを評価モードへ変更\n",
        "        ssl_net.eval()\n",
        "\n",
        "        # 評価の実行\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in test_loader:\n",
        "                # 評価用データをGPUへ\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "                # 評価用データをネットワークへ入力\n",
        "                logits = ssl_net(image)\n",
        "\n",
        "                # 正解したデータ数を取得\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                count += torch.sum(pred == label)\n",
        "                \n",
        "        # 評価結果の表示\n",
        "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml_a45JR2CdD"
      },
      "source": [
        "#### 転移学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l64RbEtSNcOY"
      },
      "outputs": [],
      "source": [
        "# 出力層のみのネットワーク\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.model = nn.Linear(n_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk4PrHsnNcOY",
        "outputId": "16a16adc-c8fc-4e76-8bd3-6ce01e5dafde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1,            mean loss: 1.909,            mean accuracy: 0.32,            elapsed_time : 1.5\n",
            "epoch: 2,            mean loss: 1.479,            mean accuracy: 0.49,            elapsed_time : 3.02\n",
            "epoch: 3,            mean loss: 1.336,            mean accuracy: 0.55,            elapsed_time : 4.51\n",
            "epoch: 4,            mean loss: 1.222,            mean accuracy: 0.59,            elapsed_time : 6.47\n",
            "epoch: 5,            mean loss: 1.155,            mean accuracy: 0.61,            elapsed_time : 8.01\n",
            "test accuracy: 0.4913\n",
            "epoch: 6,            mean loss: 1.084,            mean accuracy: 0.63,            elapsed_time : 14.28\n",
            "epoch: 7,            mean loss: 1.035,            mean accuracy: 0.66,            elapsed_time : 15.76\n",
            "epoch: 8,            mean loss: 0.983,            mean accuracy: 0.68,            elapsed_time : 17.24\n",
            "epoch: 9,            mean loss: 0.943,            mean accuracy: 0.7,            elapsed_time : 18.74\n",
            "epoch: 10,            mean loss: 0.903,            mean accuracy: 0.72,            elapsed_time : 20.23\n",
            "test accuracy: 0.5129\n",
            "epoch: 11,            mean loss: 0.865,            mean accuracy: 0.74,            elapsed_time : 26.51\n",
            "epoch: 12,            mean loss: 0.842,            mean accuracy: 0.73,            elapsed_time : 28.04\n",
            "epoch: 13,            mean loss: 0.794,            mean accuracy: 0.76,            elapsed_time : 29.55\n",
            "epoch: 14,            mean loss: 0.775,            mean accuracy: 0.78,            elapsed_time : 31.04\n",
            "epoch: 15,            mean loss: 0.741,            mean accuracy: 0.79,            elapsed_time : 32.53\n",
            "test accuracy: 0.5202\n",
            "epoch: 16,            mean loss: 0.717,            mean accuracy: 0.79,            elapsed_time : 40.3\n",
            "epoch: 17,            mean loss: 0.693,            mean accuracy: 0.81,            elapsed_time : 42.32\n",
            "epoch: 18,            mean loss: 0.667,            mean accuracy: 0.83,            elapsed_time : 44.27\n",
            "epoch: 19,            mean loss: 0.645,            mean accuracy: 0.83,            elapsed_time : 45.78\n",
            "epoch: 20,            mean loss: 0.635,            mean accuracy: 0.83,            elapsed_time : 47.39\n",
            "test accuracy: 0.5155\n",
            "epoch: 21,            mean loss: 0.614,            mean accuracy: 0.84,            elapsed_time : 53.79\n",
            "epoch: 22,            mean loss: 0.589,            mean accuracy: 0.85,            elapsed_time : 55.36\n",
            "epoch: 23,            mean loss: 0.575,            mean accuracy: 0.86,            elapsed_time : 56.86\n",
            "epoch: 24,            mean loss: 0.565,            mean accuracy: 0.87,            elapsed_time : 58.42\n",
            "epoch: 25,            mean loss: 0.532,            mean accuracy: 0.88,            elapsed_time : 59.92\n",
            "test accuracy: 0.5155\n",
            "epoch: 26,            mean loss: 0.525,            mean accuracy: 0.88,            elapsed_time : 66.27\n",
            "epoch: 27,            mean loss: 0.507,            mean accuracy: 0.88,            elapsed_time : 67.79\n",
            "epoch: 28,            mean loss: 0.491,            mean accuracy: 0.89,            elapsed_time : 69.3\n",
            "epoch: 29,            mean loss: 0.477,            mean accuracy: 0.91,            elapsed_time : 70.82\n",
            "epoch: 30,            mean loss: 0.462,            mean accuracy: 0.9,            elapsed_time : 72.36\n",
            "test accuracy: 0.5193\n"
          ]
        }
      ],
      "source": [
        "# ネットワークの設定\n",
        "encoder = CNN(widen_factor=1)  # ネットワークの用意\n",
        "encoder.l3 = Identity()  # ネットワークの3層目の全結合層を変更（入力値をそのまま返すように変更）\n",
        "encoder.load_state_dict(torch.load(weight_path))  # 重みパラメータの読み込み\n",
        "\n",
        "net_fc = LogisticRegression(encoder.l2.out_features, 10)  # 出力層のみネットワークの用意\n",
        "\n",
        "# 最適化方法の設定\n",
        "optimizer = torch.optim.Adam(net_fc.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# エポック数の設定\n",
        "NUM_EPOCH = 30\n",
        "\n",
        "# ネットワークと出力層をGPUへ\n",
        "encoder = encoder.cuda()\n",
        "net_fc  = net_fc.cuda()\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "encoder.eval()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "    # 出力層の学習 ---------------------------------------------------------------\n",
        "    # 出力層を学習用モードへ変更\n",
        "    net_fc.train()\n",
        "\n",
        "    # ログ用の設定\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        # 学習用データをGPUへ\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        # 学習用データをネットワークへ入力\n",
        "        with torch.no_grad():\n",
        "            h = encoder(image)\n",
        "\n",
        "        # ネットワークの抽出した特徴量を出力層へ入力\n",
        "        h = h.detach()  # 計算グラフのカット\n",
        "        y = net_fc(h)\n",
        "\n",
        "        # 損失の計算\n",
        "        loss = criterion(y, label)\n",
        "\n",
        "        # パラメータの更新\n",
        "        net_fc.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ログ用に損失値と正解したデータ数を取得\n",
        "        sum_loss += loss.item()\n",
        "        pred   = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    # ログの表示\n",
        "    print(f\"epoch: {epoch},\\\n",
        "            mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
        "            mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
        "            elapsed_time : {round(time()-start, 2)}\")\n",
        "\n",
        "    # ネットワーク+出力層の評価 ----------------------------------------------------\n",
        "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
        "        # ネットワークを評価モードへ変更\n",
        "        net_fc.eval()\n",
        "\n",
        "        # 評価の実行\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for image, label in test_loader:\n",
        "                # 評価用データをGPUへ\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "                # 評価用データをネットワーク+出力層へ入力\n",
        "                h = encoder(image)\n",
        "                y = net_fc(h)\n",
        "\n",
        "                # 正解したデータ数を取得\n",
        "                pred = torch.argmax(y, dim=1)\n",
        "                count += torch.sum(pred == label)\n",
        "\n",
        "        # 評価結果の表示\n",
        "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_wcKMudNcOZ"
      },
      "source": [
        "# 課題\n",
        "1. 自己教師あり学習の学習回数（エポック数）を変更してみましょう\n",
        "2. 自己教師あり学習のミニバッチサイズを変更してみましょう\n",
        "3. 下流タスクのデータ数を変更してみましょう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJzjjOaKNcOZ"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e0947fa7bad4b2cae2b4aa09f4ecaf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee3795016a6b499d9f0bbe57efdd058a",
              "IPY_MODEL_f423634827844f9595ceac78410dab9d",
              "IPY_MODEL_6d706aa25de740f48e6e07805a3657ac"
            ],
            "layout": "IPY_MODEL_861eef61304b4514b065199250c14834"
          }
        },
        "ee3795016a6b499d9f0bbe57efdd058a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5eeceb72e043dfb69f80931a94090a",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3de3aadd284a8c8cc10ffbbbd2e3c2",
            "value": "100%"
          }
        },
        "f423634827844f9595ceac78410dab9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b990ad419d847f9b5a51584e9ea7d4f",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b44381e2091d4626a097667478719c07",
            "value": 169001437
          }
        },
        "6d706aa25de740f48e6e07805a3657ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084c9ce83646445bb1934c09c7edf0e1",
            "placeholder": "​",
            "style": "IPY_MODEL_13f88eb28dcf458582fae9c4bcbe504b",
            "value": " 169001437/169001437 [00:06&lt;00:00, 29578091.70it/s]"
          }
        },
        "861eef61304b4514b065199250c14834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5eeceb72e043dfb69f80931a94090a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3de3aadd284a8c8cc10ffbbbd2e3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b990ad419d847f9b5a51584e9ea7d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44381e2091d4626a097667478719c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "084c9ce83646445bb1934c09c7edf0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f88eb28dcf458582fae9c4bcbe504b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c40b085b3d3349e88fd45b995107f498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27018c26614c4e6794bf8a128fd25c41",
              "IPY_MODEL_f63326f318f2453fb7268619ac326aab",
              "IPY_MODEL_c273b568f817436f89c2c2dde06a95a5"
            ],
            "layout": "IPY_MODEL_507698b71c7242eab1e15e64ccd7a454"
          }
        },
        "27018c26614c4e6794bf8a128fd25c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6a6d3721b140ff879b77d80d9bc542",
            "placeholder": "​",
            "style": "IPY_MODEL_8815f5f93b4c43c7a314d7d93ea77f7a",
            "value": "100%"
          }
        },
        "f63326f318f2453fb7268619ac326aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cca9ccbf8204b249782f978b408f72c",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c36bd147b6134248bd8b7d5b0be0bb19",
            "value": 170498071
          }
        },
        "c273b568f817436f89c2c2dde06a95a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_671dd49d9b834c2e97b375f95bb26e25",
            "placeholder": "​",
            "style": "IPY_MODEL_b475a1854e584023a2da3fe6805c5420",
            "value": " 170498071/170498071 [00:05&lt;00:00, 32304414.11it/s]"
          }
        },
        "507698b71c7242eab1e15e64ccd7a454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6a6d3721b140ff879b77d80d9bc542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8815f5f93b4c43c7a314d7d93ea77f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cca9ccbf8204b249782f978b408f72c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36bd147b6134248bd8b7d5b0be0bb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "671dd49d9b834c2e97b375f95bb26e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b475a1854e584023a2da3fe6805c5420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
