{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MAI7stlyXER",
    "tags": []
   },
   "source": [
    "# データサイエンス演習\n",
    "\n",
    "データを分析して，機械学習を用いて予測する一連の流れを行います．\n",
    "ここでは，Boston Housing データセットを用いて，住宅価格を予測します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lORkpTBet5eY"
   },
   "source": [
    "## Boston Housing dataset\n",
    "\n",
    "Boston Housingは，13属性（説明変数）＋1属性（目的変数）の14属性で構成されています．\n",
    "各属性は以下のようになっています．\n",
    "\n",
    "*  CRIM： 町別の「犯罪率」\n",
    "*  ZN： 25,000平方フィートを超える区画に分類される住宅地の割合＝「広い家の割合」\n",
    "*  INDUS： 町別の「非小売業の割合」\n",
    "*  CHAS： チャールズ川に接している場合は1、そうでない場合は0\n",
    "*  NOX： 「NOx濃度（0.1ppm単位）」＝一酸化窒素濃度（parts per 10 million単位）\n",
    "*  RM： 1戸当たりの「平均部屋数」\n",
    "*  AGE： 1940年より前に建てられた持ち家の割合＝「古い家の割合」\n",
    "*  DIS： 5つあるボストン雇用センターまでの加重距離＝「主要施設への距離」\n",
    "*  RAD： 「主要高速道路へのアクセス性」の指数\n",
    "*  TAX： 10,000ドル当たりの「固定資産税率」\n",
    "*  PTRATIO： 町別の「生徒と先生の比率」\n",
    "*  B： 「1000(Bk - 0.63)」の二乗値。Bk＝「町ごとの黒人の割合」を指す\n",
    "*  LSTAT： 「低所得者人口の割合」\n",
    "*  MEDV：「住宅価格」（1000ドル単位）の中央値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R19VwwqzKJz"
   },
   "source": [
    "Boston Housing datasetを読み込みます．このデータセットはScikit learnのチュートリアルでも利用されているので，データのダウンロードを行う関数load_boston()も用意されています．\n",
    "\n",
    "**注意**<br>\n",
    "~~Boston Housing datasetは黒人の割合などの差別的なデータを含んでいるため，倫理的な観点からScikit learnのversion 1.0以降から非推奨となっています．version 1.2ではこのデータが削除されることが決まっています．<br>\n",
    "代わりのデータとして，California Housing datasetが用意されています．\n",
    "version 1.2以降でもBoston Housing datasetを使用したい場合はWebから直接ダウンロードしてください．\n",
    "version 1.0以降のScikit learnでは，この旨を伝える警告文が出力されますがエラーではないです．~~\n",
    "\n",
    "* Boston Housing datasetの読み込み\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "```\n",
    "\n",
    "* California Housing datasetの読み込み\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ボストン住宅価格データセットを利用するためにversion 1.1.0のsklearnをインストール\n",
    "! pip install -U scikit-learn==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnのバージョン確認\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWcmvxd_tsNk",
    "outputId": "d37fbec9-7958-4f12-ee8b-a7e3f1197618"
   },
   "outputs": [],
   "source": [
    "#ボストン住宅価格データセットの読み込み\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "#説明変数\n",
    "X_array = boston.data\n",
    "#目的変数\n",
    "y_array = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv0J8K7ry5BL"
   },
   "source": [
    "データを分析するために，データをpandasモジュールのデータフレームに変換します．\n",
    "そして，データフレームを出力して，データの中身を確認します．\n",
    "データ数は506個，14属性(説明変数+目的変数)となっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LZoSg86u134",
    "outputId": "1ee42494-5e41-4833-c11e-747cb8490fd3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(X_array, columns = boston.feature_names).assign(MEDV=np.array(y_array))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whWMQL0M0_Hy"
   },
   "source": [
    "MEDV(住宅価格)と説明変数の関係性の可視化をします．\n",
    "関係を可視化する説明変数を以下に示します．\n",
    "* CRIM(犯罪率)とMEDV(住宅価格)\n",
    "* ZN(宅地比率)とMEDV(住宅価格)\n",
    "* INDUS(非小売業エーカーの割合)とMEDV(住宅価格)\n",
    "* CHAS(チャーリーズ川ダミー変数)とMEDV(住宅価格)\n",
    "* RM(1住戸あたりの平均部屋数)とMEDV(住宅価格)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "cSzeBx3G0Cgb",
    "outputId": "01dafa69-9129-4c48-d5f8-4618b02e1937"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n",
    "df.plot.scatter(x='CRIM', y='MEDV', ax=ax[0,0], legend=False)\n",
    "df.plot.scatter(x='ZN', y='MEDV', ax=ax[0,1], legend=False)\n",
    "df.plot.scatter(x='INDUS', y='MEDV', ax=ax[0,2], legend=False)\n",
    "df.plot.scatter(x='CHAS', y='MEDV', ax=ax[1,0], legend=False)\n",
    "df.plot.scatter(x='RM', y='MEDV', ax=ax[1,1], legend=False)\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scs05mhR2XZn"
   },
   "source": [
    "これらの結果からは，関係があるかどうかわからないので，それぞれ線形回帰してみます．\n",
    "ここでは，Seabornというモジュールを利用します．\n",
    "また，相関係数も確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "CvBwG6GEvQSb",
    "outputId": "f020b2bc-016a-4fb8-8401-262e529b5b8e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n",
    "\n",
    "#CRIM(犯罪率)，MEDV(住宅価格)で可視化\n",
    "sns.regplot(x='CRIM',y='MEDV', data = df, ax=ax[0,0])\n",
    "print(df[['CRIM','MEDV']].corr())\n",
    "\n",
    "#ZN(宅地比率)，MEDV(住宅価格)で可視化\n",
    "sns.regplot(x='ZN', y='MEDV', data = df, ax=ax[0,1])\n",
    "print(df[['ZN','MEDV']].corr())\n",
    "\n",
    "#INDUS(非小売業エーカーの割合)，MEDV(住宅価格)で可視化\n",
    "sns.regplot(x='INDUS', y='MEDV', data = df, ax=ax[0,2])\n",
    "print(df[['INDUS','MEDV']].corr())\n",
    "\n",
    "#CHAS(チャーリーズ川ダミー変数)、MEDV(住宅価格)で可視化\n",
    "sns.regplot(x='CHAS',y='MEDV',data = df, ax=ax[1,0])\n",
    "print(df[['CHAS','MEDV']].corr())\n",
    "\n",
    "#RM(1住戸あたりの平均部屋数)、MEDV(住宅価格)で可視化\n",
    "sns.regplot(x='RM',y='MEDV',data = df, ax=ax[1,1])\n",
    "print(df[['RM','MEDV']].corr())\n",
    "\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "薄い青色の範囲は，回帰を行う際の信頼区間を表しています．\n",
    "これらの結果から，RMは他のデータよりもMEDVとの相関関係が強いことが確認できます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GAYbWll3kRQ"
   },
   "source": [
    "### 線形回帰による住宅価格の予測\n",
    "\n",
    "線形回帰により，住宅価格の予測をして，各説明変数に対する係数を確認します．\n",
    "ここでは，Scikit-learnのtrain_test_splitを用いて，データを訓練用として8割，テスト用として2割となるよう分割しています．\n",
    "`test_size`という引数を変更することでデータの分割割合が操作できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv1Qk9KxvZqL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 訓練データとテストデータに8:2で分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=0)\n",
    "\n",
    "# 線形回帰で学習\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XWEoxmN5iwi"
   },
   "source": [
    "住宅価格のように数値を予測する回帰モデルの性能評価には，\n",
    "* 残差プロット：残差（目的変数の真値と予測値の差分）\n",
    "* 平均二乗誤差：残差平方和をデータ数で正規化した値\n",
    "* 決定係数：相関係数の二乗\n",
    "\n",
    "を利用します．\n",
    "\n",
    "**残差プロット**<br>\n",
    "残差プロットは，目的変数の真値と予測値の差分の分布を可視化したものです．\n",
    "回帰モデルを$f:~\\mathbb{R}^{n}\\rightarrow \\mathbb{R}$，テスト用データの真値を$\\hat{y}$とすると，残差プロット$s$は以下のように定義できます．\n",
    "$$\n",
    "s(x_{i}, \\hat{y}_{i}) = f(x_{i}) - \\hat{y}_{i}\n",
    "$$\n",
    "回帰モデルが目的変数を正しく予測できた場合の残差は0になります．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id0EU81P6JFA"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test) # 検証データを用いて目的変数を予測\n",
    "\n",
    "plt.scatter(Y_pred, Y_pred - y_test, color = 'blue')      # 残差をプロット \n",
    "plt.hlines(y = 0, xmin = -10, xmax = 50, color = 'black') # x軸に沿った直線をプロット\n",
    "plt.title('Residual Plot')                                # 図のタイトル\n",
    "plt.xlabel('Predicted Values')                            # x軸のラベル\n",
    "plt.ylabel('Residuals')                                   # y軸のラベル\n",
    "plt.grid()                                                # グリッド線を表示\n",
    "\n",
    "plt.show()                                               # 図の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zexyuYhH6hsF"
   },
   "source": [
    "**平均二乗誤差**<br>\n",
    "平均二乗誤差は，残差の平方和をデータ数で正規化したものです．\n",
    "平均二乗誤差は各データに対する値ではなく，テストデータ全体を用いて1つのスコアを出力するため，回帰モデルの性能を数値化することができます．\n",
    "平均二乗誤差が小さいほど優れた回帰モデルであることを示しています．\n",
    "\n",
    "回帰モデルを先ほどと同様に$f:~\\mathbb{R}^{n}\\rightarrow \\mathbb{R}$，$N$をテストデータの総数とすると，平均二乗誤差$s$は以下のように定義できます．\n",
    "$$\n",
    "s = \\frac{1}{N}\\sum_{i=1}^{N}\\left(f(x_{i}) - \\hat{y}_{i}\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A8_ipKX6hBZ"
   },
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train) # 学習データに対する目的変数を予測\n",
    "Y_pred = model.predict(X_test) # 学習データに対する目的変数を予測\n",
    "print('MSE train data: ', np.mean((y_train - Y_train_pred)**2)) # 学習データを用いたときの平均二乗誤差を出力\n",
    "print('MSE test data: ', np.mean((y_test - Y_pred)**2))         # 検証データを用いたときの平均二乗誤差を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OJvae1C68of"
   },
   "source": [
    "**決定係数**<br>\n",
    "決定係数も回帰モデルの予測誤差を反映した指標です．\n",
    "このスコアは0から1の範囲で表されるスコアで，1に近いほど回帰モデルがデータにフィットしていると捉えることができます．\n",
    "回帰モデルを$f:~\\mathbb{R}^{n}\\rightarrow \\mathbb{R}$，$N$をテストデータの総数，$\\mu_{y}$は真値の平均値，つまり$\\mu_{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_{i}$とすると，決定係数$s$は以下のように定義できます．\n",
    "$$\n",
    "s = 1 - \\frac{\\sum_{i=1}^{N}\\left(y_{i} - f(x_{i})\\right)^{2}}{\\sum_{i=1}^{N}\\left(y_{i} - \\mu_{y}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "決定係数は、metricsの`r2_score`を利用することで算出できます．\n",
    "\n",
    "また、以下に示すように，LinearRegressionモデルのscoreメソッドでも算出できます．\n",
    "```python\n",
    "print('r^2 train data: ', model.score(X_train, y_train))\n",
    "print('r^2 test data: ', model.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJyPldMW7HnA"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('r^2 train data: ', r2_score(y_train, Y_train_pred))\n",
    "print('r^2 test data: ', r2_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBBGz1eD8gtf"
   },
   "source": [
    "学習データと検証データに対する決定係数を比較すると，検証データを用いたときの決定係数の方が小さいです．\n",
    "これは，学習した回帰モデルが過学習している可能性があると言えます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNzoi-xp9L0G"
   },
   "source": [
    "### ランダムフォレスト回帰による住宅価格の予測\n",
    "\n",
    "次に，ランダムフォレスト回帰を用いて，回帰モデル学習・評価します．\n",
    "ランダムフォレスト回帰にはensembleのRandomForestRegressorを用います．\n",
    "決定木の数は10とします．\n",
    "学習データ，検証データに対する平均二乗誤差と決定係数を確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLKRYuXkvjWw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 訓練データとテストデータに8:2で分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#sklearnのランダムフォレスト回帰\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=10)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "Y_train_pred = rfr.predict(X_train) # 学習データに対する目的変数を予測\n",
    "Y_pred = rfr.predict(X_test) # 学習データに対する目的変数を予測\n",
    "print('MSE train data: ', np.mean((y_train - Y_train_pred)**2)) # 学習データを用いたときの平均二乗誤差を出力\n",
    "print('MSE test data: ', np.mean((y_test - Y_pred)**2))         # 検証データを用いたときの平均二乗誤差を出力\n",
    "\n",
    "\n",
    "print('r^2 train data: ', r2_score(y_train, Y_train_pred))\n",
    "print('r^2 test data: ', r2_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAecAeY69sRv"
   },
   "source": [
    "### 勾配ブースティングによる住宅価格の予測\n",
    "\n",
    "勾配ブースティングを用いて，同様に回帰モデルを学習・評価します．\n",
    "勾配ブースティングで学習する弱識別器の数は150とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX-2bQj9wB5B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 訓練データとテストデータに8:2で分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=0)\n",
    "\n",
    "#sklearnの勾配ブースティング\n",
    "gbr = ensemble.GradientBoostingRegressor(n_estimators = 150, max_depth=3)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "Y_train_pred = gbr.predict(X_train) # 学習データに対する目的変数を予測\n",
    "Y_pred = gbr.predict(X_test) # 学習データに対する目的変数を予測\n",
    "print('MSE train data: ', np.mean((y_train - Y_train_pred)**2)) # 学習データを用いたときの平均二乗誤差を出力\n",
    "print('MSE test data: ', np.mean((y_test - Y_pred)**2))         # 検証データを用いたときの平均二乗誤差を出力\n",
    "\n",
    "\n",
    "print('r^2 train data: ', r2_score(y_train, Y_train_pred))\n",
    "print('r^2 test data: ', r2_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWh3lEAY_cck"
   },
   "source": [
    "### 交差検証法\n",
    "\n",
    "学習データと検証データを分けて回帰モデルを学習・評価しました．\n",
    "学習データと検証データを入れ替えて，各回帰モデルの交差検証誤差を比較します．\n",
    "ここではデータを5分割して平均二乗誤差を比較します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9EQ0usQ_bwy"
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import linear_model, ensemble\n",
    "\n",
    "#交差検証\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "# 線形回帰で学習\n",
    "model = linear_model.LinearRegression()\n",
    "linear_preds = cross_val_score(model, X_array, y_array, cv=cv, scoring='neg_mean_squared_error')\n",
    "print(\"Linear Regression  : \", round(mean(abs(linear_preds)), 3) )\n",
    "\n",
    "\n",
    "#ランダムフォレスト回帰\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=10)\n",
    "rfr_preds = cross_val_score(rfr, X_array, y_array, cv=cv, scoring= 'neg_mean_squared_error')\n",
    "print(\"Random Forest      : \", round(mean(abs(rfr_preds)), 3) )\n",
    "\n",
    "#勾配ブースティング\n",
    "gbr = ensemble.GradientBoostingRegressor(n_estimators = 150, max_depth=3)\n",
    "gbr_preds = cross_val_score(gbr, X_array, y_array, cv=cv, scoring= 'neg_mean_squared_error')\n",
    "print(\"Gradient Boosting  : \", round(mean(abs(gbr_preds)), 3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dspj8MABHjhD"
   },
   "source": [
    "## 演習\n",
    "\n",
    "\n",
    "1.   各回帰モデルのハイパーパラメータを変えて平均二乗誤差と決定係数を比較しましょう\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjJW7rF7rX6q"
   },
   "source": [
    "# マテリアルズインフォマティクス\n",
    "\n",
    "データサイエンス演習で行ったことを踏まえて，材料データベースを対象として回帰モデルの学習・評価を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL3ENvQR-zkR"
   },
   "source": [
    "\n",
    "まず，Materials Projectのデータ取得や、物性計算に便利なモジュールをインストールします．<br>\n",
    "pymatgenのみをインストールするとscipyがうまく動かないことがあるので，その対策として`pip install pywin32-ctypes`を先に実行しておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI6tHqp_oYUn"
   },
   "outputs": [],
   "source": [
    "!pip install pywin32-ctypes\n",
    "!pip install -q pymatgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DMAkhNesKtw"
   },
   "source": [
    "ここでは，説明変数として，\n",
    "\n",
    "\n",
    "*   化学式\n",
    "*   物質特性\n",
    "\n",
    "の２つを比較します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bJ71CK_sdaQ"
   },
   "source": [
    "まず，必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2W5m7vmoQ2C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymatgen.core.composition import Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUw4hZxwrkG4"
   },
   "source": [
    "## Density Functional Theory\n",
    "Density Functional Theoryという第一原理計算の手法で計算した物質のバンドギャップデータを取得します．\n",
    "バンドギャップは，電子が移動する時の障壁の大きさを表したものです．\n",
    "例えば，石はバンドギャップが広いので電気が通りません．\n",
    "金属は，バンドギャップが狭く，電気が通ります．\n",
    "半導体は，その中間で電気を通したり通さなかったりします．\n",
    "\n",
    "bandgapDFT.csvをPandasで読み込むことでデータフレームへ変換して，データの中身を確認します．\n",
    "データフレームを表示すると，一行目が化学式、二行目がバンドギャップ(eV)が4096サンプル含まれていることが確認できます．\n",
    "eVはエネルギーの単位の１つで，電子ボルト(electron volt)を表しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPKXwZK0o0v-"
   },
   "outputs": [],
   "source": [
    "!wget -nv https://citrineinformatics.box.com/shared/static/0ch2f96jxbqtntia49ipk7g8vx64tuap.csv\n",
    "!mv 0ch2f96jxbqtntia49ipk7g8vx64tuap.csv bandgapDFT.csv\n",
    "\n",
    "bandgap_df = pd.read_csv('bandgapDFT.csv', names=('Chemical formula', 'BandGap'))\n",
    "bandgap_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-XD5aDksreo"
   },
   "source": [
    "## 化学式の構成比を用いたバンドギャップの予測\n",
    "化学式を機械学習で扱うために，固定長ベクトルに変換する必要があります．\n",
    "そのための関数としてnaiveVectorize関数を用意します．\n",
    "naiveVectorize関数の引数に与えるcompositionは，pymatgenモジュールのCompositionオブジェクトです．\n",
    "このオブジェクトは，化学式から原子や構成比などを取得できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1AnsfT3ocT0"
   },
   "outputs": [],
   "source": [
    "#input:pymatgenのCompositionオブジェクト\n",
    "#output:組成ベクトル\n",
    "def naiveVectorize(composition):\n",
    "    vector = np.zeros((MAX_Z))\n",
    "    for element in composition:\n",
    "        #elementは原子。fractionはその原子が組成に含まれる割合\n",
    "            fraction = composition.get_atomic_fraction(element)\n",
    "            vector[element.Z - 1] = fraction\n",
    "    return(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSbn9QbwtNgy"
   },
   "source": [
    "データフレームから化学式とバンドギャップを取得します．\n",
    "\n",
    "バンドギャップはデータフレームから取得したものをそのまま利用します．\n",
    "一方，化学式はそのまま利用することができないため，事前に作成したnaiveVectorize関数で説明変数のベクトルに変換します．\n",
    "* materials: 化学式が格納される空のリスト\n",
    "* naiveFeatures: 化学式の特徴ベクトルが格納される空のリスト\n",
    "* bandgaps: 各化学式に対するバンドギャップデータ\n",
    "* formulas: データフレームから取得した化学式\n",
    "\n",
    "MAX_Zはデータ内の原子中の最大原子番号-1よりも大きい値を指定する必要があります．\n",
    "これは，各原子の組成比を原子番号-1の要素に割り当てたベクトルを特徴量として利用するためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSk8zG9Dogmu"
   },
   "outputs": [],
   "source": [
    "materials = []\n",
    "naiveFeatures = []\n",
    "bandgaps = bandgap_df['BandGap'].values\n",
    "formulas = bandgap_df['Chemical formula'].values\n",
    "\n",
    "MAX_Z = 100 #特徴量ベクトル最大長さ\n",
    "\n",
    "for formula in formulas:\n",
    "    material = Composition(formula)\n",
    "    materials.append(material) #化学式\n",
    "    naiveFeatures.append(naiveVectorize(material)) #特徴量ベクトル生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK1MUEdYtffs"
   },
   "source": [
    "バンドギャップの平均値からの誤差を求めてみます．\n",
    "ここでは，平均絶対誤差(Mean Absolute Error)を用いて誤差を計算します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9Furuo_olFA"
   },
   "outputs": [],
   "source": [
    "baselineError = np.mean(abs(np.mean(bandgaps) - bandgaps))\n",
    "print(\"Mean Absolute Error : \" + str(round(baselineError, 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰による予測\n",
    "線形回帰モデルで学習します．\n",
    "説明変数は先ほど作成した，化学式の構成比が含まれる特徴ベクトル`naiveFeatures`とします．\n",
    "また，目的変数は各化学式のバンドギャップです．\n",
    "データは訓練用とテスト用で9:1として，それぞれを10分割して交差検証を行い，計算した平均絶対誤差を表示します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "linear = LinearRegression()\n",
    "scores_physical =cross_val_score(linear, naiveFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Linear Regression with composition data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO正則化を用いた線形回帰による予測\n",
    "線形回帰モデルにLASSOを追加して学習します．\n",
    "ここでは，LASSO正則化の係数を0.1としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha=0.1)\n",
    "scores_physical =cross_val_score(clf, naiveFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Linear Regression+LASSO with composition data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjnv6ZrOtj21"
   },
   "source": [
    "### ランダムフォレスト回帰による予測\n",
    "次に，ランダムフォレスト回帰で学習します．\n",
    "決定木の数は10としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU-zoMJronW6"
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=10)\n",
    "scores_composition = cross_val_score(rfr, naiveFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Random Forest with composition data: \"\\\n",
    "    + str(round(abs(np.mean(scores_composition)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配ブースティングによる予測\n",
    "勾配ブースティングで学習します．\n",
    "ここでは，弱識別器の数を150としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators = 150, max_depth=3)\n",
    "scores_physical =cross_val_score(gbr, naiveFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Gradient Boosting with composition data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOR2eT1FtuLp"
   },
   "source": [
    "## 物質の特性を用いたバンドギャップの予測\n",
    "次に物質の特性を説明変数として利用します．\n",
    "\n",
    "説明変数として使用する特性は，\n",
    "* 原子の組成比\n",
    "* 原子番号\n",
    "* 電気陰性度 (electronegativity)\n",
    "* 族\n",
    "\n",
    "の4種類の値を使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohxYpWfLoqSn"
   },
   "outputs": [],
   "source": [
    "physicalFeatures = []\n",
    "\n",
    "for material in materials:\n",
    "    theseFeatures = []\n",
    "    fraction = []\n",
    "    atomicNo = []\n",
    "    eneg = []\n",
    "    group = []\n",
    "\n",
    "    for element in material:\n",
    "        fraction.append(material.get_atomic_fraction(element))\n",
    "        atomicNo.append(float(element.Z))\n",
    "        eneg.append(element.X)\n",
    "        group.append(float(element.group))\n",
    "\n",
    "    mustReverse = False\n",
    "    if fraction[1] > fraction[0]:\n",
    "        mustReverse = True\n",
    "\n",
    "    for features in [fraction, atomicNo, eneg, group]:\n",
    "        if mustReverse:\n",
    "            features.reverse()\n",
    "    theseFeatures.append(fraction[0] / fraction[1])\n",
    "    theseFeatures.append(eneg[0] - eneg[1])\n",
    "    theseFeatures.append(group[0])\n",
    "    theseFeatures.append(group[1])\n",
    "    physicalFeatures.append(theseFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h8-wBfZI_iI"
   },
   "source": [
    "### 線形回帰モデルによる予測\n",
    "線形回帰モデルで学習します．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXTRNWr2CCNP"
   },
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "scores_physical =cross_val_score(linear, physicalFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Linear Regression with physical data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWlEJ6a1Iz8x"
   },
   "source": [
    "### LASSO正則化を用いた線形回帰モデルによる予測\n",
    "線形回帰モデルにLASSOを追加して学習します．\n",
    "ここでは，LASSO正則化の係数を0.1としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20FOzDV99gUQ"
   },
   "outputs": [],
   "source": [
    "clf = Lasso(alpha=0.1)\n",
    "scores_physical =cross_val_score(clf, physicalFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Linear Regression+LASSO with physical data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaYcxtJuMkM"
   },
   "source": [
    "### ランダムフォレスト回帰による予測\n",
    "次に，ランダムフォレスト回帰で学習します．\n",
    "決定木の数は10としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoU06bi0osZK"
   },
   "outputs": [],
   "source": [
    "rfr = ensemble.RandomForestRegressor(n_estimators=10)\n",
    "scores_physical =cross_val_score(rfr, physicalFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Random Forest with physical data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4-7JgN7JI_W"
   },
   "source": [
    "### 勾配ブースティングによる予測\n",
    "勾配ブースティングで学習します．\n",
    "ここでは，弱識別器の数を150としています．\n",
    "この予測も交差検証を行います．\n",
    "訓練データとテストデータの割合やデータの分割数は先ほどと同様です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqeMJBdJDALa"
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 150, max_depth=3)\n",
    "scores_physical =cross_val_score(gbr, physicalFeatures,\\\n",
    "    bandgaps, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Mean Absolute Error by Gradient Boosting with physical data: \"\\\n",
    "    + str(round(abs(np.mean(scores_physical)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZDKsGeCzur1"
   },
   "source": [
    "# 参考資料\n",
    "https://qiita.com/KentoObata/items/7fd8c7527d586dffc329\n",
    "\n",
    "https://qiita.com/yut-nagase/items/6c2bc025e7eaa7493f89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Dspj8MABHjhD"
   ],
   "name": "MaterialsInfo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
